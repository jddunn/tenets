{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Tenets - Context that feeds your prompts","text":"<p>Context that feeds your prompts</p> <p>Illuminate your codebase. Surface relevant files. Build optimal context.</p> <p>All without leaving your machine. 20+ languages including Python, Go, Rust, Java, C#, Kotlin, Swift, Dart, GDScript &amp; more.</p>        Quick Start             View on GitHub      Terminal <pre><code>$ pip install tenets\n$ tenets distill \"implement OAuth2 authentication\"\n\u2728 Finding relevant files...\n\ud83d\udcca Ranking by importance...\n\ud83d\udce6 Aggregating context (45,231 tokens)\n\u2705 Context ready for your LLM!</code></pre> Illuminating Features Why\u00a0Tenets? Context on Demand <p>Stop hunting for files. Tenets discovers, ranks and assembles your code for you\u2014so you can focus on solving the problem.</p> Deeper Insight <p>Visualize dependencies, uncover complexity hotspots and track velocity trends. Know your codebase like never before.</p> Local &amp; Private <p>Your source never leaves your machine. With zero external API calls, Tenets keeps your intellectual property safe.</p> Flexible &amp; Extensible <p>Dial the ranking algorithm, expand the token budget and add plugins when you need more. Tenets grows with you.</p> Architecture at a Glance Input Scanner Analyzer Ranker Aggregator <p>Tenets flows your query through a pipeline of scanners, analyzers, rankers and aggregators, delivering context precisely tailored to your task.</p> Intelligent Context <p>Multi-factor ranking finds exactly what you need. No more manual file hunting.</p> 100% Local <p>Your code never leaves your machine. Complete privacy, zero API calls.</p> Lightning Fast <p>Analyzes thousands of files in seconds with intelligent caching.</p> Guiding Principles <p>Add persistent instructions that maintain consistency across AI sessions.</p> Code Intelligence <p>Visualize dependencies, track velocity, identify hotspots at a glance.</p> Zero Config <p>Works instantly with smart defaults. Just install and start distilling.</p> How It Works 1 Scan <p>Discovers files respecting .gitignore</p> \u2192 2 Analyze <p>Extracts structure and dependencies</p> \u2192 3 Rank <p>Scores by relevance to your prompt</p> \u2192 4 Aggregate <p>Optimizes within token limits</p> See it in action CLI Bash<pre><code>$ tenets distill \"implement OAuth2 authentication\"\n\u2728 Finding relevant files...\n\ud83d\udcca Ranking by importance...\n\ud83d\udce6 Aggregating context (45,231 tokens)\n\u2705 Context ready for your LLM!\n</code></pre> Output Rank Files Bash<pre><code>$ tenets rank \"fix authentication bug\" --top 10 --factors\n\ud83d\udd0d Scanning codebase...\n\ud83d\udcca Ranking files by relevance...\n\n1. src/auth/service.py - Score: 0.892\n   - semantic_similarity: 85%\n   - keyword_match: 92%\n   - import_centrality: 78%\n\n2. src/auth/middleware.py - Score: 0.834\n   - semantic_similarity: 79%\n   - keyword_match: 88%\n   - import_centrality: 65%\n</code></pre> Tree View Bash<pre><code>$ tenets rank \"add caching\" --tree --scores\n\ud83d\udcc1 Ranked Files\n\u251c\u2500\u2500 \ud83d\udcc2 src/\n\u2502   \u251c\u2500\u2500 \ud83d\udcc4 cache_manager.py [0.892]\n\u2502   \u251c\u2500\u2500 \ud83d\udcc4 redis_client.py [0.834]\n\u2502   \u2514\u2500\u2500 \ud83d\udcc4 config.py [0.756]\n\u251c\u2500\u2500 \ud83d\udcc2 src/api/\n\u2502   \u251c\u2500\u2500 \ud83d\udcc4 endpoints.py [0.723]\n\u2502   \u2514\u2500\u2500 \ud83d\udcc4 middleware.py [0.689]\n\u2514\u2500\u2500 \ud83d\udcc2 tests/\n    \u2514\u2500\u2500 \ud83d\udcc4 test_cache.py [0.534]\n</code></pre> Python Python<pre><code>from tenets import Tenets\nt = Tenets()\nresult = t.distill(\n    prompt=\"map request lifecycle\"\n)\nprint(result.context[:500])  # First 500 chars\n</code></pre> Output Sessions Python<pre><code># Sessions are managed through distill parameters\nctx = t.distill(\"design payment flow\", session_name=\"checkout-flow\")\n# Pin files through pin_file method\nt.pin_file(\"payment.py\")\nt.pin_file(\"stripe.py\")\nctx = t.distill(\"add refund support\", session_name=\"checkout-flow\")\n</code></pre> Output Ready to illuminate your codebase? <p>Join thousands of developers building better with Tenets.</p>          Get Started Now        <code>pip install tenets</code>"},{"location":"ARCHITECTURE/","title":"Tenets Complete Architecture Documentation","text":""},{"location":"ARCHITECTURE/#table-of-contents","title":"Table of Contents","text":"<ol> <li>System Overview</li> <li>Core Philosophy &amp; Design Principles</li> <li>Complete System Architecture</li> <li>NLP/ML Pipeline Architecture</li> <li>File Discovery &amp; Scanning System</li> <li>Code Analysis Engine</li> <li>Relevance Ranking System</li> <li>Git Integration &amp; Chronicle System</li> <li>Examination &amp; Quality Analysis</li> <li>Momentum &amp; Velocity Tracking</li> <li>Context Management &amp; Optimization</li> <li>Session Management Architecture</li> <li>Storage &amp; Caching Architecture</li> <li>Prompt Parsing &amp; Understanding</li> <li>Output Generation &amp; Formatting</li> <li>Performance Architecture</li> <li>Configuration System</li> <li>CLI &amp; API Architecture</li> <li>Visualization &amp; Reporting</li> <li>Security &amp; Privacy Architecture</li> <li>Extensibility &amp; Plugin System</li> <li>Deployment Architecture</li> <li>Testing &amp; Quality Assurance</li> <li>Future Roadmap &amp; Vision</li> </ol>"},{"location":"ARCHITECTURE/#system-overview","title":"System Overview","text":""},{"location":"ARCHITECTURE/#what-is-tenets","title":"What is Tenets?","text":"<p>Tenets is a sophisticated, local-first code intelligence platform that revolutionizes how developers interact with their codebases when working with AI assistants. Unlike traditional code search tools or simple context builders, Tenets employs advanced multi-stage analysis combining natural language processing, machine learning, static code analysis, git history mining, and intelligent ranking to build optimal context for any given task.</p> <p>The system operates entirely locally, ensuring complete privacy and security while delivering advanced code understanding capabilities. Every component is designed with performance in mind, utilizing aggressive caching, parallel processing, and incremental computation to handle codebases ranging from small projects to massive monorepos with millions of files.</p>"},{"location":"ARCHITECTURE/#core-architecture-principles","title":"Core Architecture Principles","text":"<ol> <li> <p>Local-First Processing: All analysis, ranking, and context generation happens on the developer's machine. No code ever leaves the local environment. External API calls are only made for optional LLM-based summarization, and even then, only with explicit user consent.</p> </li> <li> <p>Progressive Enhancement: The system provides value immediately with just Python installed, and scales up with optional dependencies. Core functionality works without any ML libraries, git integration works without any configuration, and advanced features gracefully degrade when dependencies are missing.</p> </li> <li> <p>Intelligent Caching: Every expensive operation is cached at multiple levels - memory caches for hot data, SQLite for structured data, disk caches for analysis results, and specialized caches for embeddings. Cache invalidation is intelligent, using file modification times, git commits, and content hashes.</p> </li> <li> <p>Configurable Intelligence: Every aspect of the ranking and analysis can be configured. Users can adjust factor weights, enable/disable features, add custom ranking functions, and tune performance parameters. The system adapts to different codebases and use cases.</p> </li> <li> <p>Streaming Architecture: The system uses streaming and incremental processing wherever possible. Files are analyzed as they're discovered, rankings are computed in parallel, and results stream to the user as they become available.</p> </li> </ol>"},{"location":"ARCHITECTURE/#complete-system-architecture","title":"Complete System Architecture","text":""},{"location":"ARCHITECTURE/#high-level-data-flow","title":"High-Level Data Flow","text":"<pre><code>graph TB\n    subgraph \"User Interaction Layer\"\n        CLI[CLI Interface&lt;br/&gt;typer]\n        API[Python API&lt;br/&gt;Library]\n        WebUI[Web UI&lt;br/&gt;Future]\n        IDE[IDE Extensions]\n    end\n\n    subgraph \"Command Orchestration\"\n        DISPATCHER[Command Dispatcher]\n        DISTILL[Distill Command]\n        EXAMINE[Examine Command]\n        CHRONICLE[Chronicle Command]\n        MOMENTUM[Momentum Command]\n        SESSION[Session Management]\n    end\n\n    subgraph \"Prompt Processing Layer\"\n        PARSER[Prompt Parser]\n        INTENT[Intent Detection]\n        KEYWORDS[Keyword Extraction]\n        ENTITIES[Entity Extraction]\n\n        subgraph \"NLP Pipeline\"\n            TOKENIZER[Tokenizer]\n            STOPWORDS[Stopwords]\n            RAKE[RAKE Keywords]\n            YAKE[YAKE Fallback]\n            TFIDF[BM25/TF-IDF Analysis]\n            BM25[BM25 Ranking]\n        end\n    end\n\n    subgraph \"File Discovery &amp; Analysis\"\n        SCANNER[File Scanner]\n        GITIGNORE[.gitignore Parser]\n        BINARY[Binary Detection]\n        PARALLEL[Parallel Scanner]\n\n        subgraph \"Code Analysis Engine\"\n            PYTHON_ANALYZER[Python Analyzer]\n            JS_ANALYZER[JavaScript Analyzer]\n            GO_ANALYZER[Go Analyzer]\n            JAVA_ANALYZER[Java Analyzer]\n            GENERIC_ANALYZER[Generic Analyzer]\n        end\n\n        subgraph \"AST &amp; Structure\"\n            CLASSES[Class Extraction]\n            FUNCTIONS[Function Extraction]\n            IMPORTS[Import Analysis]\n            EXPORTS[Export Analysis]\n        end\n    end\n\n    subgraph \"Intelligence &amp; Ranking\"\n        subgraph \"Ranking Engine\"\n            FAST[Fast Strategy]\n            BALANCED[Balanced Strategy]\n            THOROUGH[Thorough Strategy]\n            ML[ML Strategy]\n        end\n\n        subgraph \"Ranking Factors\"\n            SEMANTIC[Semantic Similarity&lt;br/&gt;25%]\n            KEYWORD_MATCH[Keyword Matching&lt;br/&gt;15%]\n            BM25_SIM[BM25 Similarity&lt;br/&gt;15%]\n            IMPORT_CENT[Import Centrality&lt;br/&gt;10%]\n            PATH_REL[Path Relevance&lt;br/&gt;10%]\n            GIT_SIG[Git Signals&lt;br/&gt;15%]\n        end\n\n        subgraph \"ML/NLP Pipeline\"\n            EMBEDDINGS[Local Embeddings]\n            EMBED_CACHE[Embedding Cache]\n            SIMILARITY[Similarity Computing]\n        end\n    end\n\n    subgraph \"Context Optimization\"\n        CONTEXT_BUILDER[Context Builder]\n        TOKEN_COUNTER[Token Counter]\n        SUMMARIZER[Summarizer]\n        FORMATTER[Output Formatter]\n    end\n\n    subgraph \"Storage &amp; Persistence\"\n        SQLITE[SQLite Database&lt;br/&gt;Sessions]\n        MEMORY[Memory Cache&lt;br/&gt;LRU]\n        DISK[Disk Cache&lt;br/&gt;Analysis Results]\n    end\n\n    CLI --&gt; DISPATCHER\n    API --&gt; DISPATCHER\n    WebUI --&gt; DISPATCHER\n    IDE --&gt; DISPATCHER\n\n    DISPATCHER --&gt; DISTILL\n    DISPATCHER --&gt; EXAMINE\n    DISPATCHER --&gt; CHRONICLE\n    DISPATCHER --&gt; MOMENTUM\n    DISPATCHER --&gt; SESSION\n\n    DISTILL --&gt; PARSER\n    PARSER --&gt; INTENT\n    PARSER --&gt; KEYWORDS\n    PARSER --&gt; ENTITIES\n\n    INTENT --&gt; TOKENIZER\n    KEYWORDS --&gt; RAKE\n    RAKE --&gt; YAKE\n    ENTITIES --&gt; TFIDF\n    ENTITIES --&gt; BM25\n\n    PARSER --&gt; SCANNER\n    SCANNER --&gt; GITIGNORE\n    SCANNER --&gt; BINARY\n    SCANNER --&gt; PARALLEL\n\n    SCANNER --&gt; PYTHON_ANALYZER\n    SCANNER --&gt; JS_ANALYZER\n    SCANNER --&gt; GO_ANALYZER\n    SCANNER --&gt; JAVA_ANALYZER\n    SCANNER --&gt; GENERIC_ANALYZER\n\n    PYTHON_ANALYZER --&gt; CLASSES\n    PYTHON_ANALYZER --&gt; FUNCTIONS\n    PYTHON_ANALYZER --&gt; IMPORTS\n    PYTHON_ANALYZER --&gt; EXPORTS\n\n    CLASSES --&gt; FAST\n    FUNCTIONS --&gt; BALANCED\n    IMPORTS --&gt; THOROUGH\n    EXPORTS --&gt; ML\n\n    FAST --&gt; SEMANTIC\n    BALANCED --&gt; KEYWORD_MATCH\n    THOROUGH --&gt; BM25_SIM\n    ML --&gt; IMPORT_CENT\n\n    SEMANTIC --&gt; EMBEDDINGS\n    EMBEDDINGS --&gt; EMBED_CACHE\n    EMBED_CACHE --&gt; SIMILARITY\n\n    SIMILARITY --&gt; CONTEXT_BUILDER\n    KEYWORD_MATCH --&gt; CONTEXT_BUILDER\n    BM25_SIM --&gt; CONTEXT_BUILDER\n\n    CONTEXT_BUILDER --&gt; TOKEN_COUNTER\n    CONTEXT_BUILDER --&gt; SUMMARIZER\n    CONTEXT_BUILDER --&gt; FORMATTER\n\n    FORMATTER --&gt; SQLITE\n    FORMATTER --&gt; MEMORY\n    FORMATTER --&gt; DISK</code></pre>"},{"location":"ARCHITECTURE/#system-component-overview","title":"System Component Overview","text":"<pre><code>graph LR\n    subgraph \"Core Components\"\n        NLP[NLP/ML Pipeline]\n        SCAN[File Scanner]\n        ANALYZE[Code Analyzer]\n        RANK[Ranking Engine]\n        CONTEXT[Context Builder]\n    end\n\n    subgraph \"Analysis Tools\"\n        EXAMINE[Examine Tool]\n        CHRONICLE[Chronicle Tool]\n        MOMENTUM[Momentum Tool]\n    end\n\n    subgraph \"Storage Systems\"\n        CACHE[Cache Manager]\n        SESSION[Session Store]\n        CONFIG[Configuration]\n    end\n\n    NLP --&gt; RANK\n    SCAN --&gt; ANALYZE\n    ANALYZE --&gt; RANK\n    RANK --&gt; CONTEXT\n\n    ANALYZE --&gt; EXAMINE\n    SCAN --&gt; CHRONICLE\n    CHRONICLE --&gt; MOMENTUM\n\n    RANK --&gt; CACHE\n    CONTEXT --&gt; SESSION\n    SESSION --&gt; CONFIG</code></pre>"},{"location":"ARCHITECTURE/#nlpml-pipeline-architecture","title":"NLP/ML Pipeline Architecture","text":""},{"location":"ARCHITECTURE/#centralized-nlp-components-updated","title":"Centralized NLP Components (Updated)","text":"<p>Tenets uses a centralized NLP architecture to avoid code duplication and ensure consistency:</p>"},{"location":"ARCHITECTURE/#core-nlp-module-structure","title":"Core NLP Module Structure","text":"Text Only<pre><code>tenets/core/nlp/\n\u251c\u2500\u2500 __init__.py          # Main NLP API exports\n\u251c\u2500\u2500 similarity.py        # Centralized similarity computations (NEW)\n\u251c\u2500\u2500 keyword_extractor.py # Unified keyword extraction with SimpleRAKE\n\u251c\u2500\u2500 tokenizer.py        # Code and text tokenization\n\u251c\u2500\u2500 stopwords.py        # Stopword management with fallbacks\n\u251c\u2500\u2500 embeddings.py       # Embedding generation (ML optional)\n\u251c\u2500\u2500 ml_utils.py         # ML utility functions\n\u251c\u2500\u2500 bm25.py            # BM25 ranking algorithm (primary)\n\u2514\u2500\u2500 tfidf.py           # TF-IDF calculations (optional alternative to BM25)\n</code></pre>"},{"location":"ARCHITECTURE/#similarity-computation-consolidation","title":"Similarity Computation Consolidation","text":"<p>All similarity calculations are now centralized in <code>similarity.py</code>:</p> <p>Unified API: Python<pre><code>from tenets.core.nlp import (\n    cosine_similarity,      # Dense or sparse vectors\n    sparse_cosine_similarity,  # Dict-based sparse vectors\n    euclidean_distance,     # L2 distance\n    manhattan_distance,     # L1 distance\n    SemanticSimilarity     # ML-based semantic similarity\n)\n</code></pre></p> <p>Key Features: - Automatic Detection: <code>cosine_similarity()</code> auto-detects sparse (dict) vs dense (list/array) vectors - Sparse Vector Support: Efficient similarity for high-dimensional sparse vectors (BM25/TF-IDF) - No Duplication: All modules import from central <code>similarity.py</code> - Graceful Fallback: Works without NumPy using pure Python implementations</p> <p>Usage Examples: Python<pre><code># Dense vectors\nsim = cosine_similarity([1, 0, 0], [0, 1, 0])  # \u2192 0.0\n\n# Sparse vectors (BM25/TF-IDF)\nvec1 = {\"python\": 0.8, \"code\": 0.6}\nvec2 = {\"python\": 0.7, \"test\": 0.5}\nsim = sparse_cosine_similarity(vec1, vec2)  # \u2192 0.76\n\n# Semantic similarity (requires ML)\nsem = SemanticSimilarity()\nsim = sem.compute(\"OAuth authentication\", \"login system\")  # \u2192 0.82\n</code></pre></p>"},{"location":"ARCHITECTURE/#pipeline-component-flow","title":"Pipeline Component Flow","text":"<pre><code>graph TD\n    subgraph \"Input Processing\"\n        INPUT[Raw Text Input]\n        PROMPT[User Prompt]\n        CODE[Code Content]\n    end\n\n    subgraph \"Tokenization Layer\"\n        CODE_TOK[Code Tokenizer&lt;br/&gt;camelCase, snake_case]\n        TEXT_TOK[Text Tokenizer&lt;br/&gt;NLP processing]\n    end\n\n    subgraph \"Keyword Extraction\"\n        RAKE_EXT[RAKE Extractor&lt;br/&gt;Primary - Fast &amp; Python 3.13 Compatible]\n        YAKE_EXT[YAKE Extractor&lt;br/&gt;Secondary - Python &lt; 3.13 Only]\n        TFIDF_EXT[BM25/TF-IDF Extractor&lt;br/&gt;Frequency-based Fallback]\n        FREQ_EXT[Frequency Extractor&lt;br/&gt;Final Fallback]\n    end\n\n    subgraph \"Stopword Management\"\n        CODE_STOP[Code Stopwords&lt;br/&gt;Minimal - 30 words]\n        PROMPT_STOP[Prompt Stopwords&lt;br/&gt;Aggressive - 200+ words]\n    end\n\n    subgraph \"Embedding Generation\"\n        LOCAL_EMB[Local Embeddings&lt;br/&gt;sentence-transformers]\n        MODEL_SEL[Model Selection&lt;br/&gt;MiniLM, MPNet]\n        FALLBACK[BM25 Fallback&lt;br/&gt;No ML required]\n    end\n\n    subgraph \"Similarity Computing\"\n        COSINE[Cosine Similarity]\n        EUCLIDEAN[Euclidean Distance]\n        BATCH[Batch Processing]\n    end\n\n    subgraph \"Caching System\"\n        MEM_CACHE[Memory Cache&lt;br/&gt;LRU 1000 items]\n        DISK_CACHE[SQLite Cache&lt;br/&gt;30 day TTL]\n    end\n\n    INPUT --&gt; CODE_TOK\n    INPUT --&gt; TEXT_TOK\n    PROMPT --&gt; TEXT_TOK\n    CODE --&gt; CODE_TOK\n\n    CODE_TOK --&gt; CODE_STOP\n    TEXT_TOK --&gt; PROMPT_STOP\n\n    CODE_STOP --&gt; RAKE_EXT\n    PROMPT_STOP --&gt; RAKE_EXT\n    RAKE_EXT --&gt; YAKE_EXT\n    YAKE_EXT --&gt; TFIDF_EXT\n    TFIDF_EXT --&gt; FREQ_EXT\n\n    FREQ_EXT --&gt; LOCAL_EMB\n    LOCAL_EMB --&gt; MODEL_SEL\n    MODEL_SEL --&gt; FALLBACK\n\n    FALLBACK --&gt; COSINE\n    COSINE --&gt; EUCLIDEAN\n    EUCLIDEAN --&gt; BATCH\n\n    BATCH --&gt; MEM_CACHE\n    MEM_CACHE --&gt; DISK_CACHE</code></pre>"},{"location":"ARCHITECTURE/#keyword-extraction-algorithms-comparison","title":"Keyword Extraction Algorithms Comparison","text":""},{"location":"ARCHITECTURE/#algorithm-overview--trade-offs","title":"Algorithm Overview &amp; Trade-offs","text":"Algorithm Speed Quality Memory Python 3.13 Best For Limitations RAKE Fast Good Low \u2705 Yes \u2022 Technical docs\u2022 Multi-word phrases\u2022 Fast processing \u2022 No semantic understanding\u2022 Language-dependent stopwords\u2022 May miss single important words SimpleRAKE Fast Good Minimal \u2705 Yes \u2022 No NLTK dependencies\u2022 Built-in implementation\u2022 Fast processing \u2022 No advanced NLP features\u2022 Basic tokenization only YAKE Moderate Very Good Low \u274c No \u2022 Statistical analysis\u2022 Language independent\u2022 Capital letter aware \u2022 Python 3.13 bug\u2022 Can produce duplicates\u2022 No deep semantics BM25 Fast Excellent High(handles length variation) \u2705 Yes \u2022 Primary ranking algorithm\u2022 Better for code search\u2022 Handles file size variation \u2022 Needs document corpus\u2022 Statistical only\u2022 No semantic understanding TF-IDF Fast Good Medium(corpus-dependent) \u2705 Yes \u2022 Optional alternative to BM25\u2022 Document uniqueness\u2022 Simpler algorithm \u2022 Less effective for varying lengths\u2022 No term saturation\u2022 Statistical only Frequency Very Fast Basic Minimal \u2705 Yes \u2022 Fallback option\u2022 Simple analysis\u2022 Guaranteed to work \u2022 Very basic\u2022 No context awareness\u2022 Misses importance"},{"location":"ARCHITECTURE/#detailed-algorithm-analysis","title":"Detailed Algorithm Analysis","text":""},{"location":"ARCHITECTURE/#rake-rapid-automatic-keyword-extraction","title":"RAKE (Rapid Automatic Keyword Extraction)","text":"Text Only<pre><code>Primary method for Python 3.13+\n</code></pre> <p>How it works: - Uses word frequency and co-occurrence to identify key phrases - Builds a word co-occurrence graph - Calculates word scores based on degree/frequency ratio</p> <p>Pros: - \u2705 Extremely fast - can process thousands of documents per second - \u2705 Excellent at extracting multi-word technical phrases - \u2705 No training required - works immediately - \u2705 Python 3.13 compatible - \u2705 Good performance with technical documentation - \u2705 Low memory footprint</p> <p>Cons: - \u274c No semantic understanding of word relationships - \u274c Dependent on stopword lists for quality - \u274c May miss important single-word keywords - \u274c Can struggle with very short texts</p> <p>Best Use Cases: - API documentation keyword extraction - Technical specification analysis - Code comment summarization - Real-time keyword extraction</p>"},{"location":"ARCHITECTURE/#yake-yet-another-keyword-extractor","title":"YAKE (Yet Another Keyword Extractor)","text":"Text Only<pre><code>Secondary method for Python &lt; 3.13\n</code></pre> <p>How it works: - Uses statistical features from individual documents - Considers word position, frequency, context - Pays attention to capitalization and word casing</p> <p>Pros: - \u2705 Language independent - works without language-specific resources - \u2705 No training corpus needed - \u2705 Good at identifying proper nouns and technical terms - \u2705 Considers word position and context - \u2705 Handles multiple languages well</p> <p>Cons: - \u274c Critical: Infinite loop bug on Python 3.13 - \u274c Can generate duplicate keywords with different cases - \u274c No deep semantic understanding - \u274c Slower than RAKE for large documents</p> <p>Best Use Cases: - Multi-language codebases - Mixed content (code + documentation) - When capitalization matters (class names, constants)</p>"},{"location":"ARCHITECTURE/#bm25-best-matching-25","title":"BM25 (Best Matching 25)","text":"Text Only<pre><code>Primary text similarity algorithm - default for ranking\n</code></pre> <p>How it works: - Probabilistic ranking function with term saturation (k1=1.2) - Document length normalization (b=0.75) handles varying file sizes - Prevents over-weighting of repeated terms - Default ranking algorithm (better than TF-IDF for code search)</p> <p>Pros: - \u2705 Superior ranking for varying document lengths - \u2705 Term saturation prevents keyword stuffing bias - \u2705 Industry standard for search engines - \u2705 Pure Python implementation - \u2705 Better for code files of different sizes</p> <p>Cons: - \u274c Requires a corpus of documents - \u274c More complex than basic keyword matching - \u274c Still statistical, no semantic understanding - \u274c Memory usage grows with corpus size</p> <p>Best Use Cases: - Default text similarity for all ranking - Code search across files of varying sizes - Finding relevant files for prompts</p>"},{"location":"ARCHITECTURE/#tf-idf-optional-alternative-to-bm25","title":"TF-IDF (Optional Alternative to BM25)","text":"Text Only<pre><code>Optional fallback method - configurable via text_similarity_algorithm\n</code></pre> <p>How it works: - Calculates term importance based on frequency in document vs corpus - Higher scores for terms that are frequent in document but rare in corpus - Uses vector space model for similarity calculations</p> <p>Pros: - \u2705 Always available - pure Python implementation - \u2705 Excellent for finding document-specific terms - \u2705 Good theoretical foundation - \u2705 Can identify unique technical terms - \u2705 Supports similarity calculations between documents</p> <p>Cons: - \u274c Requires a corpus of documents for comparison - \u274c No phrase extraction (single words only) - \u274c Memory usage grows with corpus size - \u274c No understanding of word relationships</p> <p>Best Use Cases: - Finding unique terms in a file - Document similarity calculations - Corpus-wide keyword analysis - Information retrieval tasks</p>"},{"location":"ARCHITECTURE/#frequency-based-extraction","title":"Frequency-based Extraction","text":"Text Only<pre><code>Final fallback - guaranteed to work\n</code></pre> <p>How it works: - Simple word counting with basic filtering - Extracts n-grams (bigrams, trigrams) - Scores based on component frequency</p> <p>Pros: - \u2705 Always works - no dependencies - \u2705 Minimal memory usage - \u2705 Very fast - \u2705 Predictable behavior - \u2705 Good for debugging</p> <p>Cons: - \u274c Very basic - no intelligence - \u274c Misses context and importance - \u274c Common words can dominate - \u274c No semantic understanding</p> <p>Best Use Cases: - Emergency fallback - Testing and debugging - When other methods fail - Very resource-constrained environments</p>"},{"location":"ARCHITECTURE/#selection-strategy","title":"Selection Strategy","text":"Python<pre><code># Tenets automatic selection logic (simplified)\nif python_version &gt;= 3.13:\n    if rake_available:\n        use_rake()  # Primary choice\n    else:\n        use_bm25()  # Default (or use_tfidf() if configured)\nelse:  # Python &lt; 3.13\n    if rake_available:\n        use_rake()  # Still preferred for speed\n    elif yake_available:\n        use_yake()  # Good alternative\n    else:\n        use_bm25()  # Default (or use_tfidf() if configured)\n\n# Final fallback is always frequency-based\nif all_methods_fail:\n    use_frequency()\n</code></pre>"},{"location":"ARCHITECTURE/#embedding-model-architecture","title":"Embedding Model Architecture","text":"<pre><code>graph LR\n    subgraph \"Model Options\"\n        MINI_L6[all-MiniLM-L6-v2&lt;br/&gt;90MB, Fast]\n        MINI_L12[all-MiniLM-L12-v2&lt;br/&gt;120MB, Better]\n        MPNET[all-mpnet-base-v2&lt;br/&gt;420MB, Best]\n        QA_MINI[multi-qa-MiniLM&lt;br/&gt;Q&amp;A Optimized]\n    end\n\n    subgraph \"Processing Pipeline\"\n        BATCH_ENC[Batch Encoding]\n        CHUNK[Document Chunking&lt;br/&gt;1000 chars, 100 overlap]\n        VECTOR[Vector Operations&lt;br/&gt;NumPy optimized]\n    end\n\n    subgraph \"Cache Strategy\"\n        KEY_GEN[Cache Key Generation&lt;br/&gt;model + content hash]\n        WARM[Cache Warming]\n        INVALID[Intelligent Invalidation]\n    end\n\n    MINI_L6 --&gt; BATCH_ENC\n    MINI_L12 --&gt; BATCH_ENC\n    MPNET --&gt; BATCH_ENC\n    QA_MINI --&gt; BATCH_ENC\n\n    BATCH_ENC --&gt; CHUNK\n    CHUNK --&gt; VECTOR\n\n    VECTOR --&gt; KEY_GEN\n    KEY_GEN --&gt; WARM\n    WARM --&gt; INVALID</code></pre>"},{"location":"ARCHITECTURE/#file-discovery--scanning-system","title":"File Discovery &amp; Scanning System","text":""},{"location":"ARCHITECTURE/#scanner-architecture-flow","title":"Scanner Architecture Flow","text":"<pre><code>graph TD\n    subgraph \"Entry Points\"\n        ROOT[Project Root]\n        PATHS[Specified Paths]\n        PATTERNS[Include Patterns]\n    end\n\n    subgraph \"Ignore System Hierarchy\"\n        CLI_IGNORE[CLI Arguments&lt;br/&gt;--exclude&lt;br/&gt;Highest Priority]\n        TENETS_IGNORE[.tenetsignore&lt;br/&gt;Project-specific]\n        GIT_IGNORE[.gitignore&lt;br/&gt;Version control]\n        GLOBAL_IGNORE[Global Ignores&lt;br/&gt;~/.config/tenets/ignore&lt;br/&gt;Lowest Priority]\n    end\n    subgraph \"Intelligent Test Exclusion\"\n        INTENT_DETECT[Intent Detection&lt;br/&gt;Test-related prompts?]\n        CLI_OVERRIDE[CLI Override&lt;br/&gt;--include-tests / --exclude-tests]\n        TEST_PATTERNS[Test Pattern Matching&lt;br/&gt;Multi-language support]\n        TEST_DIRS[Test Directory Detection&lt;br/&gt;tests/, __tests__, spec/]\n    end\n\n    subgraph \"Minified &amp; Build File Exclusion\"\n        MINIFIED_CHECK[Minified Detection&lt;br/&gt;*.min.js, *.bundle.js]\n        BUILD_DIRS[Build Directories&lt;br/&gt;dist/, build/, out/]\n        PROD_FILES[Production Files&lt;br/&gt;*.prod.js, *.compiled.js]\n        NODE_MODULES[Dependencies&lt;br/&gt;node_modules/, vendor/]\n    end\n\n    subgraph \"Detection Systems\"\n        BINARY_DET[Binary Detection]\n        EXT_CHECK[Extension Check]\n        SIZE_CHECK[Size Check&lt;br/&gt;Max 10MB default]\n        CONTENT_CHECK[Content Sampling&lt;br/&gt;Null byte detection]\n        MAGIC_CHECK[Magic Number&lt;br/&gt;File signatures]\n    end\n\n    subgraph \"Parallel Processing\"\n        WORK_QUEUE[Work Queue]\n        PROCESS_POOL[Process Pool&lt;br/&gt;CPU-bound operations]\n        THREAD_POOL[Thread Pool&lt;br/&gt;I/O operations]\n        PROGRESS[Progress Tracking&lt;br/&gt;tqdm]\n    end\n\n    subgraph \"Output\"\n        SCANNED_FILE[Scanned File Objects]\n        METADATA[File Metadata]\n        ANALYSIS_READY[Ready for Analysis]\n    end\n\n    ROOT --&gt; CLI_IGNORE\n    PATHS --&gt; CLI_IGNORE\n    PATTERNS --&gt; CLI_IGNORE\n\n    CLI_IGNORE --&gt; TENETS_IGNORE\n    TENETS_IGNORE --&gt; GIT_IGNORE\n    GIT_IGNORE --&gt; GLOBAL_IGNORE\n\n    GLOBAL_IGNORE --&gt; BINARY_DET\n    BINARY_DET --&gt; EXT_CHECK\n    EXT_CHECK --&gt; SIZE_CHECK\n    SIZE_CHECK --&gt; CONTENT_CHECK\n    CONTENT_CHECK --&gt; MAGIC_CHECK\n\n    MAGIC_CHECK --&gt; WORK_QUEUE\n    WORK_QUEUE --&gt; PROCESS_POOL\n    WORK_QUEUE --&gt; THREAD_POOL\n    PROCESS_POOL --&gt; PROGRESS\n    THREAD_POOL --&gt; PROGRESS\n\n    PROGRESS --&gt; SCANNED_FILE\n    SCANNED_FILE --&gt; METADATA\n    METADATA --&gt; ANALYSIS_READY</code></pre>"},{"location":"ARCHITECTURE/#binary-detection-strategy","title":"Binary Detection Strategy","text":"<pre><code>flowchart TD\n    FILE[Input File] --&gt; EXT{Known Binary&lt;br/&gt;Extension?}\n    EXT --&gt;|Yes| BINARY[Mark as Binary]\n    EXT --&gt;|No| SIZE{Size &gt; 10MB?}\n    SIZE --&gt;|Yes| SKIP[Skip File]\n    SIZE --&gt;|No| SAMPLE[Sample First 8KB]\n    SAMPLE --&gt; NULL{Contains&lt;br/&gt;Null Bytes?}\n    NULL --&gt;|Yes| BINARY\n    NULL --&gt;|No| RATIO[Calculate Text Ratio]\n    RATIO --&gt; THRESHOLD{Ratio &gt; 95%&lt;br/&gt;Printable?}\n    THRESHOLD --&gt;|Yes| TEXT[Mark as Text]\n    THRESHOLD --&gt;|No| BINARY\n    TEXT --&gt; ANALYZE[Ready for Analysis]\n    BINARY --&gt; IGNORE[Skip Analysis]\n    SKIP --&gt; IGNORE</code></pre>"},{"location":"ARCHITECTURE/#minified--build-file-exclusion","title":"Minified &amp; Build File Exclusion","text":"<p>Tenets automatically excludes minified, compiled, and build output files by default to focus on source code only. This significantly improves analysis speed and context relevance.</p>"},{"location":"ARCHITECTURE/#default-exclusion-patterns","title":"Default Exclusion Patterns","text":"YAML<pre><code>scanner:\n  exclude_minified: true  # Default: exclude minified files\n  minified_patterns:\n    - '*.min.js'          # Minified JavaScript\n    - '*.min.css'         # Minified CSS\n    - '*.bundle.js'       # Webpack bundles\n    - '*.bundle.css'      # CSS bundles\n    - '*.production.js'   # Production builds\n    - '*.prod.js'         # Production builds\n    - '*.dist.js'         # Distribution files\n    - '*.compiled.js'     # Compiled output\n    - '*.minified.*'      # Any minified file\n    - '*.uglified.*'      # UglifyJS output\n  build_directory_patterns:\n    - dist/               # Distribution folder\n    - build/              # Build output\n    - out/                # Output folder\n    - output/             # Alternative output\n    - public/             # Public assets\n    - static/generated/   # Generated statics\n    - .next/              # Next.js build\n    - _next/              # Next.js build\n    - node_modules/       # Dependencies\n</code></pre>"},{"location":"ARCHITECTURE/#configuration-options","title":"Configuration Options","text":"<ol> <li> <p>Disable minified exclusion (include all files):    YAML<pre><code>scanner:\n  exclude_minified: false\n</code></pre></p> </li> <li> <p>Custom patterns:    YAML<pre><code>scanner:\n  minified_patterns:\n    - '*.custom.min.js'\n    - 'vendor/*.js'\n</code></pre></p> </li> <li> <p>CLI override:    Bash<pre><code># Include minified files for this run\ntenets distill \"analyze bundle\" --include-minified\n\n# Exclude specific patterns\ntenets examine . --exclude \"*.min.js,dist/\"\n</code></pre></p> </li> </ol>"},{"location":"ARCHITECTURE/#intelligent-test-file-exclusion","title":"Intelligent Test File Exclusion","text":"<p>Tenets implements intelligent test file handling to improve context relevance by automatically excluding or including test files based on the user's intent.</p> <pre><code>flowchart TD\n    PROMPT[User Prompt] --&gt; PARSE[Prompt Parsing]\n    PARSE --&gt; INTENT{Intent Detection&lt;br/&gt;Test-related?}\n\n    INTENT --&gt;|Yes| INCLUDE_TESTS[include_tests = True]\n    INTENT --&gt;|No| EXCLUDE_TESTS[include_tests = False]\n\n    CLI_OVERRIDE{CLI Override?&lt;br/&gt;--include-tests&lt;br/&gt;--exclude-tests}\n    CLI_OVERRIDE --&gt;|--include-tests| FORCE_INCLUDE[include_tests = True]\n    CLI_OVERRIDE --&gt;|--exclude-tests| FORCE_EXCLUDE[include_tests = False]\n    CLI_OVERRIDE --&gt;|None| INTENT\n\n    INCLUDE_TESTS --&gt; SCAN_ALL[Scan All Files]\n    EXCLUDE_TESTS --&gt; TEST_FILTER[Apply Test Filters]\n    FORCE_INCLUDE --&gt; SCAN_ALL\n    FORCE_EXCLUDE --&gt; TEST_FILTER\n\n    TEST_FILTER --&gt; PATTERN_MATCH[Pattern Matching]\n    PATTERN_MATCH --&gt; DIR_MATCH[Directory Matching]\n\n    subgraph \"Test Patterns (Multi-language)\"\n        PY_PATTERNS[\"Python: test_*.py, *_test.py\"]\n        JS_PATTERNS[\"JavaScript: *.test.js, *.spec.js\"]\n        JAVA_PATTERNS[\"Java: *Test.java, *Tests.java\"]\n        GO_PATTERNS[\"Go: *_test.go\"]\n        GENERIC_PATTERNS[\"Generic: **/test/**, **/tests/**\"]\n    end\n\n    subgraph \"Test Directories\"\n        COMMON_DIRS[\"tests, __tests__, spec\"]\n        LANG_DIRS[\"unit_tests, integration_tests\"]\n        E2E_DIRS[\"e2e, e2e_tests, functional_tests\"]\n    end\n\n    PATTERN_MATCH --&gt; PY_PATTERNS\n    PATTERN_MATCH --&gt; JS_PATTERNS\n    PATTERN_MATCH --&gt; JAVA_PATTERNS\n    PATTERN_MATCH --&gt; GO_PATTERNS\n    PATTERN_MATCH --&gt; GENERIC_PATTERNS\n\n    DIR_MATCH --&gt; COMMON_DIRS\n    DIR_MATCH --&gt; LANG_DIRS\n    DIR_MATCH --&gt; E2E_DIRS\n\n    PY_PATTERNS --&gt; FILTERED_FILES[Filtered File List]\n    JS_PATTERNS --&gt; FILTERED_FILES\n    JAVA_PATTERNS --&gt; FILTERED_FILES\n    GO_PATTERNS --&gt; FILTERED_FILES\n    GENERIC_PATTERNS --&gt; FILTERED_FILES\n\n    COMMON_DIRS --&gt; FILTERED_FILES\n    LANG_DIRS --&gt; FILTERED_FILES\n    E2E_DIRS --&gt; FILTERED_FILES\n\n    SCAN_ALL --&gt; ANALYSIS[File Analysis]\n    FILTERED_FILES --&gt; ANALYSIS</code></pre> <p>Intent Detection Patterns: - Test-related keywords: <code>test</code>, <code>tests</code>, <code>testing</code>, <code>unit</code>, <code>integration</code>, <code>spec</code>, <code>coverage</code> - Test actions: <code>write tests</code>, <code>fix tests</code>, <code>run tests</code>, <code>test coverage</code>, <code>mock</code> - Test files: <code>test_auth.py</code>, <code>auth.test.js</code>, <code>*Test.java</code> - Test frameworks: <code>pytest</code>, <code>jest</code>, <code>mocha</code>, <code>junit</code>, <code>rspec</code></p> <p>Benefits: - Improved Relevance: Non-test prompts get cleaner production code context - Automatic Intelligence: Test prompts automatically include test files - Manual Override: CLI flags provide full control when needed - Multi-language Support: Recognizes test patterns across languages - Configuration: Customizable patterns for project-specific conventions</p>"},{"location":"ARCHITECTURE/#code-analysis-engine","title":"Code Analysis Engine","text":""},{"location":"ARCHITECTURE/#language-analyzer-architecture","title":"Language Analyzer Architecture","text":"<pre><code>graph TB\n    subgraph \"Base Analyzer Interface\"\n        BASE[LanguageAnalyzer&lt;br/&gt;Abstract Base Class]\n        EXTRACT_IMP[extract_imports()]\n        EXTRACT_EXP[extract_exports()]\n        EXTRACT_CLS[extract_classes()]\n        EXTRACT_FN[extract_functions()]\n        CALC_COMP[calculate_complexity()]\n        TRACE_DEP[trace_dependencies()]\n    end\n\n    subgraph \"Language-Specific Analyzers\"\n        PYTHON[Python Analyzer&lt;br/&gt;Full AST parsing]\n        JAVASCRIPT[JavaScript Analyzer&lt;br/&gt;ES6+ support]\n        GOLANG[Go Analyzer&lt;br/&gt;Package detection]\n        JAVA[Java Analyzer&lt;br/&gt;OOP patterns]\n        RUST[Rust Analyzer&lt;br/&gt;Ownership patterns]\n        GENERIC[Generic Analyzer&lt;br/&gt;Pattern-based fallback]\n    end\n\n    subgraph \"Analysis Features\"\n        AST[AST Parsing]\n        IMPORTS[Import Resolution]\n        TYPES[Type Extraction]\n        DOCS[Documentation Parsing]\n        PATTERNS[Code Patterns]\n        COMPLEXITY[Complexity Metrics]\n    end\n\n    BASE --&gt; EXTRACT_IMP\n    BASE --&gt; EXTRACT_EXP\n    BASE --&gt; EXTRACT_CLS\n    BASE --&gt; EXTRACT_FN\n    BASE --&gt; CALC_COMP\n    BASE --&gt; TRACE_DEP\n\n    BASE --&gt; PYTHON\n    BASE --&gt; JAVASCRIPT\n    BASE --&gt; GOLANG\n    BASE --&gt; JAVA\n    BASE --&gt; RUST\n    BASE --&gt; GENERIC\n\n    PYTHON --&gt; AST\n    PYTHON --&gt; IMPORTS\n    PYTHON --&gt; TYPES\n    PYTHON --&gt; DOCS\n\n    JAVASCRIPT --&gt; PATTERNS\n    GOLANG --&gt; PATTERNS\n    JAVA --&gt; COMPLEXITY\n    RUST --&gt; COMPLEXITY\n    GENERIC --&gt; PATTERNS</code></pre>"},{"location":"ARCHITECTURE/#python-analyzer-detail","title":"Python Analyzer Detail","text":"<pre><code>graph LR\n    subgraph \"Python AST Analysis\"\n        AST_PARSE[AST Parser]\n        NODE_VISIT[Node Visitor]\n        SYMBOL_TABLE[Symbol Table]\n    end\n\n    subgraph \"Code Structure\"\n        CLASSES[Class Definitions&lt;br/&gt;Inheritance chains]\n        FUNCTIONS[Function Definitions&lt;br/&gt;Async detection]\n        DECORATORS[Decorator Analysis]\n        TYPE_HINTS[Type Hint Extraction]\n    end\n\n    subgraph \"Import Analysis\"\n        ABS_IMP[Absolute Imports]\n        REL_IMP[Relative Imports]\n        STAR_IMP[Star Imports]\n        IMPORT_GRAPH[Import Graph Building]\n    end\n\n    subgraph \"Complexity Metrics\"\n        CYCLO[Cyclomatic Complexity&lt;br/&gt;+1 for if, for, while]\n        COGNITIVE[Cognitive Complexity&lt;br/&gt;Nesting penalties]\n        HALSTEAD[Halstead Metrics&lt;br/&gt;Operators/operands]\n    end\n\n    AST_PARSE --&gt; NODE_VISIT\n    NODE_VISIT --&gt; SYMBOL_TABLE\n\n    SYMBOL_TABLE --&gt; CLASSES\n    SYMBOL_TABLE --&gt; FUNCTIONS\n    SYMBOL_TABLE --&gt; DECORATORS\n    SYMBOL_TABLE --&gt; TYPE_HINTS\n\n    NODE_VISIT --&gt; ABS_IMP\n    NODE_VISIT --&gt; REL_IMP\n    NODE_VISIT --&gt; STAR_IMP\n    ABS_IMP --&gt; IMPORT_GRAPH\n    REL_IMP --&gt; IMPORT_GRAPH\n    STAR_IMP --&gt; IMPORT_GRAPH\n\n    SYMBOL_TABLE --&gt; CYCLO\n    SYMBOL_TABLE --&gt; COGNITIVE\n    SYMBOL_TABLE --&gt; HALSTEAD</code></pre>"},{"location":"ARCHITECTURE/#relevance-ranking-system","title":"Relevance Ranking System","text":""},{"location":"ARCHITECTURE/#unified-ranking-architecture","title":"Unified Ranking Architecture","text":"<p>IMPORTANT: The <code>rank</code> command now uses the EXACT SAME sophisticated ranking pipeline as the <code>distill</code> command. This ensures consistency and leverages the full power of the multi-factor ranking system.</p> <pre><code>graph TD\n    subgraph \"Ranking Strategies\"\n        FAST[Fast Strategy&lt;br/&gt;Fastest&lt;br/&gt;Keyword + Path Only]\n        BALANCED[Balanced Strategy&lt;br/&gt;1.5x slower&lt;br/&gt;BM25 + Structure]\n        THOROUGH[Thorough Strategy&lt;br/&gt;4x slower&lt;br/&gt;Full Analysis + ML]\n        ML_STRAT[ML Strategy&lt;br/&gt;5x slower&lt;br/&gt;Semantic Embeddings]\n    end\n\n    subgraph \"Text Analysis (40% in Balanced)\"\n        KEY_MATCH[Keyword Matching&lt;br/&gt;20%&lt;br/&gt;Direct term hits]\n        BM25_SIM[BM25 Similarity&lt;br/&gt;20%&lt;br/&gt;Statistical relevance]\n        BM25_SCORE[BM25 Score&lt;br/&gt;15%&lt;br/&gt;Probabilistic ranking]\n    end\n\n    subgraph \"Code Structure Analysis (25% in Balanced)\"\n        PATH_REL[Path Relevance&lt;br/&gt;15%&lt;br/&gt;Directory structure]\n        IMP_CENT[Import Centrality&lt;br/&gt;10%&lt;br/&gt;Dependency importance]\n    end\n\n    subgraph \"File Characteristics (15% in Balanced)\"\n        COMPLEXITY_REL[Complexity Relevance&lt;br/&gt;5%&lt;br/&gt;Code complexity signals]\n        FILE_TYPE[File Type Relevance&lt;br/&gt;5%&lt;br/&gt;Extension/type matching]\n        CODE_PAT[Code Patterns&lt;br/&gt;5%&lt;br/&gt;AST pattern matching]\n    end\n\n    subgraph \"Git Signals (10% in Balanced)\"\n        GIT_REC[Git Recency&lt;br/&gt;5%&lt;br/&gt;Recent changes]\n        GIT_FREQ[Git Frequency&lt;br/&gt;5%&lt;br/&gt;Change frequency]\n    end\n\n    subgraph \"ML Enhancement (Only in ML Strategy)\"\n        SEM_SIM[Semantic Similarity&lt;br/&gt;25%&lt;br/&gt;Embedding-based understanding]\n        LOCAL_EMB[Local Embeddings&lt;br/&gt;sentence-transformers]\n        EMBED_CACHE[Embedding Cache&lt;br/&gt;Performance optimization]\n    end\n\n    subgraph \"Unified Pipeline\"\n        FILE_DISCOVERY[File Discovery&lt;br/&gt;Scanner + Filters]\n        ANALYSIS[Code Analysis&lt;br/&gt;AST + Structure]\n        RANKING[Multi-Factor Ranking&lt;br/&gt;Strategy-specific weights]\n        AGGREGATION[Context Aggregation&lt;br/&gt;Token optimization]\n    end\n\n    FAST --&gt; KEY_MATCH\n    BALANCED --&gt; BM25_SIM\n    BALANCED --&gt; BM25_SCORE\n    THOROUGH --&gt; IMP_CENT\n    ML_STRAT --&gt; SEM_SIM\n\n    FILE_DISCOVERY --&gt; ANALYSIS\n    ANALYSIS --&gt; RANKING\n    RANKING --&gt; AGGREGATION\n\n    KEY_MATCH --&gt; RANKING\n    BM25_SIM --&gt; RANKING\n    BM25_SCORE --&gt; RANKING\n    PATH_REL --&gt; RANKING\n    IMP_CENT --&gt; RANKING\n    COMPLEXITY_REL --&gt; RANKING\n    FILE_TYPE --&gt; RANKING\n    CODE_PAT --&gt; RANKING\n    GIT_REC --&gt; RANKING\n    GIT_FREQ --&gt; RANKING\n\n    SEM_SIM --&gt; LOCAL_EMB\n    LOCAL_EMB --&gt; EMBED_CACHE\n    EMBED_CACHE --&gt; RANKING</code></pre>"},{"location":"ARCHITECTURE/#strategy-comparison-and-usage","title":"Strategy Comparison and Usage","text":"Strategy Speed Accuracy Use Cases Factors Used Fast Fastest Basic \u2022 Quick file discovery\u2022 Keyword-based search\u2022 Interactive exploration \u2022 Keyword matching (60%)\u2022 Path relevance (30%)\u2022 File type (10%) Balanced 1.5x slower Good \u2022 DEFAULT for both rank and distill\u2022 Production usage\u2022 Most common scenarios \u2022 Keyword (20%), BM25 (35%)\u2022 Path (15%), Import centrality (10%)\u2022 Complexity (5%), File type (5%), Git (10%) Thorough 4x slower High \u2022 Complex codebases\u2022 Deep analysis needed\u2022 Research and investigation \u2022 All balanced factors\u2022 Enhanced git analysis\u2022 Deeper structural analysis ML 5x slower Highest \u2022 Semantic understanding needed\u2022 Natural language queries\u2022 Advanced AI workflows \u2022 All factors + semantic similarity (25%)\u2022 Local embedding models\u2022 Context-aware ranking"},{"location":"ARCHITECTURE/#modular-ranking-architecture","title":"Modular Ranking Architecture","text":"<p>The ranking system is designed as a fully modular component that can be used independently or as part of the larger distillation pipeline.</p>"},{"location":"ARCHITECTURE/#component-architecture","title":"Component Architecture","text":"Python<pre><code># Core Components and Their Responsibilities\ntenets/core/ranking/\n\u251c\u2500\u2500 __init__.py         # Public API exports\n\u251c\u2500\u2500 ranker.py           # RelevanceRanker class - main ranking engine\n\u251c\u2500\u2500 strategies.py       # Ranking strategies (Fast, Balanced, Thorough, ML)\n\u2514\u2500\u2500 factors.py          # Individual ranking factor calculations\n\n# Integration Points\ntenets/core/distiller/distiller.py\n\u251c\u2500\u2500 __init__: self.ranker = RelevanceRanker(config)  # Component instantiation\n\u2514\u2500\u2500 _rank_files(): return self.ranker.rank_files()   # Delegation to ranker\n\ntenets/__init__.py (Tenets class)\n\u251c\u2500\u2500 rank_files(): Uses distiller._rank_files()       # Reuses same pipeline\n\u2514\u2500\u2500 distill(): Uses distiller._rank_files()          # Consistent ranking\n</code></pre>"},{"location":"ARCHITECTURE/#key-design-principles","title":"Key Design Principles","text":"<ol> <li>Single Source of Truth: The <code>RelevanceRanker</code> class is the sole authority for ranking logic</li> <li>Strategy Pattern: Different ranking strategies (fast/balanced/thorough) are encapsulated</li> <li>Dependency Injection: Ranker is injected into Distiller, not hardcoded</li> <li>Interface Consistency: Both <code>rank</code> and <code>distill</code> commands use identical ranking</li> </ol>"},{"location":"ARCHITECTURE/#benefits-of-modular-design","title":"Benefits of Modular Design","text":"<ul> <li>Consistency: Same ranking behavior across all commands</li> <li>Testability: Ranker can be tested in isolation</li> <li>Extensibility: New ranking strategies can be added without changing core logic</li> <li>Reusability: Other tools can import and use the ranker independently</li> <li>Maintainability: Changes to ranking logic happen in one place</li> <li>Analyzes all discovered files (no artificial limits)</li> <li>Uses proper prompt parsing with <code>PromptParser</code></li> <li>Leverages sophisticated <code>BalancedRankingStrategy</code> by default</li> <li>Proper keyword extraction with RAKE/YAKE fallbacks</li> <li>Same file discovery and filtering as <code>distill</code></li> </ul>"},{"location":"ARCHITECTURE/#pipeline-consistency","title":"Pipeline Consistency","text":"<pre><code>graph LR\n    subgraph \"Unified Pipeline Components\"\n        PROMPT_PARSER[Prompt Parser&lt;br/&gt;Intent detection&lt;br/&gt;Keyword extraction]\n        FILE_SCANNER[File Scanner&lt;br/&gt;Gitignore support&lt;br/&gt;Test exclusion]\n        CODE_ANALYZER[Code Analyzer&lt;br/&gt;AST parsing&lt;br/&gt;Structure analysis]\n        RELEVANCE_RANKER[Relevance Ranker&lt;br/&gt;Multi-factor scoring&lt;br/&gt;Strategy selection]\n    end\n\n    subgraph \"Commands Using Same Pipeline\"\n        DISTILL_CMD[tenets distill]\n        RANK_CMD[tenets rank]\n    end\n\n    DISTILL_CMD --&gt; PROMPT_PARSER\n    RANK_CMD --&gt; PROMPT_PARSER\n\n    PROMPT_PARSER --&gt; FILE_SCANNER\n    FILE_SCANNER --&gt; CODE_ANALYZER\n    CODE_ANALYZER --&gt; RELEVANCE_RANKER\n\n    RELEVANCE_RANKER --&gt; CONTEXT_BUILDER[Context Builder&lt;br/&gt;Only for distill]</code></pre>"},{"location":"ARCHITECTURE/#why-this-matters","title":"Why This Matters","text":"<ol> <li>Consistency: Both commands use identical logic for finding and ranking relevant files</li> <li>Performance: Leverages sophisticated caching and optimization from the main pipeline</li> <li>Accuracy: Uses proper NLP and multi-factor analysis instead of simple keyword matching</li> <li>Maintainability: Single source of truth for ranking logic - no code duplication</li> <li>Feature Parity: <code>rank</code> gets all improvements made to the <code>distill</code> ranking system</li> </ol>"},{"location":"ARCHITECTURE/#factor-calculation-details","title":"Factor Calculation Details","text":"<pre><code>graph LR\n    subgraph \"Semantic Similarity Calculation\"\n        CHUNK[Chunk Long Files&lt;br/&gt;1000 chars, 100 overlap]\n        EMBED[Generate Embeddings&lt;br/&gt;Local model]\n        COSINE[Cosine Similarity]\n        CACHE_SEM[Cache Results]\n    end\n\n    subgraph \"Keyword Matching\"\n        FILENAME[Filename Match&lt;br/&gt;Weight: 0.4]\n        IMPORT_M[Import Match&lt;br/&gt;Weight: 0.3]\n        CLASS_FN[Class/Function Name&lt;br/&gt;Weight: 0.25]\n        POSITION[Position Weight&lt;br/&gt;Early lines favored]\n    end\n\n    subgraph \"Import Centrality\"\n        IN_EDGES[Incoming Edges&lt;br/&gt;Files importing this&lt;br/&gt;70% weight]\n        OUT_EDGES[Outgoing Edges&lt;br/&gt;Files this imports&lt;br/&gt;30% weight]\n        LOG_SCALE[Logarithmic Scaling&lt;br/&gt;High-degree nodes]\n        NORMALIZE[Normalize 0-1]\n    end\n\n    subgraph \"Git Signals\"\n        RECENCY[Recency Score&lt;br/&gt;Exponential decay&lt;br/&gt;30-day half-life]\n        FREQUENCY[Frequency Score&lt;br/&gt;Log of commit count]\n        EXPERTISE[Author Expertise&lt;br/&gt;Contribution volume]\n        CHURN[Recent Churn&lt;br/&gt;Lines changed]\n    end\n\n    CHUNK --&gt; EMBED\n    EMBED --&gt; COSINE\n    COSINE --&gt; CACHE_SEM\n\n    FILENAME --&gt; POSITION\n    IMPORT_M --&gt; POSITION\n    CLASS_FN --&gt; POSITION\n\n    IN_EDGES --&gt; LOG_SCALE\n    OUT_EDGES --&gt; LOG_SCALE\n    LOG_SCALE --&gt; NORMALIZE\n\n    RECENCY --&gt; EXPERTISE\n    FREQUENCY --&gt; EXPERTISE\n    EXPERTISE --&gt; CHURN</code></pre>"},{"location":"ARCHITECTURE/#git-integration--chronicle-system","title":"Git Integration &amp; Chronicle System","text":""},{"location":"ARCHITECTURE/#git-analysis-architecture","title":"Git Analysis Architecture","text":"<pre><code>graph TD\n    subgraph \"Git Data Sources\"\n        COMMIT_LOG[Commit History]\n        BLAME_DATA[Blame Information]\n        BRANCH_INFO[Branch Analysis]\n        MERGE_DATA[Merge Detection]\n        CONFLICT_HIST[Conflict History]\n    end\n\n    subgraph \"Chronicle Analysis\"\n        TEMPORAL[Temporal Analysis&lt;br/&gt;Activity patterns]\n        CONTRIBUTORS[Contributor Tracking&lt;br/&gt;Author patterns]\n        VELOCITY[Change Velocity&lt;br/&gt;Trend analysis]\n        HOTSPOTS[Change Hotspots&lt;br/&gt;Problem areas]\n    end\n\n    subgraph \"Metrics Calculation\"\n        BUS_FACTOR[Bus Factor&lt;br/&gt;Knowledge concentration]\n        EXPERTISE[Author Expertise&lt;br/&gt;Domain knowledge]\n        FRESHNESS[Code Freshness&lt;br/&gt;Age distribution]\n        STABILITY[Change Stability&lt;br/&gt;Frequency patterns]\n    end\n\n    subgraph \"Risk Assessment\"\n        KNOWLEDGE_RISK[Knowledge Risk&lt;br/&gt;Single points of failure]\n        CHURN_RISK[Churn Risk&lt;br/&gt;High-change areas]\n        COMPLEXITY_RISK[Complexity Risk&lt;br/&gt;Hard-to-maintain code]\n        SUCCESSION[Succession Planning&lt;br/&gt;Knowledge transfer]\n    end\n\n    COMMIT_LOG --&gt; TEMPORAL\n    BLAME_DATA --&gt; CONTRIBUTORS\n    BRANCH_INFO --&gt; VELOCITY\n    MERGE_DATA --&gt; HOTSPOTS\n    CONFLICT_HIST --&gt; HOTSPOTS\n\n    CONTRIBUTORS --&gt; BUS_FACTOR\n    TEMPORAL --&gt; EXPERTISE\n    VELOCITY --&gt; FRESHNESS\n    HOTSPOTS --&gt; STABILITY\n\n    BUS_FACTOR --&gt; KNOWLEDGE_RISK\n    EXPERTISE --&gt; CHURN_RISK\n    FRESHNESS --&gt; COMPLEXITY_RISK\n    STABILITY --&gt; SUCCESSION</code></pre>"},{"location":"ARCHITECTURE/#chronicle-report-structure","title":"Chronicle Report Structure","text":"<pre><code>graph LR\n    subgraph \"Executive Summary\"\n        HEALTH[Repository Health Score]\n        KEY_METRICS[Key Metrics Dashboard]\n        ALERTS[Risk Alerts]\n    end\n\n    subgraph \"Activity Analysis\"\n        TIMELINE[Activity Timeline]\n        PATTERNS[Change Patterns]\n        TRENDS[Velocity Trends]\n    end\n\n    subgraph \"Contributor Analysis\"\n        TEAM[Team Composition]\n        EXPERTISE_MAP[Expertise Mapping]\n        CONTRIBUTION[Contribution Patterns]\n    end\n\n    subgraph \"Risk Assessment\"\n        RISKS[Identified Risks]\n        RECOMMENDATIONS[Recommendations]\n        ACTION_ITEMS[Action Items]\n    end\n\n    HEALTH --&gt; TIMELINE\n    KEY_METRICS --&gt; PATTERNS\n    ALERTS --&gt; TRENDS\n\n    TIMELINE --&gt; TEAM\n    PATTERNS --&gt; EXPERTISE_MAP\n    TRENDS --&gt; CONTRIBUTION\n\n    TEAM --&gt; RISKS\n    EXPERTISE_MAP --&gt; RECOMMENDATIONS\n    CONTRIBUTION --&gt; ACTION_ITEMS</code></pre>"},{"location":"ARCHITECTURE/#context-management--optimization","title":"Context Management &amp; Optimization","text":""},{"location":"ARCHITECTURE/#context-building-pipeline","title":"Context Building Pipeline","text":"<pre><code>graph TD\n    subgraph \"Input Processing\"\n        RANKED_FILES[Ranked File Results]\n        TOKEN_BUDGET[Available Token Budget]\n        USER_PREFS[User Preferences]\n    end\n\n    subgraph \"Selection Strategy\"\n        THRESHOLD[Score Threshold Filtering]\n        TOP_N[Top-N Selection]\n        DIVERSITY[Diversity Optimization]\n        DEPENDENCIES[Dependency Inclusion]\n    end\n\n    subgraph \"Token Management\"\n        MODEL_LIMITS[Model-Specific Limits&lt;br/&gt;4K, 8K, 16K, 32K, 100K]\n        PROMPT_RESERVE[Prompt Token Reserve]\n        RESPONSE_RESERVE[Response Token Reserve&lt;br/&gt;2K-4K]\n        SAFETY_MARGIN[Safety Margin&lt;br/&gt;5% buffer]\n    end\n\n    subgraph \"Content Optimization\"\n        SUMMARIZATION[Summarization Strategy]\n        EXTRACTION[Key Component Extraction]\n        COMPRESSION[Content Compression]\n        FORMATTING[Output Formatting]\n    end\n\n    subgraph \"Quality Assurance\"\n        COHERENCE[Context Coherence Check]\n        COMPLETENESS[Completeness Validation]\n        RELEVANCE[Relevance Verification]\n        FINAL_OUTPUT[Final Context Output]\n    end\n\n    RANKED_FILES --&gt; THRESHOLD\n    TOKEN_BUDGET --&gt; MODEL_LIMITS\n    USER_PREFS --&gt; TOP_N\n\n    THRESHOLD --&gt; TOP_N\n    TOP_N --&gt; DIVERSITY\n    DIVERSITY --&gt; DEPENDENCIES\n\n    MODEL_LIMITS --&gt; PROMPT_RESERVE\n    PROMPT_RESERVE --&gt; RESPONSE_RESERVE\n    RESPONSE_RESERVE --&gt; SAFETY_MARGIN\n\n    DEPENDENCIES --&gt; SUMMARIZATION\n    SAFETY_MARGIN --&gt; SUMMARIZATION\n    SUMMARIZATION --&gt; EXTRACTION\n    EXTRACTION --&gt; COMPRESSION\n    COMPRESSION --&gt; FORMATTING\n\n    FORMATTING --&gt; COHERENCE\n    COHERENCE --&gt; COMPLETENESS\n    COMPLETENESS --&gt; RELEVANCE\n    RELEVANCE --&gt; FINAL_OUTPUT</code></pre>"},{"location":"ARCHITECTURE/#summarization-strategies","title":"Summarization Strategies","text":""},{"location":"ARCHITECTURE/#import-summarization-new","title":"Import Summarization (NEW)","text":"<p>Tenets now provides intelligent import condensing to reduce token usage while preserving context:</p> <p>How It Works: 1. Detects import statements across multiple programming languages 2. Extracts library/package names from various import formats 3. Groups and counts imports (external vs local) 4. Produces human-readable summary when threshold exceeded</p> <p>Example Transformation: Python<pre><code># Original (15+ lines):\nimport os\nimport sys\nfrom pathlib import Path\nfrom typing import Dict, List, Optional\nimport numpy as np\nimport pandas as pd\nfrom flask import Flask, request\n# ... more imports\n\n# Summarized (3 lines):\n# Imports: 15 total\n# Dependencies: flask, numpy, pandas, pathlib, typing\n# Local imports: 2\n</code></pre></p> <p>Configuration: YAML<pre><code>summarizer:\n  summarize_imports: true  # Enable/disable\n  import_summary_threshold: 5  # Minimum imports to trigger\n</code></pre></p> <p>Supported Languages: - Python: <code>import X</code>, <code>from X import Y</code> - JavaScript/TypeScript: <code>import</code>, <code>require()</code> - Java: <code>import package.Class</code> - C/C++: <code>#include &lt;header&gt;</code> - Go: <code>import \"package\"</code> - Rust: <code>use crate::module</code></p> <pre><code>graph LR\n    subgraph \"Extraction Strategy\"\n        IMPORTS_EX[Import Summarization&lt;br/&gt;Condenses when &gt; threshold]\n        SIGNATURES[Function/Class Signatures&lt;br/&gt;High priority]\n        DOCSTRINGS[Docstrings/Comments&lt;br/&gt;Documentation]\n        TYPES[Type Definitions&lt;br/&gt;Interface contracts]\n    end\n\n    subgraph \"Compression Strategy\"\n        REDUNDANCY[Remove Redundancy&lt;br/&gt;Duplicate code]\n        WHITESPACE[Normalize Whitespace&lt;br/&gt;Consistent formatting]\n        COMMENTS[Condense Comments&lt;br/&gt;Key information only]\n        BOILERPLATE[Remove Boilerplate&lt;br/&gt;Standard patterns]\n    end\n\n    subgraph \"Semantic Strategy\"\n        MEANING[Preserve Meaning&lt;br/&gt;Core logic intact]\n        CONTEXT[Maintain Context&lt;br/&gt;Relationship preservation]\n        ABSTRACTIONS[Higher-level View&lt;br/&gt;Architectural overview]\n        EXAMPLES[Key Examples&lt;br/&gt;Usage patterns]\n    end\n\n    subgraph \"LLM Strategy (Optional)\"\n        EXTERNAL_API[External LLM API&lt;br/&gt;OpenAI/Anthropic]\n        INTELLIGENT[Intelligent Summarization&lt;br/&gt;Context-aware]\n        CONSENT[User Consent Required&lt;br/&gt;Privacy protection]\n        FALLBACK[Fallback to Local&lt;br/&gt;If API unavailable]\n    end\n\n    IMPORTS_EX --&gt; REDUNDANCY\n    SIGNATURES --&gt; WHITESPACE\n    DOCSTRINGS --&gt; COMMENTS\n    TYPES --&gt; BOILERPLATE\n\n    REDUNDANCY --&gt; MEANING\n    WHITESPACE --&gt; CONTEXT\n    COMMENTS --&gt; ABSTRACTIONS\n    BOILERPLATE --&gt; EXAMPLES\n\n    MEANING --&gt; EXTERNAL_API\n    CONTEXT --&gt; INTELLIGENT\n    ABSTRACTIONS --&gt; CONSENT\n    EXAMPLES --&gt; FALLBACK</code></pre>"},{"location":"ARCHITECTURE/#session-management-architecture","title":"Session Management Architecture","text":""},{"location":"ARCHITECTURE/#session-lifecycle-flow","title":"Session Lifecycle Flow","text":"<pre><code>stateDiagram-v2\n    [*] --&gt; Created\n    Created --&gt; FirstPrompt: User provides initial prompt\n    FirstPrompt --&gt; Analyzing: Full codebase analysis\n    Analyzing --&gt; Active: Context built\n    Active --&gt; Interaction: Subsequent prompts\n    Interaction --&gt; Analyzing: Incremental updates\n    Interaction --&gt; Branching: Alternative exploration\n    Branching --&gt; Active: Branch selected\n    Active --&gt; Export: Save for sharing\n    Export --&gt; Archived: Long-term storage\n    Archived --&gt; [*]\n    Active --&gt; [*]: Session ends\n\n    note right of FirstPrompt\n        - Comprehensive analysis\n        - All relevant files\n        - Setup instructions\n        - AI guidance\n    end note\n\n    note right of Interaction\n        - Incremental updates only\n        - Changed files highlighted\n        - Previous context referenced\n        - Minimal redundancy\n    end note</code></pre>"},{"location":"ARCHITECTURE/#session-storage-architecture","title":"Session Storage Architecture","text":"<pre><code>graph TB\n    subgraph \"Session Tables\"\n        SESSIONS[sessions&lt;br/&gt;id, name, project, created, updated]\n        PROMPTS[prompts&lt;br/&gt;id, session_id, text, timestamp]\n        CONTEXTS[contexts&lt;br/&gt;id, session_id, prompt_id, content]\n        FILE_STATES[file_states&lt;br/&gt;session_id, file_path, state]\n        AI_REQUESTS[ai_requests&lt;br/&gt;id, session_id, type, request]\n    end\n\n    subgraph \"Relationships\"\n        SESSION_PROMPT[Session \u2192 Prompts&lt;br/&gt;One-to-Many]\n        PROMPT_CONTEXT[Prompt \u2192 Context&lt;br/&gt;One-to-One]\n        SESSION_FILES[Session \u2192 File States&lt;br/&gt;One-to-Many]\n        SESSION_AI[Session \u2192 AI Requests&lt;br/&gt;One-to-Many]\n    end\n\n    subgraph \"Operations\"\n        CREATE[Create Session]\n        SAVE[Save State]\n        RESTORE[Restore State]\n        BRANCH[Branch Session]\n        MERGE[Merge Sessions]\n        EXPORT[Export Session]\n    end\n\n    SESSIONS --&gt; SESSION_PROMPT\n    SESSIONS --&gt; SESSION_FILES\n    SESSIONS --&gt; SESSION_AI\n    PROMPTS --&gt; PROMPT_CONTEXT\n\n    SESSION_PROMPT --&gt; CREATE\n    PROMPT_CONTEXT --&gt; SAVE\n    SESSION_FILES --&gt; RESTORE\n    SESSION_AI --&gt; BRANCH\n    CREATE --&gt; MERGE\n    SAVE --&gt; EXPORT</code></pre>"},{"location":"ARCHITECTURE/#storage--caching-architecture","title":"Storage &amp; Caching Architecture","text":""},{"location":"ARCHITECTURE/#storage-hierarchy","title":"Storage Hierarchy","text":"<pre><code>graph TB\n    subgraph \"Memory Cache (Hottest)\"\n        LRU[LRU Cache&lt;br/&gt;1000 items default&lt;br/&gt;Sub-millisecond access]\n        HOT_DATA[Frequently accessed data&lt;br/&gt;Recent analyses&lt;br/&gt;Active embeddings]\n    end\n\n    subgraph \"SQLite Database (Structured)\"\n        SESSIONS_DB[Session Storage&lt;br/&gt;User interactions]\n        CONFIG_DB[Configuration&lt;br/&gt;Settings &amp; preferences]\n        RELATIONS[Relationship data&lt;br/&gt;File dependencies]\n        PERF[1-10ms access time]\n    end\n\n    subgraph \"Disk Cache (Bulk)\"\n        ANALYSIS[Analysis Results&lt;br/&gt;File parsing cache]\n        EMBEDDINGS[Embedding Cache&lt;br/&gt;ML vectors]\n        FILE_CONTENT[File Content Cache&lt;br/&gt;Preprocessed data]\n        BULK_PERF[10-100ms access time]\n    end\n\n    subgraph \"File System (Cold)\"\n        LOGS[Application Logs&lt;br/&gt;Debugging information]\n        EXPORTS[Exported Sessions&lt;br/&gt;Sharing &amp; backup]\n        ARCHIVES[Archived Data&lt;br/&gt;Historical sessions]\n        COLD_PERF[100ms+ access time]\n    end\n\n    LRU --&gt; SESSIONS_DB\n    HOT_DATA --&gt; CONFIG_DB\n\n    SESSIONS_DB --&gt; ANALYSIS\n    CONFIG_DB --&gt; EMBEDDINGS\n    RELATIONS --&gt; FILE_CONTENT\n\n    ANALYSIS --&gt; LOGS\n    EMBEDDINGS --&gt; EXPORTS\n    FILE_CONTENT --&gt; ARCHIVES</code></pre>"},{"location":"ARCHITECTURE/#cache-invalidation-strategy","title":"Cache Invalidation Strategy","text":"<pre><code>graph LR\n    subgraph \"Invalidation Triggers\"\n        FILE_MTIME[File Modification Time&lt;br/&gt;Filesystem change]\n        CONTENT_HASH[Content Hash Change&lt;br/&gt;Actual content differs]\n        GIT_COMMIT[Git Commit&lt;br/&gt;Version control change]\n        DEP_CHANGE[Dependency Change&lt;br/&gt;Import graph update]\n        TTL_EXPIRE[TTL Expiration&lt;br/&gt;Time-based cleanup]\n        MANUAL[Manual Refresh&lt;br/&gt;User-initiated]\n    end\n\n    subgraph \"Cache Levels Affected\"\n        MEMORY_INV[Memory Cache&lt;br/&gt;Immediate eviction]\n        SQLITE_INV[SQLite Cache&lt;br/&gt;Mark as stale]\n        DISK_INV[Disk Cache&lt;br/&gt;File removal]\n        CASCADE[Cascade Invalidation&lt;br/&gt;Dependent entries]\n    end\n\n    subgraph \"Rebuilding Strategy\"\n        LAZY[Lazy Rebuilding&lt;br/&gt;On-demand refresh]\n        EAGER[Eager Rebuilding&lt;br/&gt;Background refresh]\n        PARTIAL[Partial Rebuilding&lt;br/&gt;Incremental updates]\n        BATCH[Batch Rebuilding&lt;br/&gt;Multiple files]\n    end\n\n    FILE_MTIME --&gt; MEMORY_INV\n    CONTENT_HASH --&gt; SQLITE_INV\n    GIT_COMMIT --&gt; DISK_INV\n    DEP_CHANGE --&gt; CASCADE\n    TTL_EXPIRE --&gt; CASCADE\n    MANUAL --&gt; CASCADE\n\n    MEMORY_INV --&gt; LAZY\n    SQLITE_INV --&gt; EAGER\n    DISK_INV --&gt; PARTIAL\n    CASCADE --&gt; BATCH</code></pre>"},{"location":"ARCHITECTURE/#performance-architecture","title":"Performance Architecture","text":""},{"location":"ARCHITECTURE/#optimization-strategy-overview","title":"Optimization Strategy Overview","text":"<pre><code>graph TD\n    subgraph \"Parallel Processing\"\n        FILE_SCAN[File Scanning&lt;br/&gt;Process Pool&lt;br/&gt;CPU-bound operations]\n        ANALYSIS[Code Analysis&lt;br/&gt;Thread Pool&lt;br/&gt;I/O operations]\n        RANKING[Relevance Ranking&lt;br/&gt;Thread Pool&lt;br/&gt;Computation]\n        EMBEDDING[Embedding Generation&lt;br/&gt;Batch Processing&lt;br/&gt;GPU if available]\n    end\n\n    subgraph \"Streaming Architecture\"\n        INCREMENTAL[Incremental Discovery&lt;br/&gt;Stream files as found]\n        PROGRESSIVE[Progressive Ranking&lt;br/&gt;Rank as analyzed]\n        CHUNKED[Chunked Analysis&lt;br/&gt;Process in batches]\n        STREAMING[Result Streaming&lt;br/&gt;First results quickly]\n    end\n\n    subgraph \"Lazy Evaluation\"\n        DEFER[Defer Analysis&lt;br/&gt;Until needed]\n        ON_DEMAND[On-demand Embeddings&lt;br/&gt;Generate when required]\n        PROGRESSIVE_ENH[Progressive Enhancement&lt;br/&gt;Add features incrementally]\n        JIT[Just-in-time Compilation&lt;br/&gt;Optimize hot paths]\n    end\n\n    subgraph \"Memory Management\"\n        STREAMING_PROC[Streaming Processing&lt;br/&gt;Constant memory usage]\n        GC[Incremental GC&lt;br/&gt;Prevent pauses]\n        MMAP[Memory-mapped Files&lt;br/&gt;Large file handling]\n        PRESSURE[Memory Pressure Monitor&lt;br/&gt;Adaptive behavior]\n    end\n\n    FILE_SCAN --&gt; INCREMENTAL\n    ANALYSIS --&gt; PROGRESSIVE\n    RANKING --&gt; CHUNKED\n    EMBEDDING --&gt; STREAMING\n\n    INCREMENTAL --&gt; DEFER\n    PROGRESSIVE --&gt; ON_DEMAND\n    CHUNKED --&gt; PROGRESSIVE_ENH\n    STREAMING --&gt; JIT\n\n    DEFER --&gt; STREAMING_PROC\n    ON_DEMAND --&gt; GC\n    PROGRESSIVE_ENH --&gt; MMAP\n    JIT --&gt; PRESSURE</code></pre>"},{"location":"ARCHITECTURE/#performance-benchmarks","title":"Performance Benchmarks","text":""},{"location":"ARCHITECTURE/#file-analysis-performance","title":"File Analysis Performance","text":"<p>Performance benchmarks coming soon</p> <p>We're currently collecting comprehensive performance data across different file sizes and languages. Check back for detailed metrics.</p>"},{"location":"ARCHITECTURE/#system-performance","title":"System Performance","text":"Codebase Files Size Analysis Speed Memory Usage Small &lt;100 &lt;10MB Fast Low Medium ~1K ~50MB Fast Low Large ~10K ~500MB Moderate Moderate Huge ~100K ~5GB Slower High Monorepo 1M+ 50GB+ Variable High"},{"location":"ARCHITECTURE/#configuration-system","title":"Configuration System","text":""},{"location":"ARCHITECTURE/#configuration-hierarchy","title":"Configuration Hierarchy","text":"<pre><code>graph TB\n    subgraph \"Configuration Sources (Priority Order)\"\n        CLI[Command-line Arguments&lt;br/&gt;Highest Priority&lt;br/&gt;--algorithm, --exclude]\n        ENV[Environment Variables&lt;br/&gt;TENETS_ALGORITHM=ml]\n        PROJECT[Project Configuration&lt;br/&gt;.tenets.yml in project root]\n        USER[User Configuration&lt;br/&gt;~/.config/tenets/config.yml]\n        SYSTEM[System Defaults&lt;br/&gt;Built-in fallbacks&lt;br/&gt;Lowest Priority]\n    end\n\n    subgraph \"Configuration Categories\"\n        RANKING_CONFIG[Ranking Configuration&lt;br/&gt;Algorithms, weights, factors]\n        NLP_CONFIG[NLP Configuration&lt;br/&gt;Tokenization, stopwords]\n        ML_CONFIG[ML Configuration&lt;br/&gt;Models, caching, devices]\n        CACHE_CONFIG[Cache Configuration&lt;br/&gt;TTL, size limits, storage]\n        SCANNER_CONFIG[Scanner Configuration&lt;br/&gt;Ignore patterns, limits&lt;br/&gt;Minified exclusion]\n        OUTPUT_CONFIG[Output Configuration&lt;br/&gt;Format, tokens, metadata]\n    end\n\n    subgraph \"Dynamic Configuration\"\n        HOT_RELOAD[Hot Reload&lt;br/&gt;File change detection]\n        API_UPDATE[Runtime API Updates&lt;br/&gt;Programmatic changes]\n        VALIDATION[Configuration Validation&lt;br/&gt;Type checking, constraints]\n        ROLLBACK[Error Rollback&lt;br/&gt;Revert on failure]\n    end\n\n    CLI --&gt; RANKING_CONFIG\n    ENV --&gt; NLP_CONFIG\n    PROJECT --&gt; ML_CONFIG\n    USER --&gt; CACHE_CONFIG\n    SYSTEM --&gt; SCANNER_CONFIG\n\n    RANKING_CONFIG --&gt; HOT_RELOAD\n    NLP_CONFIG --&gt; API_UPDATE\n    ML_CONFIG --&gt; VALIDATION\n    CACHE_CONFIG --&gt; ROLLBACK\n    SCANNER_CONFIG --&gt; ROLLBACK\n    OUTPUT_CONFIG --&gt; ROLLBACK</code></pre>"},{"location":"ARCHITECTURE/#complete-configuration-schema","title":"Complete Configuration Schema","text":"YAML<pre><code># .tenets.yml\nversion: 2\n\n# Ranking configuration\nranking:\n  algorithm: balanced  # fast|balanced|thorough|ml\n  threshold: 0.1       # Minimum relevance score\n  use_git: true        # Enable git signals\n  use_ml: true         # Enable ML features\n\n  # Factor weights (must sum to ~1.0)\n  weights:\n    semantic_similarity: 0.25\n    keyword_match: 0.15\n    bm25_similarity: 0.15\n    import_centrality: 0.10\n    path_relevance: 0.10\n    git_recency: 0.05\n    git_frequency: 0.05\n    git_authors: 0.05\n    file_type: 0.05\n    code_patterns: 0.05\n\n  # Performance\n  workers: 8           # Parallel workers\n  batch_size: 100      # Batch size for ML\n\n# NLP configuration\nnlp:\n  use_stopwords: true\n  stopword_set: minimal  # minimal|aggressive|custom\n  tokenizer: code        # code|text\n  keyword_extractor: rake # rake|yake|bm25|tfidf|frequency (rake is default for Python 3.13+)\n\n# ML configuration\nml:\n  model: all-MiniLM-L6-v2\n  device: auto         # auto|cpu|cuda\n  cache_embeddings: true\n  embedding_dim: 384\n\n# Cache configuration\ncache:\n  enabled: true\n  directory: ~/.tenets/cache\n  max_size_mb: 1000\n  ttl_days: 7\n\n  # SQLite pragmas\n  sqlite_pragmas:\n    journal_mode: WAL\n    synchronous: NORMAL\n    cache_size: -64000\n    temp_store: MEMORY\n\n# File scanning\nscanner:\n  respect_gitignore: true\n  include_hidden: false\n  follow_symlinks: false\n  max_file_size_mb: 10\n  binary_detection: true\n\n  # Global ignores\n  ignore_patterns:\n    - \"*.pyc\"\n    - \"__pycache__\"\n    - \"node_modules\"\n    - \".git\"\n    - \".venv\"\n    - \"venv\"\n    - \"*.egg-info\"\n    - \"dist\"\n    - \"build\"\n\n# Summarization configuration\nsummarizer:\n  # Documentation context-aware summarization\n  docs_context_aware: true           # Enable smart context-aware documentation summarization\n  docs_show_in_place_context: true   # Preserve relevant context sections in-place within summaries\n  docs_context_search_depth: 2       # 1=direct mentions, 2=semantic similarity, 3=deep analysis\n  docs_context_min_confidence: 0.6   # Minimum confidence for context relevance (0.0-1.0)\n  docs_context_max_sections: 10      # Maximum contextual sections to preserve per document\n  docs_context_preserve_examples: true # Always preserve code examples and snippets\n\n# Output configuration\noutput:\n  format: markdown     # markdown|json|xml\n  max_tokens: 100000\n  include_metadata: true\n  include_instructions: true\n  copy_on_distill: false\n\n# Session configuration\nsession:\n  auto_save: true\n  history_limit: 100\n  branch_on_conflict: true\n\n# Examination configuration\nexamination:\n  complexity_threshold: 10\n  duplication_threshold: 0.1\n  min_test_coverage: 0.8\n\n# Chronicle configuration\nchronicle:\n  include_merges: false\n  max_commits: 1000\n  analyze_patterns: true\n\n# Momentum configuration\nmomentum:\n  sprint_duration: 14\n  velocity_window: 6\n  include_weekends: false\n</code></pre>"},{"location":"ARCHITECTURE/#cli--api-architecture","title":"CLI &amp; API Architecture","text":""},{"location":"ARCHITECTURE/#command-structure","title":"Command Structure","text":"YAML<pre><code># Main Commands\ntenets:\n  distill:           # Build optimal context for prompts\n    --copy           # Copy to clipboard\n    --format         # Output format (markdown, xml, json)\n    --max-tokens     # Token limit\n    --exclude        # Exclude patterns\n    --session        # Session name\n\n  examine:           # Code quality analysis\n    --show-details   # Detailed metrics\n    --hotspots       # Show maintenance hotspots\n    --ownership      # Show code ownership\n    --format         # Output format\n\n  chronicle:         # Git history analysis\n    --since          # Time range\n    --author         # Filter by author\n    --format         # Output format\n\n  momentum:          # Velocity tracking (WIP)\n    --team           # Team metrics\n    --detailed       # Detailed breakdown\n\n  session:           # Session management\n    create           # Create new session\n    list             # List sessions\n    delete           # Delete session\n\n  tenet:            # Manage guiding principles\n    add             # Add new tenet\n    list            # List tenets\n    remove          # Remove tenet\n\n  instill:          # Apply tenets and system instructions\n    --dry-run       # Preview what would be applied\n    --force         # Force application\n\n  system-instruction: # Manage system instructions\n    set             # Set instruction\n    get             # Get current\n    enable/disable  # Toggle\n</code></pre>"},{"location":"ARCHITECTURE/#python-api-design","title":"Python API Design","text":"Python<pre><code>from tenets import Tenets\n\n# Initialize\ntenets = Tenets(path=\"./my-project\")\n\n# Simple usage\ncontext = tenets.distill(\"implement OAuth2 authentication\")\n\n# Advanced usage\nresult = tenets.distill(\n    prompt=\"refactor database layer\",\n    algorithm=\"ml\",\n    max_tokens=50000,\n    filters=[\"*.py\", \"!test_*\"]\n)\n\n# Session management\nsession = tenets.create_session(\"oauth-implementation\")\ncontext1 = session.distill(\"add OAuth2 support\")\ncontext2 = session.distill(\"add unit tests\", incremental=True)\n\n# Analysis tools\nexamination = tenets.examine()\nchronicle = tenets.chronicle()\nmomentum = tenets.momentum()\n\n# Configuration\ntenets.configure(\n    ranking_algorithm=\"thorough\",\n    use_ml=True,\n    cache_ttl_days=30\n)\n</code></pre>"},{"location":"ARCHITECTURE/#security--privacy-architecture","title":"Security &amp; Privacy Architecture","text":""},{"location":"ARCHITECTURE/#local-first-security-model","title":"Local-First Security Model","text":"<pre><code>graph TB\n    subgraph \"Privacy Guarantees\"\n        LOCAL[All Processing Local&lt;br/&gt;No external API calls for analysis]\n        NO_TELEMETRY[No Telemetry&lt;br/&gt;No usage tracking]\n        NO_CLOUD[No Cloud Storage&lt;br/&gt;All data stays local]\n        NO_PHONE_HOME[No Phone Home&lt;br/&gt;No automatic updates]\n    end\n\n    subgraph \"Secret Detection\"\n        API_KEYS[API Key Detection&lt;br/&gt;Common patterns]\n        PASSWORDS[Password Detection&lt;br/&gt;Credential patterns]\n        TOKENS[Token Detection&lt;br/&gt;JWT, OAuth tokens]\n        PRIVATE_KEYS[Private Key Detection&lt;br/&gt;RSA, SSH keys]\n        CONNECTION_STRINGS[Connection Strings&lt;br/&gt;Database URLs]\n        ENV_VARS[Environment Variables&lt;br/&gt;Sensitive values]\n    end\n\n    subgraph \"Output Sanitization (Roadmap)\"\n        REDACT[Redact Secrets&lt;br/&gt;**WIP** - Coming soon]\n        MASK_PII[Mask PII&lt;br/&gt;**WIP** - Planned feature]\n        CLEAN_PATHS[Clean File Paths&lt;br/&gt;Remove sensitive paths]\n        REMOVE_URLS[Remove Internal URLs&lt;br/&gt;**WIP** - Under development]\n        ANONYMIZE[Anonymization&lt;br/&gt;**WIP** - Future release]\n    end\n\n    subgraph \"Data Protection\"\n        ENCRYPTED_CACHE[Encrypted Cache&lt;br/&gt;Optional encryption at rest]\n        SECURE_DELETE[Secure Deletion&lt;br/&gt;Overwrite sensitive data]\n        ACCESS_CONTROL[File Access Control&lt;br/&gt;Respect permissions]\n        AUDIT_LOG[Audit Logging&lt;br/&gt;Security events]\n    end\n\n    LOCAL --&gt; API_KEYS\n    NO_TELEMETRY --&gt; PASSWORDS\n    NO_CLOUD --&gt; TOKENS\n    NO_PHONE_HOME --&gt; PRIVATE_KEYS\n\n    API_KEYS --&gt; REDACT\n    PASSWORDS --&gt; MASK_PII\n    TOKENS --&gt; CLEAN_PATHS\n    PRIVATE_KEYS --&gt; REMOVE_URLS\n    CONNECTION_STRINGS --&gt; ANONYMIZE\n    ENV_VARS --&gt; ANONYMIZE\n\n    REDACT --&gt; ENCRYPTED_CACHE\n    MASK_PII --&gt; SECURE_DELETE\n    CLEAN_PATHS --&gt; ACCESS_CONTROL\n    REMOVE_URLS --&gt; AUDIT_LOG\n    ANONYMIZE --&gt; AUDIT_LOG</code></pre>"},{"location":"ARCHITECTURE/#secret-detection-patterns-roadmap","title":"Secret Detection Patterns (Roadmap)","text":"<p>Note: Secret detection and redaction is a planned feature for future releases.</p> <p>The following architecture represents the planned implementation:</p> <pre><code>graph LR\n    subgraph \"Detection Methods\"\n        REGEX[Regex Patterns&lt;br/&gt;Known formats]\n        ENTROPY[Entropy Analysis&lt;br/&gt;Random strings]\n        CONTEXT[Context Analysis&lt;br/&gt;Variable names]\n        KEYWORDS[Keyword Detection&lt;br/&gt;password, secret, key]\n    end\n\n    subgraph \"Secret Types\"\n        AWS[AWS Access Keys&lt;br/&gt;AKIA...]\n        GITHUB[GitHub Tokens&lt;br/&gt;ghp_, gho_]\n        JWT[JWT Tokens&lt;br/&gt;eyJ pattern]\n        RSA[RSA Private Keys&lt;br/&gt;-----BEGIN RSA]\n        DATABASE[Database URLs&lt;br/&gt;postgres://, mysql://]\n        GENERIC[Generic Secrets&lt;br/&gt;High entropy strings]\n    end\n\n    subgraph \"Response Actions\"\n        FLAG[Flag for Review&lt;br/&gt;Warn user]\n        REDACT_AUTO[Auto Redaction&lt;br/&gt;Replace with [REDACTED]]\n        EXCLUDE[Exclude File&lt;br/&gt;Skip entirely]\n        LOG[Security Log&lt;br/&gt;Record detection]\n    end\n\n    REGEX --&gt; AWS\n    ENTROPY --&gt; GITHUB\n    CONTEXT --&gt; JWT\n    KEYWORDS --&gt; RSA\n\n    AWS --&gt; FLAG\n    GITHUB --&gt; REDACT_AUTO\n    JWT --&gt; EXCLUDE\n    RSA --&gt; LOG\n    DATABASE --&gt; LOG\n    GENERIC --&gt; FLAG</code></pre>"},{"location":"ARCHITECTURE/#testing--quality-assurance","title":"Testing &amp; Quality Assurance","text":""},{"location":"ARCHITECTURE/#test-architecture","title":"Test Architecture","text":"<pre><code>graph TB\n    subgraph \"Test Categories\"\n        UNIT[Unit Tests&lt;br/&gt;Target: High coverage&lt;br/&gt;Fast, isolated]\n        INTEGRATION[Integration Tests&lt;br/&gt;Component interaction&lt;br/&gt;Real workflows]\n        E2E[End-to-End Tests&lt;br/&gt;Complete user journeys&lt;br/&gt;CLI to output]\n        PERFORMANCE[Performance Tests&lt;br/&gt;Benchmark regression&lt;br/&gt;Memory usage]\n    end\n\n    subgraph \"Test Structure\"\n        FIXTURES[Test Fixtures&lt;br/&gt;Sample codebases&lt;br/&gt;Known outputs]\n        MOCKS[Mock Objects&lt;br/&gt;External dependencies&lt;br/&gt;Controlled behavior]\n        HELPERS[Test Helpers&lt;br/&gt;Common operations&lt;br/&gt;Assertion utilities]\n        FACTORIES[Data Factories&lt;br/&gt;Generate test data&lt;br/&gt;Realistic scenarios]\n    end\n\n    subgraph \"Quality Metrics\"\n        COVERAGE[Code Coverage&lt;br/&gt;Line and branch coverage]\n        COMPLEXITY[Complexity Limits&lt;br/&gt;Cyclomatic &lt; 10]\n        DUPLICATION[Duplication Check&lt;br/&gt;&lt; 5% duplicate code]\n        DOCUMENTATION[Documentation&lt;br/&gt;100% public API]\n    end\n\n    subgraph \"Continuous Testing\"\n        PRE_COMMIT[Pre-commit Hooks&lt;br/&gt;Fast feedback]\n        CI_PIPELINE[CI Pipeline&lt;br/&gt;Full test suite]\n        NIGHTLY[Nightly Tests&lt;br/&gt;Extended scenarios]\n        BENCHMARKS[Benchmark Tracking&lt;br/&gt;Performance trends]\n    end\n\n    UNIT --&gt; FIXTURES\n    INTEGRATION --&gt; MOCKS\n    E2E --&gt; HELPERS\n    PERFORMANCE --&gt; FACTORIES\n\n    FIXTURES --&gt; COVERAGE\n    MOCKS --&gt; COMPLEXITY\n    HELPERS --&gt; DUPLICATION\n    FACTORIES --&gt; DOCUMENTATION\n\n    COVERAGE --&gt; PRE_COMMIT\n    COMPLEXITY --&gt; CI_PIPELINE\n    DUPLICATION --&gt; NIGHTLY\n    DOCUMENTATION --&gt; BENCHMARKS</code></pre>"},{"location":"ARCHITECTURE/#test-coverage-requirements","title":"Test Coverage Requirements","text":"<pre><code>graph LR\n    subgraph \"Coverage Targets\"\n        UNIT_COV[Unit Tests&lt;br/&gt;&gt;90% coverage&lt;br/&gt;Critical paths 100%]\n        INTEGRATION_COV[Integration Tests&lt;br/&gt;All major workflows&lt;br/&gt;Error scenarios]\n        E2E_COV[E2E Tests&lt;br/&gt;Critical user journeys&lt;br/&gt;Happy paths]\n        PERF_COV[Performance Tests&lt;br/&gt;Regression prevention&lt;br/&gt;Memory leak detection]\n    end\n\n    subgraph \"Quality Gates\"\n        CODE_QUALITY[Code Quality&lt;br/&gt;Complexity &lt; 10&lt;br/&gt;Function length &lt; 50]\n        DOCUMENTATION[Documentation&lt;br/&gt;100% public API&lt;br/&gt;Usage examples]\n        SECURITY[Security Tests&lt;br/&gt;Secret detection&lt;br/&gt;Input validation]\n        COMPATIBILITY[Compatibility&lt;br/&gt;Python 3.8+&lt;br/&gt;Multiple platforms]\n    end\n\n    UNIT_COV --&gt; CODE_QUALITY\n    INTEGRATION_COV --&gt; DOCUMENTATION\n    E2E_COV --&gt; SECURITY\n    PERF_COV --&gt; COMPATIBILITY</code></pre>"},{"location":"ARCHITECTURE/#guiding-principles-tenets-system","title":"Guiding Principles (Tenets) System","text":""},{"location":"ARCHITECTURE/#overview","title":"Overview","text":"<p>The Guiding Principles system (internally called \"Tenets\") provides a way to inject persistent, context-aware instructions into generated code context. These principles help maintain consistency across AI interactions and combat context drift by ensuring important architectural decisions, coding standards, and project-specific requirements are consistently reinforced.</p>"},{"location":"ARCHITECTURE/#output-format-conventions","title":"Output Format Conventions","text":"<p>Following OpenAI's recommendations for structured output, tenets are formatted as \"guiding principles\" in human-readable formats:</p>"},{"location":"ARCHITECTURE/#markdown-format","title":"Markdown Format","text":"Markdown<pre><code>**\ud83c\udfaf Key Guiding Principle:** Always validate user input before processing\n**\ud83d\udccc Important Guiding Principle:** Use async/await for all I/O operations\n**\ud83d\udca1 Guiding Principle:** Prefer composition over inheritance\n</code></pre>"},{"location":"ARCHITECTURE/#xml-format-recommended-by-openai","title":"XML Format (Recommended by OpenAI)","text":"XML<pre><code>&lt;guiding_principle priority=\"high\" category=\"security\"&gt;\n  Always validate and sanitize user input\n&lt;/guiding_principle&gt;\n\n&lt;guiding_principles&gt;\n  &lt;guiding_principle priority=\"critical\"&gt;Maintain backward compatibility&lt;/guiding_principle&gt;\n  &lt;guiding_principle priority=\"medium\"&gt;Use descriptive variable names&lt;/guiding_principle&gt;\n&lt;/guiding_principles&gt;\n</code></pre>"},{"location":"ARCHITECTURE/#json-format","title":"JSON Format","text":"JSON<pre><code>/* GUIDING PRINCIPLE: Follow REST API conventions for all endpoints */\n</code></pre>"},{"location":"ARCHITECTURE/#injection-strategy","title":"Injection Strategy","text":"<p>The system uses intelligent injection strategies to place guiding principles where they'll be most effective:</p> <pre><code>graph TD\n    subgraph \"Injection Decision Engine\"\n        ANALYZER[Content Analyzer&lt;br/&gt;Structure &amp; complexity]\n        STRATEGY[Strategy Selector&lt;br/&gt;Top, distributed, contextual]\n        INJECTOR[Smart Injector&lt;br/&gt;Natural break detection]\n    end\n\n    subgraph \"Priority System\"\n        CRITICAL[Critical Principles&lt;br/&gt;Security, data integrity]\n        HIGH[High Priority&lt;br/&gt;Architecture, performance]\n        MEDIUM[Medium Priority&lt;br/&gt;Style, conventions]\n        LOW[Low Priority&lt;br/&gt;Preferences, suggestions]\n    end\n\n    subgraph \"Reinforcement\"\n        TOP_INJECTION[Top of Context&lt;br/&gt;Most visible]\n        DISTRIBUTED[Throughout Content&lt;br/&gt;Natural sections]\n        END_SUMMARY[End Reinforcement&lt;br/&gt;Key reminders]\n    end\n\n    ANALYZER --&gt; STRATEGY\n    STRATEGY --&gt; INJECTOR\n\n    CRITICAL --&gt; TOP_INJECTION\n    HIGH --&gt; DISTRIBUTED\n    MEDIUM --&gt; DISTRIBUTED\n    LOW --&gt; END_SUMMARY</code></pre>"},{"location":"ARCHITECTURE/#injection-behavior","title":"Injection Behavior","text":"<p>The system ensures guiding principles are present when needed:</p>"},{"location":"ARCHITECTURE/#session-based-injection","title":"Session-Based Injection","text":"<ul> <li>First Output Rule: Guiding principles are ALWAYS injected on the first distill in any session</li> <li>Named Sessions: After first injection, follows configured frequency (adaptive/periodic/always)</li> <li>Unnamed Sessions: Treated as important contexts that always receive guiding principles</li> <li>No Delay: Previously required 5 operations before first injection; now immediate</li> </ul>"},{"location":"ARCHITECTURE/#configuration","title":"Configuration","text":"YAML<pre><code>tenet:\n  auto_instill: true\n  max_per_context: 5\n  injection_strategy: strategic\n  injection_frequency: adaptive  # 'always', 'periodic', 'adaptive', 'manual'\n  injection_interval: 3          # For periodic mode\n  min_session_length: 1          # Now 1 (was 5) - first injection always happens\n  system_instruction: \"Prefer small, safe diffs and add tests\"\n  system_instruction_enabled: true\n</code></pre>"},{"location":"ARCHITECTURE/#injection-frequencies","title":"Injection Frequencies","text":"<ul> <li>always: Inject on every distill operation</li> <li>periodic: Inject every N operations (set by <code>injection_interval</code>)</li> <li>adaptive: Smart injection based on context complexity and session state</li> <li>manual: Only inject when explicitly requested</li> </ul>"},{"location":"ARCHITECTURE/#integration-with-distill-command","title":"Integration with Distill Command","text":"<p>When using the <code>distill</code> command, guiding principles are automatically injected based on configuration.</p> <p>Note: System instructions are excluded from HTML reports (which are meant for human consumption) but included in formats intended for AI consumption (markdown, XML, JSON).</p>"},{"location":"ARCHITECTURE/#future-roadmap--vision","title":"Future Roadmap &amp; Vision","text":""},{"location":"ARCHITECTURE/#near-term-q1-2025","title":"Near Term (Q1 2025)","text":"<pre><code>graph TB\n    subgraph \"Core Improvements\"\n        INCREMENTAL[Incremental Indexing&lt;br/&gt;Real-time updates&lt;br/&gt;Watch file changes]\n        FASTER_EMBED[Faster Embeddings&lt;br/&gt;Model quantization&lt;br/&gt;ONNX optimization]\n        LANGUAGE_SUP[Better Language Support&lt;br/&gt;30+ languages&lt;br/&gt;Language-specific patterns]\n        IDE_PLUGINS[IDE Plugin Ecosystem&lt;br/&gt;VS Code, IntelliJ, Vim]\n        CROSS_REPO[Cross-repository Analysis&lt;br/&gt;Monorepo support&lt;br/&gt;Dependency tracking]\n    end\n\n    subgraph \"ML Enhancements\"\n        NEWER_MODELS[Newer Embedding Models&lt;br/&gt;Code-specific transformers&lt;br/&gt;Better accuracy]\n        FINE_TUNING[Fine-tuning Pipeline&lt;br/&gt;Domain-specific models&lt;br/&gt;Custom training]\n        MULTIMODAL[Multi-modal Understanding&lt;br/&gt;Diagrams, images&lt;br/&gt;Architecture docs]\n        CODE_TRANSFORMERS[Code-specific Models&lt;br/&gt;Programming language aware&lt;br/&gt;Syntax understanding]\n    end\n\n    INCREMENTAL --&gt; NEWER_MODELS\n    FASTER_EMBED --&gt; FINE_TUNING\n    LANGUAGE_SUP --&gt; MULTIMODAL\n    IDE_PLUGINS --&gt; CODE_TRANSFORMERS\n    CROSS_REPO --&gt; CODE_TRANSFORMERS</code></pre>"},{"location":"ARCHITECTURE/#medium-term-q2-q3-2025","title":"Medium Term (Q2-Q3 2025)","text":"<pre><code>graph TB\n    subgraph \"Platform Features\"\n        WEB_UI[Web UI&lt;br/&gt;Real-time collaboration&lt;br/&gt;Team workspaces]\n        SHARED_CONTEXT[Shared Context Libraries&lt;br/&gt;Team knowledge base&lt;br/&gt;Best practices]\n        KNOWLEDGE_GRAPHS[Knowledge Graphs&lt;br/&gt;Code relationships&lt;br/&gt;Semantic connections]\n        AI_AGENTS[AI Agent Integration&lt;br/&gt;Autonomous assistance&lt;br/&gt;Proactive suggestions]\n    end\n\n    subgraph \"Enterprise Features\"\n        SSO[SSO/SAML Support&lt;br/&gt;Enterprise authentication&lt;br/&gt;Role-based access]\n        AUDIT[Audit Logging&lt;br/&gt;Compliance tracking&lt;br/&gt;Usage monitoring]\n        COMPLIANCE[Compliance Modes&lt;br/&gt;GDPR, SOX, HIPAA&lt;br/&gt;Data governance]\n        AIR_GAPPED[Air-gapped Deployment&lt;br/&gt;Offline operation&lt;br/&gt;Secure environments]\n        CUSTOM_ML[Custom ML Models&lt;br/&gt;Private model training&lt;br/&gt;Domain expertise]\n    end\n\n    WEB_UI --&gt; SSO\n    SHARED_CONTEXT --&gt; AUDIT\n    KNOWLEDGE_GRAPHS --&gt; COMPLIANCE\n    AI_AGENTS --&gt; AIR_GAPPED\n    AI_AGENTS --&gt; CUSTOM_ML</code></pre>"},{"location":"ARCHITECTURE/#long-term-2026","title":"Long Term (2026+)","text":"<pre><code>graph TB\n    subgraph \"Vision Goals\"\n        AUTONOMOUS[Autonomous Code Understanding&lt;br/&gt;Self-improving analysis&lt;br/&gt;Minimal human input]\n        PREDICTIVE[Predictive Development&lt;br/&gt;Anticipate needs&lt;br/&gt;Suggest improvements]\n        UNIVERSAL[Universal Code Intelligence&lt;br/&gt;Any language, any domain&lt;br/&gt;Contextual understanding]\n        INDUSTRY_STANDARD[Industry Standard&lt;br/&gt;AI pair programming&lt;br/&gt;Developer toolchain]\n    end\n\n    subgraph \"Research Areas\"\n        GRAPH_NEURAL[Graph Neural Networks&lt;br/&gt;Code structure understanding&lt;br/&gt;Relationship modeling]\n        REINFORCEMENT[Reinforcement Learning&lt;br/&gt;Ranking optimization&lt;br/&gt;Adaptive behavior]\n        FEW_SHOT[Few-shot Learning&lt;br/&gt;New language support&lt;br/&gt;Rapid adaptation]\n        EXPLAINABLE[Explainable AI&lt;br/&gt;Ranking transparency&lt;br/&gt;Decision reasoning]\n        FEDERATED[Federated Learning&lt;br/&gt;Team knowledge sharing&lt;br/&gt;Privacy-preserving]\n    end\n\n    AUTONOMOUS --&gt; GRAPH_NEURAL\n    PREDICTIVE --&gt; REINFORCEMENT\n    UNIVERSAL --&gt; FEW_SHOT\n    INDUSTRY_STANDARD --&gt; EXPLAINABLE\n    INDUSTRY_STANDARD --&gt; FEDERATED</code></pre>"},{"location":"ARCHITECTURE/#technology-evolution","title":"Technology Evolution","text":"<pre><code>timeline\n    title Tenets Technology Roadmap\n\n    2025 Q1 : Core ML Pipeline\n            : Local Embeddings\n            : Multi-language Support\n            : IDE Integrations\n\n    2025 Q2 : Web Collaboration\n            : Team Features\n            : Enterprise Security\n            : Performance Optimization\n\n    2025 Q3 : Knowledge Graphs\n            : AI Agent Integration\n            : Custom Models\n            : Advanced Analytics\n\n    2026    : Autonomous Understanding\n            : Predictive Intelligence\n            : Graph Neural Networks\n            : Industry Adoption\n\n    2027+   : Universal Code Intelligence\n            : Federated Learning\n            : Next-gen AI Integration\n            : Market Leadership</code></pre>"},{"location":"ARCHITECTURE/#output-generation--visualization","title":"Output Generation &amp; Visualization","text":""},{"location":"ARCHITECTURE/#output-formatting-system","title":"Output Formatting System","text":"<p>The output formatting system in Tenets provides multiple format options to suit different use cases and integrations:</p> <pre><code>graph TB\n    subgraph \"Format Types\"\n        MARKDOWN[Markdown Format&lt;br/&gt;Human-readable]\n        JSON[JSON Format&lt;br/&gt;Machine-parseable]\n        XML[XML Format&lt;br/&gt;Structured data]\n        HTML[HTML Format&lt;br/&gt;Interactive reports]\n    end\n\n    subgraph \"HTML Report Features\"\n        INTERACTIVE[Interactive Elements&lt;br/&gt;Collapsible sections]\n        VISUALS[Visualizations&lt;br/&gt;Charts &amp; graphs]\n        STYLING[Professional Styling&lt;br/&gt;Modern UI]\n        RESPONSIVE[Responsive Design&lt;br/&gt;Mobile-friendly]\n    end\n\n    subgraph \"Report Components\"\n        HEADER[Report Header&lt;br/&gt;Title &amp; metadata]\n        PROMPT_DISPLAY[Prompt Analysis&lt;br/&gt;Keywords &amp; intent]\n        STATS[Statistics Dashboard&lt;br/&gt;Metrics &amp; KPIs]\n        FILES[File Listings&lt;br/&gt;Code previews]\n        GIT[Git Context&lt;br/&gt;Commits &amp; contributors]\n    end\n\n    HTML --&gt; INTERACTIVE\n    HTML --&gt; VISUALS\n    HTML --&gt; STYLING\n    HTML --&gt; RESPONSIVE\n\n    INTERACTIVE --&gt; HEADER\n    VISUALS --&gt; STATS\n    STYLING --&gt; FILES\n    RESPONSIVE --&gt; GIT</code></pre>"},{"location":"ARCHITECTURE/#html-report-generation","title":"HTML Report Generation","text":"<p>The HTML formatter leverages the reporting infrastructure to create rich, interactive reports:</p>"},{"location":"ARCHITECTURE/#features","title":"Features:","text":"<ul> <li>Interactive Dashboard: Collapsible sections, sortable tables, and filterable content</li> <li>Visual Statistics: Charts for file distribution, token usage, and relevance scores</li> <li>Code Previews: Syntax-highlighted code snippets with truncation for large files</li> <li>Responsive Design: Mobile-friendly layout that adapts to screen size</li> <li>Professional Styling: Modern UI with gradients, shadows, and animations</li> <li>Git Integration: Display of recent commits, contributors, and branch information</li> </ul>"},{"location":"ARCHITECTURE/#architecture","title":"Architecture:","text":"Python<pre><code>class HTMLFormatter:\n    \"\"\"HTML report generation for distill command.\"\"\"\n\n    def format_html(self, aggregated, prompt_context, session):\n        # Create HTML template with modern styling\n        template = HTMLTemplate(theme=\"modern\", include_charts=True)\n\n        # Build report sections\n        sections = [\n            self._build_header(prompt_context, session),\n            self._build_prompt_analysis(prompt_context),\n            self._build_statistics(aggregated),\n            self._build_file_cards(aggregated[\"included_files\"]),\n            self._build_git_context(aggregated.get(\"git_context\"))\n        ]\n\n        # Generate final HTML with embedded styles and scripts\n        return template.render(sections)\n</code></pre>"},{"location":"ARCHITECTURE/#visualization-components","title":"Visualization Components","text":"<p>The visualization system provides rich visual representations of code analysis with intelligent project detection:</p> <pre><code>graph LR\n    subgraph \"Project Detection\"\n        DETECTOR[Project Detector&lt;br/&gt;Auto-detects type]\n        LANGUAGES[Language Analysis&lt;br/&gt;% distribution]\n        FRAMEWORKS[Framework Detection&lt;br/&gt;Django, React, etc]\n        ENTRYPOINTS[Entry Points&lt;br/&gt;main.py, index.js]\n    end\n\n    subgraph \"Graph Generation\"\n        GRAPHGEN[Graph Generator&lt;br/&gt;Multiple formats]\n        NETWORKX[NetworkX&lt;br/&gt;Graph algorithms]\n        GRAPHVIZ[Graphviz&lt;br/&gt;DOT rendering]\n        PLOTLY[Plotly&lt;br/&gt;Interactive HTML]\n        D3JS[D3.js&lt;br/&gt;Web visualization]\n    end\n\n    subgraph \"Dependency Visualization\"\n        FILE_DEPS[File-level&lt;br/&gt;Individual files]\n        MODULE_DEPS[Module-level&lt;br/&gt;Aggregated modules]\n        PACKAGE_DEPS[Package-level&lt;br/&gt;Top-level packages]\n        CLUSTERING[Clustering&lt;br/&gt;Group by criteria]\n    end\n\n    subgraph \"Output Formats\"\n        ASCII[ASCII Tree&lt;br/&gt;Terminal output]\n        SVG[SVG&lt;br/&gt;Vector graphics]\n        PNG[PNG/PDF&lt;br/&gt;Static images]\n        HTML_INT[Interactive HTML&lt;br/&gt;D3.js/Plotly]\n        DOT[DOT Format&lt;br/&gt;Graphviz source]\n        JSON_OUT[JSON&lt;br/&gt;Raw data]\n    end\n\n    subgraph \"Layout Algorithms\"\n        HIERARCHICAL[Hierarchical&lt;br/&gt;Tree layout]\n        CIRCULAR[Circular&lt;br/&gt;Radial layout]\n        SHELL[Shell&lt;br/&gt;Concentric circles]\n        KAMADA[Kamada-Kawai&lt;br/&gt;Force-directed]\n    end\n\n    DETECTOR --&gt; LANGUAGES\n    DETECTOR --&gt; FRAMEWORKS\n    DETECTOR --&gt; ENTRYPOINTS\n\n    GRAPHGEN --&gt; NETWORKX\n    GRAPHGEN --&gt; GRAPHVIZ\n    GRAPHGEN --&gt; PLOTLY\n    GRAPHGEN --&gt; D3JS\n\n    FILE_DEPS --&gt; MODULE_DEPS\n    MODULE_DEPS --&gt; PACKAGE_DEPS\n    PACKAGE_DEPS --&gt; CLUSTERING\n\n    GRAPHGEN --&gt; ASCII\n    GRAPHGEN --&gt; SVG\n    GRAPHGEN --&gt; PNG\n    GRAPHGEN --&gt; HTML_INT\n    GRAPHGEN --&gt; DOT\n    GRAPHGEN --&gt; JSON_OUT</code></pre>"},{"location":"ARCHITECTURE/#project-detection-system","title":"Project Detection System","text":"<p>The new ProjectDetector automatically identifies: - Project Type: Python package, Node.js app, Django project, React app, etc. - Language Distribution: Percentages of each language in the codebase - Frameworks: Detects Django, Flask, React, Vue, Spring, Rails, etc. - Entry Points: Finds main.py, index.js, package.json main field, etc. - Project Structure: Identifies src/, tests/, docs/ directories</p>"},{"location":"ARCHITECTURE/#dependency-visualization-modes","title":"Dependency Visualization Modes","text":"<p>Three levels of dependency aggregation: 1. File-level: Shows individual file dependencies (detailed view) 2. Module-level: Aggregates to module/directory level (balanced view) 3. Package-level: Shows only top-level package dependencies (high-level view)</p>"},{"location":"ARCHITECTURE/#graph-generation-features","title":"Graph Generation Features","text":"<ul> <li>Multiple Formats: SVG, PNG, PDF, HTML, DOT, JSON</li> <li>Pure Python: All dependencies installable via pip (no system deps)</li> <li>Interactive HTML: D3.js or Plotly-based interactive visualizations</li> <li>Clustering: Group nodes by directory, module, or package</li> <li>Layout Algorithms: Hierarchical, circular, shell, force-directed</li> <li>Node Limiting: Handle large graphs with --max-nodes option</li> </ul>"},{"location":"ARCHITECTURE/#integration-with-report-generator","title":"Integration with Report Generator","text":"<p>The distill command now fully integrates with the report generation infrastructure:</p> <ol> <li>Shared Templates: Uses the same HTML templates as the examine command</li> <li>Consistent Styling: Unified visual design across all report types</li> <li>Reusable Components: Shared visualization libraries and chart generators</li> <li>Export Options: Support for PDF export via HTML rendering</li> </ol>"},{"location":"ARCHITECTURE/#usage-examples","title":"Usage Examples","text":"Bash<pre><code># Generate HTML report for context\ntenets distill \"review API\" --format html -o report.html\n\n# Create interactive dashboard with verbose details\ntenets distill \"analyze security\" --format html --verbose -o security_context.html\n\n# Generate report with custom styling\ntenets distill \"refactor database\" --format html --theme dark -o refactor.html\n\n# Dependency visualization with auto-detection\ntenets viz deps  # Auto-detects project type and generates ASCII tree\ntenets viz deps --output deps.svg  # Generate SVG dependency graph\ntenets viz deps --format html --output interactive.html  # Interactive visualization\n\n# Different aggregation levels\ntenets viz deps --level file  # Show all file dependencies (detailed)\ntenets viz deps --level module  # Aggregate by module (balanced)\ntenets viz deps --level package  # Show package architecture (high-level)\n\n# Advanced visualization options\ntenets viz deps --cluster-by directory --layout circular  # Circular with clustering\ntenets viz deps --max-nodes 100 --format png  # Limit to top 100 nodes\ntenets viz deps src/ --include \"*.py\" --exclude \"*test*\"  # Filter files\n\n# Export formats\ntenets viz deps --format dot --output graph.dot  # Graphviz DOT for further processing\ntenets viz deps --format json --output data.json  # Raw JSON for custom tools\n</code></pre>"},{"location":"ARCHITECTURE/#performance-optimizations","title":"Performance Optimizations","text":"<ul> <li>Lazy Loading: Large code sections load on-demand</li> <li>Virtual Scrolling: Efficient rendering of long file lists</li> <li>Minified Assets: Compressed CSS and JavaScript</li> <li>Inline Resources: No external dependencies for offline viewing</li> </ul>"},{"location":"ARCHITECTURE/#conclusion","title":"Conclusion","text":"<p>Tenets represents a fundamental shift in how developers interact with their codebases when working with AI. By combining sophisticated NLP/ML techniques with traditional code analysis, git mining, and intelligent caching, we've created a system that truly understands code in context.</p> <p>The architecture is designed to be:</p> <ul> <li>Performant: Sub-second responses for most operations</li> <li>Scalable: From small projects to massive monorepos</li> <li>Extensible: Plugin system for custom logic</li> <li>Private: Everything runs locally</li> <li>Intelligent: Advanced ML when available</li> <li>Practical: Works today, improves tomorrow</li> </ul>"},{"location":"ARCHITECTURE/#key-architectural-strengths","title":"Key Architectural Strengths","text":"<ol> <li>Multi-Modal Intelligence: Combines semantic understanding, structural analysis, and historical context</li> <li>Progressive Enhancement: Works with minimal dependencies, scales with available resources</li> <li>Local-First Privacy: Complete data sovereignty and security</li> <li>Configurable Ranking: Every factor can be tuned for specific use cases</li> <li>Streaming Performance: Results available as soon as possible</li> <li>Intelligent Caching: Multiple cache levels with smart invalidation</li> <li>Extensible Design: Plugin architecture for custom functionality</li> </ol> <p>The future of code intelligence is local, intelligent, and developer-centric. Tenets embodies this vision while remaining practical and immediately useful for development teams of any size.</p>"},{"location":"BRANDING/","title":"BRANDING","text":""},{"location":"BRANDING/#primary-colors","title":"Primary Colors","text":"<p>$navy-900: #1a2332;  // Logo dark blue $navy-800: #263244;  // Slightly lighter $navy-700: #364152;  // Card backgrounds (dark mode)</p>"},{"location":"BRANDING/#accent-colors","title":"Accent Colors","text":"<p>$amber-500: #f59e0b;  // Lantern flame/glow effect $amber-400: #fbbf24;  // Hover states $amber-300: #fcd34d;  // Highlights</p>"},{"location":"BRANDING/#neutral-palette","title":"Neutral Palette","text":"<p>$cream-50:  #fdfdf9;  // Light mode background (Victorian paper) $cream-100: #f7f5f0;  // Card backgrounds (light mode) $sepia-200: #e8e2d5;  // Borders light mode $sepia-600: #6b5d4f;  // Muted text $sepia-800: #3e342a;  // Body text light mode</p>"},{"location":"BRANDING/#semantic-colors","title":"Semantic Colors","text":"<p>$success: #059669;    // Victorian green $warning: #d97706;    // Brass/copper $error:   #dc2626;    // Deep red $info:    #0891b2;    // Teal</p>"},{"location":"CLI/","title":"Tenets CLI Reference","text":"<p>tenets - Context that feeds your prompts. A command-line tool for intelligent code aggregation, analysis, and visualization.</p>"},{"location":"CLI/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Installation</li> <li>Quick Start</li> <li>Core Commands</li> <li>distill</li> <li>instill</li> <li>rank</li> <li>examine</li> <li>chronicle</li> <li>momentum</li> <li>tenet</li> <li>Visualization Commands</li> <li>viz deps</li> <li>viz complexity</li> <li>viz coupling</li> <li>viz contributors</li> <li>Session Commands</li> <li>Tenet Commands</li> <li>Instill Command</li> <li>System Instruction Commands</li> <li>Configuration</li> <li>Common Use Cases</li> <li>Examples</li> </ul>"},{"location":"CLI/#installation","title":"Installation","text":"Bash<pre><code># Basic install (core features only)\npip install tenets\n\n# With visualization support\npip install tenets[viz]\n\n# With ML-powered ranking\npip install tenets[ml]\n\n# Everything\npip install tenets[all]\n</code></pre>"},{"location":"CLI/#quick-start","title":"Quick Start","text":"Bash<pre><code># Generate context for AI pair programming\ntenets distill \"implement OAuth2\" ./src\n\n# Analyze your codebase\ntenets examine\n\n# Track recent changes\ntenets chronicle --since yesterday\n\n# Visualize dependencies (ASCII by default)\ntenets viz deps\n</code></pre>"},{"location":"CLI/#core-commands","title":"Core Commands","text":""},{"location":"CLI/#distill","title":"distill","text":"<p>Generate optimized context for LLMs from your codebase.</p> Bash<pre><code>tenets distill &lt;prompt&gt; [path] [options]\n</code></pre> <p>Arguments:</p> <ul> <li>prompt: Your query or task description (can be text or URL)</li> <li>path: Directory or files to analyze (default: current directory)</li> </ul> <p>Options:</p> <ul> <li><code>--format</code>, <code>-f</code>: Output format: markdown (default), xml, json</li> <li><code>--model</code>, <code>-m</code>: Target LLM model (e.g., gpt-4o, claude-3-opus)</li> <li><code>--output</code>, <code>-o</code>: Save to file instead of stdout</li> <li><code>--max-tokens</code>: Maximum tokens for context</li> <li><code>--mode</code>: Analysis mode: fast, balanced (default), thorough</li> <li><code>--no-git</code>: Disable git context inclusion</li> <li><code>--use-stopwords</code>: Enable stopword filtering for keyword analysis</li> <li><code>--include</code>, <code>-i</code>: Include file patterns (e.g., \".py,.js\")</li> <li><code>--exclude</code>, <code>-e</code>: Exclude file patterns (e.g., \"test_,.backup\")</li> <li><code>--session</code>, <code>-s</code>: Use a named session for stateful context</li> <li><code>--estimate-cost</code>: Show token usage and cost estimate</li> <li><code>--verbose</code>, <code>-v</code>: Show detailed analysis info</li> <li><code>--full</code>: Include full content for all ranked files (no summarization) until token budget reached</li> <li><code>--condense</code>: Condense whitespace (collapse large blank runs, trim trailing spaces) before token counting</li> <li><code>--remove-comments</code>: Strip comments (heuristic, language-aware) before token counting</li> <li><code>--copy</code>: Copy distilled context directly to clipboard (or set output.copy_on_distill: true in config)</li> </ul> <p>Examples:</p> Bash<pre><code># Basic usage - finds all relevant files for implementing OAuth2\ntenets distill \"implement OAuth2 authentication\"\n\n# From a GitHub issue\ntenets distill https://github.com/org/repo/issues/123\n\n# Target specific model with cost estimation\ntenets distill \"add caching layer\" --model gpt-4o --estimate-cost\n\n# Filter by file types\ntenets distill \"review API endpoints\" --include \"*.py,*.yaml\" --exclude \"test_*\"\n\n# Save context to file\ntenets distill \"debug login issue\" --output context.md\n\n# Use thorough analysis for complex tasks\ntenets distill \"refactor authentication system\" --mode thorough\n\n# Session-based context (maintains state)\ntenets distill \"build payment system\" --session payment-feature\n\n# Full mode (force raw content inclusion)\ntenets distill \"inspect performance code\" --full --max-tokens 60000\n\n# Reduce token usage by stripping comments &amp; whitespace\ntenets distill \"understand API surface\" --remove-comments --condense --stats\n</code></pre>"},{"location":"CLI/#content-transformations","title":"Content Transformations","text":"<p>You can optionally transform file content prior to aggregation/token counting:</p> Flag Effect Safety <code>--full</code> Disables summarization; includes raw file content until budget is hit Budget only <code>--remove-comments</code> Removes line &amp; block comments (language-aware heuristics) Aborts if &gt;60% of non-empty lines would vanish <code>--condense</code> Collapses 3+ blank lines to 1, trims trailing spaces, ensures final newline Lossless for code logic <p>Transformations are applied in this order: comment stripping -&gt; whitespace condensation. Statistics (e.g. removed comment lines) are tracked internally and may be surfaced in future <code>--stats</code> expansions.</p>"},{"location":"CLI/#pinned-files","title":"Pinned Files","text":"<p>Pin critical files so they're always considered first in subsequent distill runs for the same session:</p> Bash<pre><code># Pin individual files\ntenets instill --session refactor-auth --add-file src/auth/service.py --add-file src/auth/models.py\n\n# Pin all files in a folder (respects .gitignore)\ntenets instill --session refactor-auth --add-folder src/auth\n\n# List pinned files\ntenets instill --session refactor-auth --list-pinned\n\n# Generate context (pinned files prioritized)\ntenets distill \"add JWT refresh tokens\" --session refactor-auth --remove-comments\n</code></pre> <p>Pinned files are stored in the session metadata (SQLite) and reloaded automatically\u2014no extra flags needed when distilling.</p>"},{"location":"CLI/#ranking-presets-and-thresholds","title":"Ranking presets and thresholds","text":"<ul> <li>Presets (selected via <code>--mode</code> or config <code>ranking.algorithm</code>):</li> <li><code>fast</code> \u2013 keyword + path signals (broad, quick)</li> <li><code>balanced</code> (default) \u2013 multi-factor (keywords, path, imports, git, complexity)</li> <li> <p><code>thorough</code> \u2013 deeper analysis (heavier)</p> </li> <li> <p>Threshold (config <code>ranking.threshold</code>) controls inclusion. Lower = include more files.</p> </li> <li>Typical ranges:<ul> <li>fast: 0.05\u20130.10</li> <li>balanced: 0.10\u20130.20</li> <li>thorough: 0.10\u20130.20</li> </ul> </li> </ul> <p>Configure in <code>.tenets.yml</code> (repo root):</p> YAML<pre><code>ranking:\n  algorithm: fast      # fast | balanced | thorough\n  threshold: 0.05      # 0.0\u20131.0\n</code></pre> <p>One-off overrides (environment, Git Bash):</p> Bash<pre><code>TENETS_RANKING_THRESHOLD=0.05 TENETS_RANKING_ALGORITHM=fast \\\n  tenets distill \"implement OAuth2\" . --include \"*.py,*.md\" --max-tokens 50000\n\n# Copy output to clipboard directly\ntenets distill \"implement OAuth2\" --copy\n\n# Enable automatic copying in config\noutput:\n  copy_on_distill: true\n</code></pre> <p>Inspect current config:</p> Bash<pre><code>tenets config show --key ranking\n</code></pre> <p>See also: docs/CONFIG.md for full configuration details.</p>"},{"location":"CLI/#rank","title":"rank","text":"<p>Show ranked files by relevance without their content.</p> Bash<pre><code>tenets rank &lt;prompt&gt; [path] [options]\n</code></pre> <p>Arguments:</p> <ul> <li>prompt: Your query or task to rank files against</li> <li>path: Directory or files to analyze (default: current directory)</li> </ul> <p>Options:</p> <ul> <li><code>--format</code>, <code>-f</code>: Output format: markdown (default), json, xml, html, tree</li> <li><code>--output</code>, <code>-o</code>: Save to file instead of stdout</li> <li><code>--mode</code>, <code>-m</code>: Ranking mode: fast, balanced (default), thorough</li> <li><code>--top</code>, <code>-t</code>: Show only top N files</li> <li><code>--min-score</code>: Minimum relevance score (0.0-1.0)</li> <li><code>--max-files</code>: Maximum number of files to show</li> <li><code>--tree</code>: Show results as directory tree</li> <li><code>--scores/--no-scores</code>: Show/hide relevance scores (default: show)</li> <li><code>--factors</code>: Show ranking factor breakdown</li> <li><code>--path-style</code>: Path display: relative (default), absolute, name</li> <li><code>--include</code>, <code>-i</code>: Include file patterns (e.g., \".py,.js\")</li> <li><code>--exclude</code>, <code>-e</code>: Exclude file patterns (e.g., \"test_,.backup\")</li> <li><code>--include-tests</code>: Include test files</li> <li><code>--exclude-tests</code>: Explicitly exclude test files</li> <li><code>--no-git</code>: Disable git signals in ranking</li> <li><code>--session</code>, <code>-s</code>: Use session for stateful ranking</li> <li><code>--stats</code>: Show ranking statistics</li> <li><code>--verbose</code>, <code>-v</code>: Show detailed debug information</li> <li><code>--copy</code>: Copy file list to clipboard (also enabled automatically if config.output.copy_on_rank is true)</li> </ul> <p>Examples:</p> Bash<pre><code># Show top 10 most relevant files for OAuth implementation\ntenets rank \"implement OAuth2\" --top 10\n\n# Show files above a relevance threshold\ntenets rank \"fix authentication bug\" --min-score 0.3\n\n# Tree view with ranking factors breakdown\ntenets rank \"add caching layer\" --tree --factors\n\n# Export ranking as JSON for automation\ntenets rank \"review API endpoints\" --format json -o ranked_files.json\n\n# Quick file list to clipboard (no scores)\ntenets rank \"database queries\" --top 20 --copy --no-scores\n\n# Show only Python files with detailed factors\ntenets rank \"refactor models\" --include \"*.py\" --factors --stats\n\n# HTML report with interactive tree view\ntenets rank \"security audit\" --format html -o security_files.html --tree\n</code></pre> <p>Use Cases:</p> <ol> <li>Understanding Context: See which files would be included in a <code>distill</code> command without generating the full context</li> <li>File Discovery: Find relevant files for manual inspection</li> <li>Automation: Export ranked file lists for feeding into other tools or scripts</li> <li>Code Review: Identify files most relevant to a particular feature or bug</li> <li>Impact Analysis: See which files are most connected to a specific query</li> </ol> <p>Output Formats:</p> <ul> <li>Markdown: Numbered list sorted by relevance with scores and optional factors</li> <li>Tree: Directory tree structure sorted by relevance (directories ordered by their highest-scoring file)</li> <li>JSON: Structured data with paths, scores, ranks, and factors (preserves relevance order)</li> <li>XML: Structured XML for integration with other tools</li> <li>HTML: Interactive web page with relevance-sorted display</li> </ul> <p>The ranking uses the same intelligent multi-factor analysis as <code>distill</code>: - Semantic similarity (ML-based when available) - Keyword matching - BM25/TF-IDF statistical relevance - Import/dependency centrality - Path relevance - Git signals (recent changes, frequency)</p>"},{"location":"CLI/#examine","title":"examine","text":"<p>Analyze codebase structure, complexity, and patterns.</p> Bash<pre><code>tenets examine [path] [options]\n</code></pre> <p>Options: - <code>--deep, -d</code>: Perform deep analysis with AST parsing - <code>--output, -o</code>: Save results to file - <code>--metrics</code>: Show detailed code metrics - <code>--complexity</code>: Show complexity analysis - <code>--ownership</code>: Show code ownership (requires git) - <code>--hotspots</code>: Show frequently changed files - <code>--format, -f</code>: Output format: <code>table</code> (default), <code>json</code>, <code>yaml</code> - <code>--no-git</code>: Disable git analysis</p> <p>Examples:</p> Bash<pre><code># Basic analysis with summary table\ntenets examine\n\n# Deep analysis with metrics\ntenets examine --deep --metrics\n\n# Show complexity hotspots\ntenets examine --complexity --hotspots\n\n# Export full analysis as JSON\ntenets examine --output analysis.json --format json\n\n# Generate HTML examination report\ntenets examine --format html --output examination_report.html\n\n# Generate detailed HTML report with all analyses\ntenets examine --ownership --hotspots --show-details --format html -o report.html\n\n# Analyze specific directory with ownership tracking\ntenets examine ./src --ownership\n\n# Generate multiple format reports\ntenets examine --format json -o analysis.json\ntenets examine --format html -o analysis.html\ntenets examine --format markdown -o analysis.md\n</code></pre> <p>Coverage Reports:</p> Bash<pre><code># Run tests with coverage and generate HTML report\npytest --cov=tenets --cov-report=html\n\n# View HTML coverage report (opens htmlcov/index.html)\npython -m webbrowser htmlcov/index.html\n\n# Run tests with multiple coverage formats\npytest --cov=tenets --cov-report=html --cov-report=xml --cov-report=term\n\n# Run specific test module with coverage\npytest tests/cli/commands/test_examine.py --cov=tenets.cli.commands.examine --cov-report=html\n</code></pre> <p>Output Example (Table Format): Text Only<pre><code>Codebase Analysis\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Metric          \u2503 Value     \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 Total Files     \u2502 156       \u2502\n\u2502 Total Lines     \u2502 24,531    \u2502\n\u2502 Languages       \u2502 Python,   \u2502\n\u2502                 \u2502 JavaScript\u2502\n\u2502 Avg Complexity  \u2502 4.32      \u2502\n\u2502 Git Branch      \u2502 main      \u2502\n\u2502 Contributors    \u2502 8         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p>"},{"location":"CLI/#chronicle","title":"chronicle","text":"<p>Track code changes over time using git history.</p> Bash<pre><code>tenets chronicle [options]\n</code></pre> <p>Options: - <code>--since, -s</code>: Time period (e.g., \"yesterday\", \"last-month\", \"2024-01-01\") - <code>--path, -p</code>: Repository path (default: current directory) - <code>--author, -a</code>: Filter by author - <code>--limit, -n</code>: Maximum commits to display</p> <p>Examples:</p> Bash<pre><code># Changes in the last week\ntenets chronicle --since \"last-week\"\n\n# Changes since yesterday\ntenets chronicle --since yesterday\n\n# Filter by author\ntenets chronicle --author \"alice@example.com\"\n</code></pre>"},{"location":"CLI/#momentum","title":"momentum","text":"<p>Track development velocity and team productivity metrics.</p> Bash<pre><code>tenets momentum [options]\n</code></pre> <p>Options: - <code>--path, -p</code>: Repository path (default: current directory) - <code>--since, -s</code>: Time period (default: \"last-month\") - <code>--team</code>: Show team-wide statistics - <code>--author, -a</code>: Show stats for specific author</p> <p>Examples:</p> Bash<pre><code># Personal velocity for last month\ntenets momentum\n\n# Team velocity for the quarter\ntenets momentum --team --since \"3 months\"\n\n# Individual contributor stats\ntenets momentum --author \"alice@example.com\"\n</code></pre>"},{"location":"CLI/#instill","title":"instill","text":"<p>Apply tenets to your current context by injecting them into prompts and outputs.</p> Bash<pre><code>tenets instill [context] [options]\n</code></pre> <p>Options: - <code>--session, -s</code>: Session name for tracking - <code>--frequency</code>: Injection frequency: <code>always</code>, <code>periodic</code>, <code>adaptive</code> - <code>--priority</code>: Minimum tenet priority: <code>low</code>, <code>medium</code>, <code>high</code>, <code>critical</code> - <code>--max-tokens</code>: Maximum tokens to add - <code>--format</code>: Output format</p> <p>Examples:</p> Bash<pre><code># Apply all pending tenets\ntenets instill \"Current code context\"\n\n# Apply tenets for specific session\ntenets instill --session feature-x\n\n# Adaptive injection based on complexity\ntenets instill --frequency adaptive\n</code></pre>"},{"location":"CLI/#tenet","title":"tenet","text":"<p>Manage project tenets - rules and guidelines for your codebase.</p> Bash<pre><code>tenets tenet [subcommand] [options]\n</code></pre> <p>Subcommands: - <code>add</code>: Add a new tenet - <code>list</code>: List all tenets - <code>remove</code>: Remove a tenet - <code>show</code>: Show tenet details - <code>export</code>: Export tenets - <code>import</code>: Import tenets</p> <p>Examples:</p> Bash<pre><code># Add a new tenet\ntenets tenet add \"Always use type hints\"\n\n# List all tenets\ntenets tenet list\n\n# Remove a tenet\ntenets tenet remove &lt;tenet-id&gt;\n</code></pre>"},{"location":"CLI/#visualization-commands","title":"Visualization Commands","text":"<p>All visualization commands support ASCII output for terminal display, with optional graphical formats.</p>"},{"location":"CLI/#viz-deps","title":"viz deps","text":"<p>Visualize code dependencies and architecture with intelligent project detection.</p> Bash<pre><code>tenets viz deps [path] [options]\n</code></pre> <p>Options: - <code>--output, -o</code>: Save to file (e.g., architecture.svg) - <code>--format, -f</code>: Output format: <code>ascii</code>, <code>svg</code>, <code>png</code>, <code>html</code>, <code>json</code>, <code>dot</code> - <code>--level, -l</code>: Dependency level: <code>file</code> (default), <code>module</code>, <code>package</code> - <code>--cluster-by</code>: Group nodes by: <code>directory</code>, <code>module</code>, <code>package</code> - <code>--max-nodes</code>: Maximum nodes to display - <code>--include, -i</code>: Include file patterns (e.g., \".py\") - <code>--exclude, -e</code>: Exclude file patterns (e.g., \"*test\") - <code>--layout</code>: Graph layout: <code>hierarchical</code>, <code>circular</code>, <code>shell</code>, <code>kamada</code></p> <p>Features: - Auto-detection: Automatically detects project type (Python, Node.js, Java, Go, etc.) - Smart aggregation: Three levels of dependency views (file, module, package) - Interactive HTML: D3.js or Plotly-based interactive visualizations - Pure Python: All visualization libraries installable via <code>pip install tenets[viz]</code></p> <p>Examples:</p> Bash<pre><code># Auto-detect project type and show dependencies\ntenets viz deps\n\n# Generate interactive HTML visualization\ntenets viz deps --format html --output deps.html\n\n# Module-level dependencies as SVG\ntenets viz deps --level module --format svg --output modules.svg\n\n# Package architecture with clustering\ntenets viz deps --level package --cluster-by package --output packages.png\n\n# Circular layout for better visibility\ntenets viz deps --layout circular --format svg --output circular.svg\n\n# Limit to top 50 nodes for large projects\ntenets viz deps --max-nodes 50 --format png --output top50.png\n\n# Export to Graphviz DOT format\ntenets viz deps --format dot --output graph.dot\n\n# Filter specific files\ntenets viz deps src/ --include \"*.py\" --exclude \"*test*\"\n</code></pre> <p>ASCII Output Example: Text Only<pre><code>Dependency Tree\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\n\u251c\u2500\u2500 main.py\n\u2502   \u251c\u2500\u2500 auth/handler.py\n\u2502   \u2502   \u251c\u2500\u2500 auth/oauth.py\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 utils/crypto.py\n\u2502   \u2502   \u2514\u2500\u2500 models/user.py\n\u2502   \u2502       \u2514\u2500\u2500 db/base.py\n\u2502   \u2514\u2500\u2500 api/routes.py\n\u2502       \u251c\u2500\u2500 api/endpoints.py\n\u2502       \u2514\u2500\u2500 middleware/cors.py\n\u2514\u2500\u2500 config.py\n</code></pre></p>"},{"location":"CLI/#viz-complexity","title":"viz complexity","text":"<p>Visualize code complexity metrics.</p> Bash<pre><code>tenets viz complexity [path] [options]\n</code></pre> <p>Options: - <code>--output, -o</code>: Save to file - <code>--format, -f</code>: Format: <code>ascii</code>, <code>png</code>, <code>html</code> - <code>--metric, -m</code>: Metric type: <code>cyclomatic</code> (default), <code>cognitive</code> - <code>--threshold</code>: Highlight files above threshold - <code>--hotspots</code>: Focus on complexity hotspots</p> <p>Examples:</p> Bash<pre><code># ASCII bar chart of complexity\ntenets viz complexity\n\n# Show only high-complexity files\ntenets viz complexity --threshold 10 --hotspots\n\n# Save as image\ntenets viz complexity --output complexity.png\n</code></pre> <p>ASCII Output Example: Text Only<pre><code>Complexity Analysis (cyclomatic)\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\nauth/oauth.py                 \u25cf \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 28\nmodels/user.py               \u25d0 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 15\napi/endpoints.py             \u25d0 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 12\nutils/validators.py          \u25cf \u2588\u2588\u2588\u2588\u2588\u2588 8\nconfig/settings.py           \u25cf \u2588\u2588\u2588\u2588 5\n\nLegend: \u25cf Low  \u25d0 Medium  \u25d1 High  \u25cb Very High\n</code></pre></p>"},{"location":"CLI/#viz-coupling","title":"viz coupling","text":"<p>Visualize files that frequently change together.</p> Bash<pre><code>tenets viz coupling [path] [options]\n</code></pre> <p>Options: - <code>--output, -o</code>: Save to file - <code>--format, -f</code>: Format: <code>ascii</code>, <code>html</code> - <code>--min-coupling</code>: Minimum coupling count (default: 2)</p> <p>Examples:</p> Bash<pre><code># Show file coupling matrix\ntenets viz coupling\n\n# Only strong couplings\ntenets viz coupling --min-coupling 5\n\n# Interactive HTML matrix\ntenets viz coupling --output coupling.html\n</code></pre> <p>ASCII Output Example: Text Only<pre><code>File Coupling Matrix\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\n                    auth.py  user.py  api.py  test.py\nauth.py               -        8       3       12\nuser.py               8        -       5       10\napi.py                3        5       -       7\ntest_auth.py         12       10      7        -\n</code></pre></p>"},{"location":"CLI/#viz-contributors","title":"viz contributors","text":"<p>Visualize contributor activity and code ownership.</p> Bash<pre><code>tenets viz contributors [path] [options]\n</code></pre> <p>Options: - <code>--output, -o</code>: Save to file - <code>--format, -f</code>: Format: <code>ascii</code>, <code>png</code> - <code>--active</code>: Show only currently active contributors</p> <p>Examples:</p> Bash<pre><code># Contributor stats\ntenets viz contributors\n\n# Active contributors only\ntenets viz contributors --active\n</code></pre>"},{"location":"CLI/#session-commands","title":"Session Commands","text":"<p>Tenets can persist session state across distill runs. When a configuration is loaded, sessions are stored in a local SQLite database under the cache directory (see Storage below). Use <code>--session &lt;name&gt;</code> with commands like <code>distill</code> to build iterative context.</p> <ul> <li>Only one session is considered active at a time. Resuming a session will mark all others inactive.</li> <li>If a session NAME is omitted for <code>resume</code> or <code>exit</code>, Tenets operates on the currently active session.</li> </ul>"},{"location":"CLI/#session-create","title":"session create","text":"<p>Create a new analysis session.</p> Bash<pre><code>tenets session create &lt;name&gt;\n</code></pre> <p>Example: Bash<pre><code>tenets session create payment-integration\n</code></pre></p>"},{"location":"CLI/#session-start","title":"session start","text":"<p>Alias of <code>session create</code>.</p> Bash<pre><code>tenets session start &lt;name&gt;\n</code></pre>"},{"location":"CLI/#session-resume","title":"session resume","text":"<p>Mark an existing session as active.</p> Bash<pre><code># Resume the active session (if one exists)\ntenets session resume\n\n# Or specify by name\ntenets session resume &lt;name&gt;\n</code></pre>"},{"location":"CLI/#session-exit","title":"session exit","text":"<p>Mark a session as inactive.</p> Bash<pre><code># Exit the current active session\ntenets session exit\n\n# Or exit a specific session by name\ntenets session exit &lt;name&gt;\n</code></pre>"},{"location":"CLI/#session-list","title":"session list","text":"<p>List all sessions.</p> Bash<pre><code>tenets session list\n</code></pre> <p>The output includes an Active column (\"yes\" indicates the current session).</p>"},{"location":"CLI/#session-delete","title":"session delete","text":"<p>Delete a specific session.</p> Bash<pre><code>tenets session delete &lt;name&gt; [--keep-context]\n</code></pre> <p>Options: - <code>--keep-context</code>: Keep stored context artifacts (default: false)</p>"},{"location":"CLI/#session-reset","title":"session reset","text":"<p>Reset (delete and recreate) a session, purging its context.</p> Bash<pre><code>tenets session reset &lt;name&gt;\n</code></pre>"},{"location":"CLI/#session-clear","title":"session clear","text":"<p>Delete ALL sessions at once. Useful for clearing cache and starting fresh.</p> Bash<pre><code>tenets session clear [--keep-context]\n</code></pre> <p>Options: - <code>--keep-context</code>: Keep stored artifacts (default: false, deletes everything)</p> <p>Example: Bash<pre><code># Clear all sessions and their data\ntenets session clear\n\n# Clear sessions but preserve context files\ntenets session clear --keep-context\n</code></pre></p>"},{"location":"CLI/#session-show","title":"session show","text":"<p>Show details for a specific session.</p> Bash<pre><code>tenets session show &lt;name&gt;\n</code></pre>"},{"location":"CLI/#session-add","title":"session add","text":"<p>Attach arbitrary content to a session.</p> Bash<pre><code>tenets session add &lt;name&gt; &lt;kind&gt; &lt;file&gt;\n</code></pre> <p>Arguments: - <code>name</code>: Session name - <code>kind</code>: Content type tag (e.g., note, context_result) - <code>file</code>: File to attach</p> <p>Notes: - Creating or resetting a session marks it active. - Only one session is active at a time (resuming one deactivates others). - Session data is stored in SQLite under <code>~/.tenets/cache/sessions.db</code></p>"},{"location":"CLI/#tenet-commands","title":"Tenet Commands","text":"<p>Create and manage guiding principles (\u201ctenets\u201d) that can be injected into context.</p>"},{"location":"CLI/#tenet-add","title":"tenet add","text":"<p>Add a new tenet.</p> Bash<pre><code>tenets tenet add \"Always use type hints\" --priority high --category style\ntenets tenet add \"Validate all user inputs\" --priority critical --category security\ntenets tenet add \"Use async/await for I/O\" --session feature-x\n</code></pre> <p>Options: - <code>--priority, -p</code>: low | medium | high | critical (default: medium) - <code>--category, -c</code>: Freeform tag (e.g., architecture, security, style, performance, testing) - <code>--session, -s</code>: Bind tenet to a session</p>"},{"location":"CLI/#tenet-list","title":"tenet list","text":"<p>List tenets with filters.</p> Bash<pre><code>tenets tenet list\ntenets tenet list --pending\ntenets tenet list --session oauth --category security --verbose\n</code></pre> <p>Options: - <code>--pending</code>: Only pending - <code>--instilled</code>: Only instilled - <code>--session, -s</code>: Filter by session - <code>--category, -c</code>: Filter by category - <code>--verbose, -v</code>: Show full content and metadata</p>"},{"location":"CLI/#tenet-remove","title":"tenet remove","text":"<p>Remove a tenet by ID (partial ID accepted).</p> Bash<pre><code>tenets tenet remove abc123\ntenets tenet remove abc123 --force\n</code></pre>"},{"location":"CLI/#tenet-show","title":"tenet show","text":"<p>Show details for a tenet.</p> Bash<pre><code>tenets tenet show abc123\n</code></pre>"},{"location":"CLI/#tenet-export--import","title":"tenet export / import","text":"<p>Export/import tenets.</p> Bash<pre><code># Export to stdout or file\ntenets tenet export\ntenets tenet export --format json --session oauth -o team-tenets.json\n\n# Import from file (optionally into a session)\ntenets tenet import team-tenets.yml\ntenets tenet import standards.json --session feature-x\n</code></pre>"},{"location":"CLI/#instill-command","title":"Instill Command","text":"<p>Apply tenets to the current context with smart strategies (periodic/adaptive/manual).</p> Bash<pre><code>tenets instill [options]\n</code></pre> <p>Common options: - <code>--session, -s</code>: Use a named session for history and pinned files - <code>--force</code>: Force instillation regardless of frequency - <code>--max-tenets</code>: Cap number of tenets applied</p> <p>Examples:</p> Bash<pre><code># Apply pending tenets for a session\ntenets instill --session refactor-auth\n\n# Force all tenets once\ntenets instill --force\n</code></pre>"},{"location":"CLI/#system-instruction-commands","title":"System Instruction Commands","text":"<p>Manage the system instruction (system prompt) that can be auto-injected at the start of a session\u2019s first distill (or every output if no session is used).</p>"},{"location":"CLI/#system-instruction-set","title":"system-instruction set","text":"<p>Set/update the system instruction and options.</p> Bash<pre><code>tenets system-instruction set \"You are a helpful coding assistant\" \\\n  --enable \\\n  --position top \\\n  --format markdown\n\n# From file\ntenets system-instruction set --file prompts/system.md --enable\n</code></pre> <p>Options: - <code>--file, -f</code>: Read instruction from file - <code>--enable/--disable</code>: Enable or disable auto-injection - <code>--position</code>: Placement: <code>top</code>, <code>after_header</code>, <code>before_content</code> - <code>--format</code>: Format of injected block: <code>markdown</code>, <code>xml</code>, <code>comment</code>, <code>plain</code> - <code>--save/--no-save</code>: Persist to config</p>"},{"location":"CLI/#system-instruction-show","title":"system-instruction show","text":"<p>Display current configuration and instruction.</p> Bash<pre><code>tenets system-instruction show\ntenets system-instruction show --raw\n</code></pre> <p>Options: - <code>--raw</code>: Print raw instruction only</p>"},{"location":"CLI/#system-instruction-clear","title":"system-instruction clear","text":"<p>Clear and disable the system instruction.</p> Bash<pre><code>tenets system-instruction clear\ntenets system-instruction clear --yes\n</code></pre> <p>Options: - <code>--yes, -y</code>: Skip confirmation</p>"},{"location":"CLI/#system-instruction-test","title":"system-instruction test","text":"<p>Preview how injection would modify content.</p> Bash<pre><code>tenets system-instruction test\ntenets system-instruction test --session my-session\n</code></pre> <p>Options: - <code>--session</code>: Test with a session to respect once-per-session behavior</p>"},{"location":"CLI/#system-instruction-export","title":"system-instruction export","text":"<p>Export the instruction to a file.</p> Bash<pre><code>tenets system-instruction export prompts/system.md\n</code></pre>"},{"location":"CLI/#system-instruction-validate","title":"system-instruction validate","text":"<p>Validate the instruction for basic issues and optional token estimates.</p> Bash<pre><code>tenets system-instruction validate\ntenets system-instruction validate --tokens --max-tokens 800\n</code></pre> <p>Options: - <code>--tokens</code>: Show a rough token estimate - <code>--max-tokens</code>: Threshold for warnings/errors</p>"},{"location":"CLI/#system-instruction-edit","title":"system-instruction edit","text":"<p>Edit the instruction in your editor and save changes back to config.</p> Bash<pre><code>tenets system-instruction edit\ntenets system-instruction edit --editor code\n</code></pre>"},{"location":"CLI/#session-show_1","title":"session show","text":"<p>Show session details.</p> Bash<pre><code>tenets session show &lt;name&gt;\n</code></pre>"},{"location":"CLI/#session-add_1","title":"session add","text":"<p>Attach an artifact (stored as text) to a session.</p> Bash<pre><code>tenets session add &lt;name&gt; &lt;kind&gt; &lt;file&gt;\n</code></pre> <p>Examples of <code>kind</code>: <code>note</code>, <code>context_result</code>, <code>summary</code></p>"},{"location":"CLI/#session-reset_1","title":"session reset","text":"<p>Reset (delete and recreate) a session and purge its context.</p> Bash<pre><code>tenets session reset &lt;name&gt;\n</code></pre>"},{"location":"CLI/#session-delete_1","title":"session delete","text":"<p>Delete a session. Optionally keep stored artifacts.</p> Bash<pre><code>tenets session delete &lt;name&gt; [--keep-context]\n</code></pre>"},{"location":"CLI/#cache-management","title":"Cache Management","text":"Text Only<pre><code># Show cache stats (path, file count, size)\ntenets config cache-stats\n\n# Cleanup old/oversized entries respecting TTL\ntenets config cleanup-cache\n\n# Clear ALL caches (analysis + general) \u2013 destructive\ntenets config clear-cache --yes\n</code></pre> <p>Git data is used strictly for ranking relevance unless explicitly requested via commands like <code>chronicle</code> or <code>viz contributors</code>; it is not embedded in <code>distill</code> output.</p>"},{"location":"CLI/#configuration","title":"Configuration","text":""},{"location":"CLI/#config-set","title":"config set","text":"<p>Set configuration values.</p> Bash<pre><code>tenets config set &lt;key&gt; &lt;value&gt;\n</code></pre> <p>Examples: Bash<pre><code># Set default ranking algorithm\ntenets config set ranking.algorithm balanced\n\n# Set maximum file size\ntenets config set scanner.max_file_size 10000000\n\n# Enable ML features\ntenets config set nlp.use_embeddings true\n</code></pre></p>"},{"location":"CLI/#config-show","title":"config show","text":"<p>Show configuration.</p> Bash<pre><code>tenets config show [options]\n</code></pre> <p>Options: - <code>--key, -k</code>: Show specific key</p> <p>Examples: Bash<pre><code># Show all config\ntenets config show\n\n# Show model costs\ntenets config show --key costs\n\n# Show specific setting\ntenets config show --key ranking.algorithm\n</code></pre></p>"},{"location":"CLI/#storage","title":"Storage","text":"<p>Writable data is stored in a user/project cache directory:</p> <ul> <li>Default: <code>${HOME}/.tenets/cache</code> (Windows: <code>%USERPROFILE%\\\\.tenets\\\\cache</code>)</li> <li>Main DB: <code>${CACHE_DIR}/tenets.db</code> (sessions and future state)</li> <li>Analysis cache: <code>${CACHE_DIR}/analysis/analysis.db</code></li> </ul> <p>Override via <code>.tenets.yml</code>:</p> YAML<pre><code>cache:\n  directory: /path/to/custom/cache\n</code></pre> <p>Or environment:</p> Bash<pre><code>TENETS_CACHE_DIRECTORY=/path/to/custom/cache\n</code></pre> <p>Note on cost estimation: When <code>--estimate-cost</code> is used with <code>distill</code>, Tenets estimates costs using model limits and the built-in pricing table from <code>SUPPORTED_MODELS</code>.</p>"},{"location":"CLI/#common-use-cases","title":"Common Use Cases","text":""},{"location":"CLI/#1-ai-pair-programming","title":"1. AI Pair Programming","text":"<p>Generate context for ChatGPT/Claude when working on features:</p> Bash<pre><code># Initial context for new feature\ntenets distill \"implement user authentication with JWT\" &gt; auth_context.md\n\n# Paste auth_context.md into ChatGPT, then iterate:\ntenets distill \"add password reset functionality\" --session auth-feature\n\n# AI needs to see session info?\ntenets session show auth-feature\n</code></pre>"},{"location":"CLI/#2-code-review-preparation","title":"2. Code Review Preparation","text":"<p>Understand what changed and why:</p> Bash<pre><code># See what changed in the sprint\ntenets chronicle --since \"2 weeks\" --summary\n\n# Get context for reviewing a PR\ntenets distill \"review payment processing changes\"\n\n# Check complexity of changed files\ntenets examine --complexity --hotspots\n</code></pre>"},{"location":"CLI/#3-onboarding-to-new-codebase","title":"3. Onboarding to New Codebase","text":"<p>Quickly understand project structure:</p> Bash<pre><code># Get project overview\ntenets examine --metrics\n\n# Visualize architecture\ntenets viz deps --format ascii\n\n# Find the most complex areas\ntenets viz complexity --hotspots\n\n# See who knows what\ntenets viz contributors\n</code></pre>"},{"location":"CLI/#4-debugging-production-issues","title":"4. Debugging Production Issues","text":"<p>Find relevant code for debugging:</p> Bash<pre><code># Get all context related to the error\ntenets distill \"users getting 500 error on checkout\" --mode thorough\n\n# Include recent changes summary\ntenets chronicle --since \"last-deploy\"\n\n# Search for patterns within a session by iterating with prompts\ntenets distill \"find error handlers\" --session debug-session\n</code></pre>"},{"location":"CLI/#5-technical-debt-assessment","title":"5. Technical Debt Assessment","text":"<p>Identify areas needing refactoring:</p> Bash<pre><code># Find complex files\ntenets examine --complexity --threshold 15\n\n# Find tightly coupled code\ntenets viz coupling --min-coupling 5\n\n# Track velocity trends\ntenets momentum --team --since \"6 months\"\n</code></pre>"},{"location":"CLI/#6-architecture-documentation","title":"6. Architecture Documentation","text":"<p>Generate architecture insights:</p> Bash<pre><code># Export dependency graph\ntenets viz deps --output architecture.svg --cluster-by directory\n\n# Generate comprehensive analysis\ntenets examine --deep --output analysis.json --format json\n\n# Create context for documentation\ntenets distill \"document API architecture\" ./src/api\n</code></pre>"},{"location":"CLI/#examples","title":"Examples","text":""},{"location":"CLI/#complete-workflow-example","title":"Complete Workflow Example","text":"Bash<pre><code># 1. Start a new feature\ntenets session create oauth-integration\n\n# 2. Get initial context\ntenets distill \"implement OAuth2 with Google and GitHub\" \\\n  --session oauth-integration \\\n  --include \"*.py,*.yaml\" \\\n  --exclude \"test_*\" \\\n  --model gpt-4o \\\n  --estimate-cost &gt; oauth_context.md\n\n# 3. Paste into ChatGPT, start coding...\n\n# 4. AI needs more specific context\n# (Show session details)\ntenets session show oauth-integration\n\n# 5. Check your progress\ntenets chronicle --since \"today\"\n\n# 6. Visualize what you built\ntenets viz deps src/auth --format ascii\n\n# 7. Check complexity\ntenets examine src/auth --complexity\n\n# 8. Prepare for review\ntenets distill \"OAuth implementation ready for review\" \\\n  --session oauth-integration\n</code></pre>"},{"location":"CLI/#configuration-file-example","title":"Configuration File Example","text":"<p>Create <code>.tenets.yml</code> in your project:</p> YAML<pre><code># .tenets.yml\ncontext:\n  ranking: balanced\n  max_tokens: 100000\n  include_git: true\n\nscanner:\n  respect_gitignore: true\n  max_file_size: 5000000\n\nignore:\n  - \"*.generated.*\"\n  - \"vendor/\"\n  - \"build/\"\n\noutput:\n  format: markdown\n  summarize_long_files: true\n</code></pre>"},{"location":"CLI/#tips-and-tricks","title":"Tips and Tricks","text":"<ol> <li>Start with fast mode for quick exploration, use thorough for complex tasks</li> <li>Use sessions for multi-step features to maintain context</li> <li>ASCII visualizations are great for README files and documentation</li> <li>Combine commands - examine first, then distill with insights</li> <li>Git integration works automatically - no setup needed</li> <li>Include/exclude patterns support standard glob syntax</li> <li>Cost estimation helps budget API usage before sending to LLMs</li> </ol>"},{"location":"CLI/#environment-variables","title":"Environment Variables","text":"<ul> <li><code>TENETS_CONFIG_PATH</code>: Custom config file location</li> <li><code>TENETS_LOG_LEVEL</code>: Set log level (DEBUG, INFO, WARNING, ERROR)</li> <li><code>TENETS_CACHE_DIR</code>: Custom cache directory</li> <li><code>TENETS_NO_COLOR</code>: Disable colored output</li> </ul>"},{"location":"CLI/#exit-codes","title":"Exit Codes","text":"<ul> <li><code>0</code>: Success</li> <li><code>1</code>: General error</li> <li><code>2</code>: Invalid arguments</li> <li><code>3</code>: File not found</li> <li><code>4</code>: Git repository required but not found</li> </ul> <p>For more information, visit https://github.com/jddunn/tenets</p>"},{"location":"CLI/#verbosity--output-controls","title":"Verbosity &amp; Output Controls","text":"<p>Control log verbosity globally:</p> Bash<pre><code># Default (warnings and above only)\nTENETS_LOG_LEVEL=WARNING tenets distill \"add caching layer\"\n\n# Verbose\ntenets --verbose distill \"add caching layer\"\n\n# Quiet / errors only\ntenets --quiet distill \"add caching layer\"\n# or\ntenets --silent distill \"add caching layer\"\n</code></pre> <p>The <code>distill</code> command includes a Suggestions section when no files are included, with tips to adjust relevance thresholds, token budget, and include patterns.</p>"},{"location":"CODE_OF_CONDUCT/","title":"Code of Conduct","text":"<p>We are committed to a respectful, inclusive, and collaborative community. This Code of Conduct applies to all project spaces (GitHub issues/PRs, discussions, docs, chat) and anyone interacting with the project.</p>"},{"location":"CODE_OF_CONDUCT/#our-standards","title":"Our Standards","text":"<p>Positive behaviors: - Being welcoming, empathetic, and considerate - Giving and gracefully accepting constructive feedback - Focusing on what is best for the community - Showing respect for differing viewpoints and experiences</p> <p>Unacceptable behaviors: - Harassment, intimidation, or discrimination of any kind - Personal attacks or insults - Doxxing or sharing private information - Trolling, excessive disruption, or derailing conversations - Sexualized language or imagery; unwelcome advances</p>"},{"location":"CODE_OF_CONDUCT/#scope","title":"Scope","text":"<p>Applies within all project spaces and when representing the project in public.</p>"},{"location":"CODE_OF_CONDUCT/#enforcement","title":"Enforcement","text":"<p>Report incidents to: team@tenets.dev (core maintainers). Include: - Your contact (optional) - Names, links, or references involved - Context, timeline, and any evidence (screenshots, logs)</p> <p>Reports are handled confidentially. Maintainers may take any reasonable action: - Verbal / written warning - Temporary or permanent ban from interactions - Removal of unacceptable content - Escalation to hosting platforms if required</p>"},{"location":"CODE_OF_CONDUCT/#maintainer-responsibilities","title":"Maintainer Responsibilities","text":"<p>Maintainers must model acceptable behavior and are responsible for clarifying standards and taking corrective action when misconduct occurs.</p>"},{"location":"CODE_OF_CONDUCT/#attribution","title":"Attribution","text":"<p>Adapted from the Contributor Covenant v2.1 (https://www.contributor-covenant.org) with project-specific clarifications.</p>"},{"location":"CODE_OF_CONDUCT/#contact","title":"Contact","text":"<p>Questions or concerns: team@tenets.dev</p> <p>We strive for a community where all contributors feel safe and empowered to improve Tenets.</p>"},{"location":"CONFIG/","title":"Configuration Guide","text":"<p>Comprehensive guide to configuring Tenets for optimal code context building.</p>"},{"location":"CONFIG/#overview","title":"Overview","text":"<p>Tenets uses a hierarchical configuration system with multiple override levels:</p> <p>Precedence (lowest \u2192 highest): 1. Default configuration (built-in) 2. Project file (<code>.tenets.yml</code> at repo root) 3. User file (<code>~/.config/tenets/config.yml</code> or <code>~/.tenets.yml</code>) 4. Environment variables (<code>TENETS_*</code>) 5. CLI flags (<code>--mode</code>, <code>--max-tokens</code>, etc.) 6. Programmatic overrides (<code>Tenets(config=...)</code>)</p> <p>Inspect configuration: Bash<pre><code>tenets config show                # Full config\ntenets config show --key ranking  # Specific section\ntenets config show --format json  # JSON output\n</code></pre></p>"},{"location":"CONFIG/#files-and-locations","title":"Files and locations","text":"<p>Tenets searches these locations in order and uses the first it finds: - ./.tenets.yml - ./.tenets.yaml - ./tenets.yml - ./.config/tenets.yml - ~/.config/tenets/config.yml - ~/.tenets.yml</p> <p>Create a starter file:</p> <ul> <li>tenets config init  # writes .tenets.yml in the current directory</li> </ul>"},{"location":"CONFIG/#complete-configuration-schema","title":"Complete Configuration Schema","text":"<p>All available configuration sections and their options:</p> YAML<pre><code># ============= Core Settings =============\nmax_tokens: 100000          # Maximum tokens for context (default: 100000)\ndebug: false                # Enable debug logging\nquiet: false                # Suppress non-essential output\n\n# ============= File Scanning =============\nscanner:\n  respect_gitignore: true          # Honor .gitignore patterns\n  follow_symlinks: false           # Follow symbolic links\n  max_file_size: 5000000          # Max file size in bytes (5MB)\n  max_files: 10000                # Maximum files to scan\n  binary_check: true              # Skip binary files\n  encoding: utf-8                 # File encoding\n  workers: 4                      # Parallel scanning workers\n  parallel_mode: auto             # auto | thread | process\n  timeout: 5.0                    # Timeout per file (seconds)\n  exclude_minified: true          # Skip minified files\n  exclude_tests_by_default: true  # Skip test files unless explicit\n\n  # Ignore patterns (in addition to .gitignore)\n  additional_ignore_patterns:\n    - '*.generated.*'\n    - vendor/\n    - node_modules/\n    - '*.egg-info/'\n    - __pycache__/\n    - .pytest_cache/\n\n  # Test file patterns\n  test_patterns:\n    - test_*.py\n    - '*_test.py'\n    - '*.test.js'\n    - '*.spec.ts'\n\n  # Test directories\n  test_directories:\n    - test\n    - tests\n    - __tests__\n    - spec\n\n# ============= Ranking System =============\nranking:\n  algorithm: balanced             # fast | balanced | thorough | ml | custom\n  threshold: 0.10                 # 0.0-1.0 (lower includes more files)\n  text_similarity_algorithm: bm25 # bm25 (default) | tfidf (optional)\n  text_similarity_algorithm: bm25  # Using BM25 (default)               # Deprecated - use text_similarity_algorithm instead\n  use_stopwords: false           # Filter common tokens\n  use_embeddings: false          # Semantic similarity (requires ML)\n  use_git: true                  # Include git signals\n  use_ml: false                  # Machine learning features\n  embedding_model: all-MiniLM-L6-v2  # Embedding model name\n  workers: 2                     # Parallel ranking workers\n  parallel_mode: auto            # thread | process | auto\n  batch_size: 100               # Files per batch\n\n  # Custom factor weights (0.0-1.0)\n  custom_weights:\n    keyword_match: 0.25\n    path_relevance: 0.20\n    import_graph: 0.20\n    git_activity: 0.15\n    file_type: 0.10\n    complexity: 0.10\n\n# ============= Summarization =============\nsummarizer:\n  default_mode: auto             # auto | extractive | abstractive\n  target_ratio: 0.3              # Target compression ratio\n  enable_cache: true             # Cache summaries\n  preserve_code_structure: true  # Keep code structure intact\n  summarize_imports: true        # Condense import statements\n  import_summary_threshold: 5    # Min imports to trigger summary\n  max_cache_size: 100           # Max cached summaries\n  quality_threshold: medium      # low | medium | high\n  batch_size: 10                # Files per batch\n  docstring_weight: 0.5         # Weight for docstrings\n  include_all_signatures: true   # Include all function signatures\n\n  # LLM settings (optional)\n  llm_provider: null            # openai | anthropic | null\n  llm_model: null               # Model name\n  llm_temperature: 0.3          # Creativity (0.0-1.0)\n  llm_max_tokens: 500           # Max tokens per summary\n  enable_ml_strategies: false    # Use ML summarization\n\n# ============= Tenet System =============\ntenet:\n  auto_instill: true              # Auto-apply tenets\n  max_per_context: 5              # Max tenets per context\n  reinforcement: true             # Reinforce important tenets\n  injection_strategy: strategic   # strategic | sequential | random\n  min_distance_between: 1000      # Min chars between injections\n  prefer_natural_breaks: true     # Insert at natural boundaries\n  storage_path: ~/.tenets/tenets  # Tenet storage location\n  collections_enabled: true       # Enable tenet collections\n\n  # Injection frequency\n  injection_frequency: adaptive   # always | periodic | adaptive | manual\n  injection_interval: 3           # For periodic mode\n  session_complexity_threshold: 0.7  # Triggers adaptive injection\n  min_session_length: 5           # Min prompts before injection\n\n  # Advanced settings\n  adaptive_injection: true        # Smart injection timing\n  track_injection_history: true   # Track what was injected\n  decay_rate: 0.1                # How fast tenets decay\n  reinforcement_interval: 10      # Reinforce every N prompts\n  session_aware: true            # Use session context\n  session_memory_limit: 100      # Max session history\n  persist_session_history: true   # Save session data\n\n  # Priority settings\n  priority_boost_critical: 2.0    # Boost for critical tenets\n  priority_boost_high: 1.5       # Boost for high priority\n  skip_low_priority_on_complex: true  # Skip low priority when complex\n\n  # System instruction\n  system_instruction: null        # Global system instruction\n  system_instruction_enabled: false  # Enable system instruction\n  system_instruction_position: top   # top | bottom\n  system_instruction_format: markdown  # markdown | plain\n  system_instruction_once_per_session: true  # Inject once per session\n\n# ============= Caching =============\ncache:\n  enabled: true                  # Enable caching\n  directory: ~/.tenets/cache     # Cache directory\n  ttl_days: 7                   # Time to live (days)\n  max_size_mb: 500              # Max cache size (MB)\n  compression: false            # Compress cache data\n  memory_cache_size: 1000       # In-memory cache entries\n  max_age_hours: 24            # Max cache age (hours)\n\n  # SQLite settings\n  sqlite_pragmas:\n    journal_mode: WAL\n    synchronous: NORMAL\n    cache_size: '-64000'\n    temp_store: MEMORY\n\n  # LLM cache\n  llm_cache_enabled: true       # Cache LLM responses\n  llm_cache_ttl_hours: 24      # LLM cache TTL\n\n# ============= Output Formatting =============\noutput:\n  default_format: markdown       # markdown | xml | json | html\n  syntax_highlighting: true      # Enable syntax highlighting\n  line_numbers: false           # Show line numbers\n  max_line_length: 120          # Max line length\n  include_metadata: true        # Include metadata\n  compression_threshold: 10000  # Compress if larger (chars)\n  summary_ratio: 0.25           # Summary compression ratio\n  copy_on_distill: false        # Auto-copy to clipboard\n  show_token_usage: true        # Show token counts\n  show_cost_estimate: true      # Show LLM cost estimates\n\n# ============= Git Integration =============\ngit:\n  enabled: true                 # Use git information\n  include_history: true         # Include commit history\n  history_limit: 100           # Max commits to analyze\n  include_blame: false         # Include git blame\n  include_stats: true          # Include statistics\n\n  # Ignore these authors\n  ignore_authors:\n    - dependabot[bot]\n    - github-actions[bot]\n    - renovate[bot]\n\n  # Main branch names\n  main_branches:\n    - main\n    - master\n    - develop\n    - trunk\n\n# ============= NLP Settings =============\nnlp:\n  enabled: true                    # Enable NLP features\n  stopwords_enabled: true          # Use stopwords\n  code_stopword_set: minimal       # minimal | standard | aggressive\n  prompt_stopword_set: aggressive  # minimal | standard | aggressive\n  custom_stopword_files: []        # Custom stopword files\n\n  # Tokenization\n  tokenization_mode: auto          # auto | simple | advanced\n  preserve_original_tokens: true   # Keep original tokens\n  split_camelcase: true           # Split CamelCase\n  split_snakecase: true           # Split snake_case\n  min_token_length: 2             # Min token length\n\n  # Keyword extraction\n  keyword_extraction_method: auto  # auto | rake | yake | bm25 | tfidf\n  max_keywords: 30                # Max keywords to extract\n  ngram_size: 3                  # N-gram size\n  yake_dedup_threshold: 0.7      # YAKE deduplication\n\n  # BM25 settings\n  bm25_k1: 1.2                   # Term frequency saturation parameter\n  bm25_b: 0.75                   # Length normalization parameter\n\n  # TF-IDF settings (when explicitly configured as alternative to BM25)\n  tfidf_use_sublinear: true      # Sublinear TF scaling (only when TF-IDF is used)\n  tfidf_use_idf: true           # Use IDF\n  tfidf_norm: l2                # Normalization\n\n  # Embeddings\n  embeddings_enabled: false       # Enable embeddings\n  embeddings_model: all-MiniLM-L6-v2  # Model name\n  embeddings_device: auto        # cpu | cuda | auto\n  embeddings_cache: true         # Cache embeddings\n  embeddings_batch_size: 32      # Batch size\n  similarity_metric: cosine      # cosine | euclidean | manhattan\n  similarity_threshold: 0.7      # Similarity threshold\n\n  # Cache settings\n  cache_embeddings_ttl_days: 30  # Embeddings cache TTL\n  cache_tfidf_ttl_days: 7       # BM25/TF-IDF cache TTL\n  cache_keywords_ttl_days: 7     # Keywords cache TTL\n\n  # Performance\n  multiprocessing_enabled: true   # Use multiprocessing\n  multiprocessing_workers: null   # null = auto-detect\n  multiprocessing_chunk_size: 100 # Chunk size\n\n# ============= LLM Settings (Optional) =============\nllm:\n  enabled: false                # Enable LLM features\n  provider: openai              # openai | anthropic | ollama\n  fallback_providers:           # Fallback providers\n    - anthropic\n    - openrouter\n\n  # API keys (use environment variables)\n  api_keys:\n    openai: ${OPENAI_API_KEY}\n    anthropic: ${ANTHROPIC_API_KEY}\n    openrouter: ${OPENROUTER_API_KEY}\n\n  # API endpoints\n  api_base_urls:\n    openai: https://api.openai.com/v1\n    anthropic: https://api.anthropic.com/v1\n    openrouter: https://openrouter.ai/api/v1\n    ollama: http://localhost:11434\n\n  # Model selection\n  models:\n    default: gpt-4o-mini\n    summarization: gpt-3.5-turbo\n    analysis: gpt-4o\n    embeddings: text-embedding-3-small\n    code_generation: gpt-4o\n\n  # Rate limits and costs\n  max_cost_per_run: 0.1         # Max $ per run\n  max_cost_per_day: 10.0        # Max $ per day\n  max_tokens_per_request: 4000   # Max tokens per request\n  max_context_length: 100000     # Max context length\n\n  # Generation settings\n  temperature: 0.3              # Creativity (0.0-1.0)\n  top_p: 0.95                  # Nucleus sampling\n  frequency_penalty: 0.0        # Frequency penalty\n  presence_penalty: 0.0         # Presence penalty\n\n  # Network settings\n  requests_per_minute: 60       # Rate limit\n  retry_on_error: true         # Retry failed requests\n  max_retries: 3              # Max retry attempts\n  retry_delay: 1.0            # Initial retry delay\n  retry_backoff: 2.0          # Backoff multiplier\n  timeout: 30                 # Request timeout (seconds)\n  stream: false               # Stream responses\n\n  # Logging and caching\n  cache_responses: true        # Cache LLM responses\n  cache_ttl_hours: 24         # Cache TTL (hours)\n  log_requests: false         # Log requests\n  log_responses: false        # Log responses\n\n# ============= Custom Settings =============\ncustom: {}  # User-defined custom settings\n</code></pre>"},{"location":"CONFIG/#key-configuration-notes","title":"Key Configuration Notes","text":"<p>Ranking: - <code>threshold</code>: Lower values (0.05-0.10) include more files, higher (0.20-0.30) for stricter matching - <code>algorithm</code>:   - <code>fast</code>: Quick keyword matching (~10ms/file)   - <code>balanced</code>: Structural analysis + BM25 (default)   - <code>thorough</code>: Full analysis with relationships   - <code>ml</code>: Machine learning with embeddings (requires extras) - <code>custom_weights</code>: Fine-tune ranking factors (values 0.0-1.0)</p> <p>Scanner: - <code>respect_gitignore</code>: Always honors .gitignore patterns - <code>exclude_tests_by_default</code>: Tests excluded unless <code>--include-tests</code> used - <code>additional_ignore_patterns</code>: Added to built-in patterns</p> <p>Tenet System: - <code>auto_instill</code>: Automatically applies relevant tenets to context - <code>injection_frequency</code>:   - <code>always</code>: Every distill   - <code>periodic</code>: Every N distills   - <code>adaptive</code>: Based on complexity   - <code>manual</code>: Only when explicitly called - <code>system_instruction</code>: Global instruction added to all contexts</p> <p>Output: - <code>copy_on_distill</code>: Auto-copy result to clipboard - <code>default_format</code>: Default output format (markdown recommended for LLMs)</p> <p>Performance: - <code>workers</code>: More workers = faster but more CPU/memory - <code>cache.enabled</code>: Significantly speeds up repeated operations - <code>ranking.batch_size</code>: Larger batches = more memory but faster</p>"},{"location":"CONFIG/#environment-variable-overrides","title":"Environment Variable Overrides","text":"<p>Any configuration option can be overridden via environment variables.</p> <p>Format: - Nested keys: <code>TENETS_&lt;SECTION&gt;_&lt;KEY&gt;=value</code> - Top-level keys: <code>TENETS_&lt;KEY&gt;=value</code> - Lists: Comma-separated values - Booleans: <code>true</code> or <code>false</code> (case-insensitive)</p> <p>Common Examples: Bash<pre><code># Core settings\nexport TENETS_MAX_TOKENS=150000\nexport TENETS_DEBUG=true\nexport TENETS_QUIET=false\n\n# Ranking configuration\nexport TENETS_RANKING_ALGORITHM=thorough\nexport TENETS_RANKING_THRESHOLD=0.05\nexport TENETS_RANKING_TEXT_SIMILARITY_ALGORITHM=tfidf  # Use TF-IDF instead of BM25\nexport TENETS_RANKING_USE_EMBEDDINGS=true\nexport TENETS_RANKING_WORKERS=4\n\n# Scanner settings\nexport TENETS_SCANNER_MAX_FILE_SIZE=10000000\nexport TENETS_SCANNER_RESPECT_GITIGNORE=true\nexport TENETS_SCANNER_EXCLUDE_TESTS_BY_DEFAULT=false\n\n# Output settings\nexport TENETS_OUTPUT_DEFAULT_FORMAT=xml\nexport TENETS_OUTPUT_COPY_ON_DISTILL=true\nexport TENETS_OUTPUT_SHOW_TOKEN_USAGE=false\n\n# Cache settings\nexport TENETS_CACHE_ENABLED=false\nexport TENETS_CACHE_DIRECTORY=/tmp/tenets-cache\nexport TENETS_CACHE_TTL_DAYS=14\n\n# Git settings\nexport TENETS_GIT_ENABLED=false\nexport TENETS_GIT_HISTORY_LIMIT=50\n\n# Tenet system\nexport TENETS_TENET_AUTO_INSTILL=false\nexport TENETS_TENET_MAX_PER_CONTEXT=10\nexport TENETS_TENET_INJECTION_FREQUENCY=periodic\nexport TENETS_TENET_INJECTION_INTERVAL=5\n\n# System instruction\nexport TENETS_TENET_SYSTEM_INSTRUCTION=\"You are a senior engineer. Focus on security and performance.\"\nexport TENETS_TENET_SYSTEM_INSTRUCTION_ENABLED=true\n</code></pre></p> <p>Usage Patterns: Bash<pre><code># One-time override\nTENETS_RANKING_ALGORITHM=fast tenets distill \"fix bug\"\n\n# Session-wide settings\nexport TENETS_RANKING_THRESHOLD=0.05\nexport TENETS_OUTPUT_COPY_ON_DISTILL=true\ntenets distill \"implement feature\"  # Uses exported settings\n\n# Verify configuration\ntenets config show --key ranking\ntenets config show --format json | jq '.ranking'\n</code></pre></p>"},{"location":"CONFIG/#cli-flags-and-programmatic-control","title":"CLI Flags and Programmatic Control","text":""},{"location":"CONFIG/#cli-flags","title":"CLI Flags","text":"<p>Command-line flags override configuration for that specific run:</p> Bash<pre><code># Core overrides\ntenets distill \"query\" --max-tokens 50000\ntenets distill \"query\" --format xml\ntenets distill \"query\" --copy\n\n# Ranking mode\ntenets distill \"query\" --mode fast      # Quick analysis\ntenets distill \"query\" --mode thorough  # Deep analysis\ntenets distill \"query\" --mode ml        # With embeddings\n\n# File filtering\ntenets distill \"query\" --include \"*.py\" --exclude \"test_*.py\"\ntenets distill \"query\" --include-tests  # Include test files\n\n# Git control\ntenets distill \"query\" --no-git  # Disable git signals\n\n# Session management\ntenets distill \"query\" --session feature-x\n\n# Content optimization\ntenets distill \"query\" --condense        # Aggressive compression\ntenets distill \"query\" --remove-comments # Strip comments\ntenets distill \"query\" --full            # No summarization\n</code></pre>"},{"location":"CONFIG/#programmatic-configuration","title":"Programmatic Configuration","text":"<p>Basic usage with custom config: Python<pre><code>from tenets import Tenets\nfrom tenets.config import TenetsConfig\n\n# Create custom configuration\nconfig = TenetsConfig(\n    max_tokens=150000,\n    ranking={\n        \"algorithm\": \"thorough\",\n        \"threshold\": 0.05,\n        \"text_similarity_algorithm\": \"bm25\",  # or \"tfidf\" for TF-IDF\n        \"use_embeddings\": True,\n        \"workers\": 4,\n        \"custom_weights\": {\n            \"keyword_match\": 0.30,\n            \"path_relevance\": 0.25,\n            \"git_activity\": 0.20,\n        }\n    },\n    scanner={\n        \"respect_gitignore\": True,\n        \"max_file_size\": 10_000_000,\n        \"exclude_tests_by_default\": False,\n    },\n    output={\n        \"default_format\": \"xml\",\n        \"copy_on_distill\": True,\n    },\n    tenet={\n        \"auto_instill\": True,\n        \"max_per_context\": 10,\n        \"system_instruction\": \"Focus on security and performance\",\n        \"system_instruction_enabled\": True,\n    }\n)\n\n# Initialize with custom config\ntenets = Tenets(config=config)\n\n# Use it\nresult = tenets.distill(\n    \"implement caching layer\",\n    max_tokens=80000,  # Override config for this call\n    mode=\"balanced\",    # Override algorithm\n)\n</code></pre></p> <p>Load and modify existing config: Python<pre><code>from tenets import Tenets\nfrom tenets.config import TenetsConfig\n\n# Load from file\nconfig = TenetsConfig.from_file(\".tenets.yml\")\n\n# Modify specific settings\nconfig.ranking.algorithm = \"fast\"\nconfig.ranking.threshold = 0.08\nconfig.output.copy_on_distill = True\n\n# Use modified config\ntenets = Tenets(config=config)\n</code></pre></p> <p>Runtime overrides: Python<pre><code># Config precedence: method args &gt; instance config &gt; file config\nresult = tenets.distill(\n    prompt=\"add authentication\",\n    mode=\"thorough\",        # Overrides config.ranking.algorithm\n    max_tokens=100000,      # Overrides config.max_tokens\n    format=\"json\",          # Overrides config.output.default_format\n    session_name=\"auth\",    # Session-specific\n    include_patterns=[\"*.py\", \"*.js\"],\n    exclude_patterns=[\"*.test.js\"],\n)\n</code></pre></p>"},{"location":"CONFIG/#configuration-recipes","title":"Configuration Recipes","text":""},{"location":"CONFIG/#for-different-use-cases","title":"For Different Use Cases","text":"<p>Large Monorepo (millions of files): YAML<pre><code>max_tokens: 150000\nscanner:\n  max_files: 50000\n  workers: 8\n  parallel_mode: process\n  exclude_tests_by_default: true\nranking:\n  algorithm: fast\n  threshold: 0.15\n  workers: 4\n  batch_size: 500\ncache:\n  enabled: true\n  memory_cache_size: 5000\n</code></pre></p> <p>Small Project (high precision): YAML<pre><code>max_tokens: 80000\nranking:\n  algorithm: thorough\n  threshold: 0.08\n  text_similarity_algorithm: bm25  # Default algorithm\n  use_embeddings: true\n  custom_weights:\n    keyword_match: 0.35\n    import_graph: 0.25\n</code></pre></p> <p>Documentation-Heavy Project: YAML<pre><code>summarizer:\n  docstring_weight: 0.8\n  include_all_signatures: true\n  preserve_code_structure: false\nranking:\n  custom_weights:\n    keyword_match: 0.20\n    path_relevance: 0.30  # Prioritize doc paths\n</code></pre></p> <p>Security-Focused Analysis: YAML<pre><code>tenet:\n  system_instruction: |\n    Focus on security implications.\n    Flag any potential vulnerabilities.\n    Suggest secure alternatives.\n  system_instruction_enabled: true\n  auto_instill: true\nscanner:\n  additional_ignore_patterns: []  # Don't skip anything\n  exclude_tests_by_default: false\n</code></pre></p>"},{"location":"CONFIG/#performance-tuning","title":"Performance Tuning","text":"<p>Maximum Speed (sacrifices precision): YAML<pre><code>ranking:\n  algorithm: fast\n  threshold: 0.05\n  text_similarity_algorithm: bm25  # Using BM25 (default)\n  use_embeddings: false\n  workers: 8\nscanner:\n  workers: 8\n  timeout: 2.0\ncache:\n  enabled: true\n  compression: false\n</code></pre></p> <p>Maximum Precision (slower): YAML<pre><code>ranking:\n  algorithm: thorough\n  threshold: 0.20\n  text_similarity_algorithm: bm25  # Default algorithm\n  use_embeddings: true\n  use_git: true\n  workers: 2\nsummarizer:\n  quality_threshold: high\n  enable_ml_strategies: true\n</code></pre></p> <p>Memory-Constrained Environment: YAML<pre><code>scanner:\n  max_files: 1000\n  workers: 1\nranking:\n  workers: 1\n  batch_size: 50\ncache:\n  memory_cache_size: 100\n  max_size_mb: 100\nnlp:\n  embeddings_batch_size: 8\n  multiprocessing_enabled: false\n</code></pre></p>"},{"location":"CONFIG/#common-workflows","title":"Common Workflows","text":"<p>Bug Investigation: YAML<pre><code>ranking:\n  algorithm: balanced\n  threshold: 0.10\n  custom_weights:\n    git_activity: 0.30  # Recent changes matter\n    complexity: 0.20    # Complex code = more bugs\ngit:\n  include_history: true\n  history_limit: 200\n  include_blame: true\n</code></pre></p> <p>New Feature Development: YAML<pre><code>ranking:\n  algorithm: balanced\n  threshold: 0.08\n  custom_weights:\n    import_graph: 0.30  # Dependencies matter\n    path_relevance: 0.25 # Related modules\noutput:\n  copy_on_distill: true\n  show_token_usage: true\n</code></pre></p> <p>Code Review Preparation: YAML<pre><code>summarizer:\n  target_ratio: 0.5  # More detail\n  preserve_code_structure: true\n  include_all_signatures: true\noutput:\n  syntax_highlighting: true\n  line_numbers: true\n  include_metadata: true\n</code></pre></p>"},{"location":"CONFIG/#troubleshooting","title":"Troubleshooting","text":""},{"location":"CONFIG/#common-issues-and-solutions","title":"Common Issues and Solutions","text":"<p>No files included in context: - Lower <code>ranking.threshold</code> (try 0.05) - Use <code>--mode fast</code> for broader inclusion - Increase <code>max_tokens</code> limit - Check if files match <code>--include</code> patterns - Verify files aren't in <code>.gitignore</code> - Use <code>--include-tests</code> if analyzing test files</p> <p>Configuration not taking effect: Bash<pre><code># Check which config file is loaded\ntenets config show | head -20\n\n# Verify specific setting\ntenets config show --key ranking.threshold\n\n# Check config file location\nls -la .tenets.yml\n\n# Test with explicit config\ntenets --config ./my-config.yml distill \"query\"\n</code></pre></p> <p>Environment variables not working: Bash<pre><code># Verify export (not just set)\nexport TENETS_RANKING_THRESHOLD=0.05  # Correct\nTENETS_RANKING_THRESHOLD=0.05         # Wrong (not exported)\n\n# Check if variable is set\necho $TENETS_RANKING_THRESHOLD\n\n# Debug with explicit env\nTENETS_DEBUG=true tenets config show\n</code></pre></p> <p>Performance issues: - Reduce <code>scanner.max_files</code> and <code>scanner.max_file_size</code> - Enable caching: <code>cache.enabled: true</code> - Use <code>ranking.algorithm: fast</code> - Reduce <code>ranking.workers</code> if CPU-constrained - Exclude unnecessary paths with <code>additional_ignore_patterns</code></p> <p>Token limit exceeded: - Increase <code>max_tokens</code> or use <code>--max-tokens</code> - Enable <code>--condense</code> flag - Use <code>--remove-comments</code> - Increase <code>ranking.threshold</code> for stricter filtering - Exclude test files: <code>scanner.exclude_tests_by_default: true</code></p> <p>Cache issues: Bash<pre><code># Clear cache\nrm -rf ~/.tenets/cache\n\n# Disable cache temporarily\nTENETS_CACHE_ENABLED=false tenets distill \"query\"\n\n# Use custom cache location\nexport TENETS_CACHE_DIRECTORY=/tmp/tenets-cache\n</code></pre></p>"},{"location":"CONFIG/#validation-commands","title":"Validation Commands","text":"Bash<pre><code># Validate configuration syntax\ntenets config validate\n\n# Show effective configuration\ntenets config show --format json | jq\n\n# Test configuration with dry run\ntenets distill \"test query\" --dry-run\n\n# Check what files would be scanned\ntenets examine . --dry-run\n\n# Debug ranking process\nTENETS_DEBUG=true tenets distill \"query\" 2&gt;debug.log\n</code></pre>"},{"location":"CONFIG/#advanced-topics","title":"Advanced Topics","text":""},{"location":"CONFIG/#custom-ranking-strategies","title":"Custom Ranking Strategies","text":"<p>Create a custom ranking strategy by combining weights:</p> YAML<pre><code>ranking:\n  algorithm: custom\n  custom_weights:\n    keyword_match: 0.40    # Emphasize keyword relevance\n    path_relevance: 0.15   # De-emphasize path matching\n    import_graph: 0.15     # Moderate dependency weight\n    git_activity: 0.10     # Low git signal weight\n    file_type: 0.10        # File type matching\n    complexity: 0.10       # Code complexity\n</code></pre>"},{"location":"CONFIG/#multi-environment-setup","title":"Multi-Environment Setup","text":"<p>Create environment-specific configs:</p> Bash<pre><code># Development\ncp .tenets.yml .tenets.dev.yml\n# Edit for dev settings\n\n# Production analysis\ncp .tenets.yml .tenets.prod.yml\n# Edit for production settings\n\n# Use specific config\ntenets --config .tenets.dev.yml distill \"query\"\n</code></pre>"},{"location":"CONFIG/#integration-with-cicd","title":"Integration with CI/CD","text":"YAML<pre><code># .tenets.ci.yml - Optimized for CI\nmax_tokens: 50000\nquiet: true\nscanner:\n  max_files: 5000\n  workers: 2\nranking:\n  algorithm: fast\n  threshold: 0.10\ncache:\n  enabled: false  # Fresh analysis each run\noutput:\n  default_format: json  # Machine-readable\n</code></pre>"},{"location":"CONFIG/#see-also","title":"See Also","text":"<ul> <li>CLI Reference - Complete command documentation</li> <li>API Reference - Python API documentation</li> <li>Architecture - System design details</li> </ul>"},{"location":"CONTRIBUTING/","title":"Contributing to Tenets","text":"<p>Thanks for your interest in improving Tenets! Contributions of all kinds are welcome: bug reports, docs, tests, features, performance improvements, refactors, and feedback.</p>"},{"location":"CONTRIBUTING/#quick-start-tldr","title":"Quick Start (TL;DR)","text":"Bash<pre><code># Fork / clone\n git clone https://github.com/jddunn/tenets.git\n cd tenets\n\n# Create a virtual environment (or use pyenv / conda)\n python -m venv .venv &amp;&amp; source .venv/bin/activate  # Windows: .venv\\Scripts\\activate\n\n# Install core + dev extras\n pip install -e .[dev]\n # (or: make dev)\n\n# Run tests\n pytest -q\n\n# Lint &amp; type check\n ruff check .\n mypy tenets\n\n# Format\n black .\n\n# Run a sample command\n tenets distill \"hello world\" . --stats\n</code></pre>"},{"location":"CONTRIBUTING/#project-philosophy","title":"Project Philosophy","text":"<p>Tenets is: - Local-first, privacy-preserving - Fast with graceful scalability (analyze only as deep as necessary) - Extensible without forcing heavyweight ML (opt-in extras) - Transparent in ranking decisions (explanations where reasonable)</p>"},{"location":"CONTRIBUTING/#issue-tracking","title":"Issue Tracking","text":"<p>Before filing: 1. Search existing issues (open + closed) 2. For questions / ideas, consider starting a GitHub Discussion (if enabled) or Discord 3. Provide reproduction steps and environment info (OS, Python version, extras installed)</p> <p>Good bug report template: Text Only<pre><code>### Description\nClear, concise description of the problem.\n\n### Reproduction\nCommands or code snippet that reproduces the issue.\n\n### Expected vs Actual\nWhat you expected / what happened.\n\n### Environment\nOS / Python / tenets version / installed extras.\n</code></pre></p>"},{"location":"CONTRIBUTING/#branch--commit-conventions","title":"Branch &amp; Commit Conventions","text":"<ul> <li>Create feature branches off <code>dev</code> (default contribution branch)</li> <li>Keep PRs narrowly scoped when possible</li> <li>Conventional Commit prefixes (enforced via commitizen config):</li> <li>feat: new user-facing feature</li> <li>fix: bug fix</li> <li>refactor: code change without feature/bug semantics</li> <li>perf: performance improvement</li> <li>docs: docs only changes</li> <li>test: add or improve tests</li> <li>chore: tooling / infra / build</li> </ul> <p>Example: Text Only<pre><code>feat(ranking): add parallel TF-IDF corpus prepass\n</code></pre></p> <p>Use <code>cz commit</code> if you have commitizen installed.</p>"},{"location":"CONTRIBUTING/#code-style--tooling","title":"Code Style &amp; Tooling","text":"Tool Purpose Command black Formatting <code>black .</code> ruff Linting (multi-plugin) <code>ruff check .</code> mypy Static typing <code>mypy tenets</code> pytest Tests + coverage <code>pytest -q</code> coverage HTML / XML reports <code>pytest --cov</code> commitizen Conventional versioning <code>cz bump</code> <p>Pre-commit hooks (optional): Bash<pre><code>pip install pre-commit\npre-commit install\n</code></pre></p>"},{"location":"CONTRIBUTING/#tests","title":"Tests","text":"<p>Guidelines: - Place tests under <code>tests/</code> mirroring module paths - Use <code>pytest</code> fixtures; prefer explicit data over deep mocks - Mark slow tests with <code>@pytest.mark.slow</code> - Keep unit tests fast (&lt;300ms ideally) - Add at least one failing test before a bug fix</p> <p>Run selectively: Bash<pre><code>pytest tests/core/analysis -k python_analyzer\npytest -m \"not slow\"\n</code></pre></p>"},{"location":"CONTRIBUTING/#type-hints","title":"Type Hints","text":"<ul> <li>New/modified public functions must be fully typed</li> <li>Avoid <code>Any</code> unless absolutely necessary; justify in a comment</li> <li>mypy config is strict\u2014fix or silence with narrow <code># type: ignore[...]</code></li> </ul>"},{"location":"CONTRIBUTING/#documentation","title":"Documentation","text":"<p>User docs live in <code>docs/</code> (MkDocs Material). For changes affecting users: - Update <code>README.md</code> - Update or create relevant page under <code>docs/</code> - Add examples (<code>quickstart.md</code>) if CLI/API behavior changes - Link new pages in <code>mkdocs.yml</code></p> <p>Serve docs locally: Bash<pre><code>mkdocs serve\n</code></pre></p>"},{"location":"CONTRIBUTING/#adding-a-language-analyzer","title":"Adding a Language Analyzer","text":"<ol> <li>Create <code>&lt;language&gt;_analyzer.py</code> under <code>tenets/core/analysis/implementations/</code></li> <li>Subclass <code>LanguageAnalyzer</code></li> <li>Implement <code>match(path)</code> and <code>analyze(content)</code></li> <li>Add tests under <code>tests/core/analysis/implementations/</code></li> <li>Update <code>supported-languages.md</code></li> </ol>"},{"location":"CONTRIBUTING/#ranking-extensions","title":"Ranking Extensions","text":"<ul> <li>Register custom rankers via provided registration API (see <code>tenets/core/ranking/ranker.py</code>)</li> <li>Provide deterministic output; avoid network calls in ranking stage</li> <li>Document new algorithm flags in <code>CONFIG.md</code></li> </ul>"},{"location":"CONTRIBUTING/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Avoid O(n^2) scans over file lists when possible</li> <li>Cache expensive analysis (see existing caching layer)</li> <li>Add benchmarks if adding heavy operations (future / optional)</li> </ul>"},{"location":"CONTRIBUTING/#security--privacy","title":"Security / Privacy","text":"<ul> <li>Never exfiltrate code or send network requests without explicit user config</li> <li>Keep default extras minimal</li> </ul>"},{"location":"CONTRIBUTING/#release-process-maintainers","title":"Release Process (Maintainers)","text":"<ol> <li>Ensure <code>dev</code> is green (CI + coverage)</li> <li>Bump version: <code>cz bump</code> (updates <code>pyproject.toml</code>, tag, CHANGELOG)</li> <li>Build: <code>make build</code> (or <code>python -m build</code>)</li> <li>Publish: <code>twine upload dist/*</code></li> <li>Merge <code>dev</code> -&gt; <code>master</code> and push tags</li> </ol>"},{"location":"CONTRIBUTING/#code-of-conduct","title":"Code of Conduct","text":"<p>This project follows the Code of Conduct. By participating you agree to uphold it.</p>"},{"location":"CONTRIBUTING/#license","title":"License","text":"<p>By contributing you agree your contributions are licensed under the MIT License.</p> <p>Questions? Open an issue or reach out via Discord.</p>"},{"location":"DEPLOYMENT/","title":"Deployment Guide","text":"<p>This guide outlines the process for releasing new versions of Tenets to PyPI and deploying documentation.</p>"},{"location":"DEPLOYMENT/#release-process-automated","title":"Release Process (Automated)","text":"<p>Standard path: merge conventional commits into <code>main</code>; automation versions &amp; publishes.</p>"},{"location":"DEPLOYMENT/#how-it-works","title":"How It Works","text":"<ol> <li>Merge PR \u2192 <code>version-bump.yml</code> runs</li> <li>Determines bump size (major / minor / patch / skip) from commit messages</li> <li>Updates <code>pyproject.toml</code> + appends grouped section to <code>CHANGELOG.md</code></li> <li>Commits <code>chore(release): vX.Y.Z</code> and tags <code>vX.Y.Z</code></li> <li>Tag triggers <code>release.yml</code>: build, publish to PyPI, (future) Docker, docs deploy</li> <li>Release notes composed from changelog / draft config</li> </ol>"},{"location":"DEPLOYMENT/#bump-rules-summary","title":"Bump Rules (Summary)","text":"Commit Types Seen Result BREAKING CHANGE / <code>!</code> Major feat / perf Minor fix / refactor / chore Patch (unless higher trigger present) Only docs / test / style Skip"},{"location":"DEPLOYMENT/#manual-overrides-rare","title":"Manual Overrides (Rare)","text":"<p>If automation blocked (workflow infra outage): Bash<pre><code>git checkout main &amp;&amp; git pull\ncz bump --increment PATCH  # or MINOR / MAJOR\ngit push &amp;&amp; git push --tags\n</code></pre> Resume automation next merge.</p>"},{"location":"DEPLOYMENT/#first-release-bootstrap-v010","title":"First Release Bootstrap (v0.1.0)","text":"<p>For the initial v0.1.0 release, follow this manual process:</p> <ol> <li>Update CHANGELOG.md with v0.1.0 entries (on dev branch)</li> <li>Commit and push to dev: <code>git commit -m \"docs: update CHANGELOG for v0.1.0\"</code></li> <li>Merge dev \u2192 master</li> <li>From master, create and push tag: Bash<pre><code>git checkout master &amp;&amp; git pull\ngit tag -a v0.1.0 -m \"Release v0.1.0 - Initial public release\"\ngit push origin v0.1.0  # This triggers everything!\n</code></pre></li> <li>The tag push automatically triggers:</li> <li>GitHub Release creation with artifacts</li> <li>PyPI package publishing (if PYPI_API_TOKEN is set)</li> <li>Documentation deployment to GitHub Pages</li> </ol> <p>After v0.1.0: Automation takes over - commits trigger version bumps based on conventional commit messages.</p>"},{"location":"DEPLOYMENT/#verification-checklist","title":"Verification Checklist","text":"Step Command / Action Install published wheel <code>pip install --no-cache-dir tenets==X.Y.Z</code> CLI version matches <code>tenets --version</code> Release notes present Check GitHub Release page Docs updated Visit docs site / gh-pages commit"},{"location":"DEPLOYMENT/#troubleshooting","title":"Troubleshooting","text":"Issue Cause Resolution No tag created Only docs/test/style commits Land a fix/feat/perf commit Wrong bump size Mis-typed commit message Amend &amp; force push before merge; or follow-up commit PyPI publish failed Missing PyPI token / trust approval pending Add <code>PYPI_API_TOKEN</code> or approve trusted publisher Duplicate releases Manual tag + automated tag Avoid manual tagging unless emergency"},{"location":"DEPLOYMENT/#documentation-deployment","title":"Documentation Deployment","text":"<p>Docs are (a) built in CI on PR for validation; (b) deployed on release tag push by <code>release.yml</code> (or dedicated docs deploy step on main). GitHub Pages serves from <code>gh-pages</code>.</p>"},{"location":"DEPLOYMENT/#required--optional-secrets","title":"Required / Optional Secrets","text":"Secret Required Purpose Notes <code>PYPI_API_TOKEN</code> Yes* PyPI publish in <code>release.yml</code> *Omit if using Trusted Publishing (approve first build). <code>CODECOV_TOKEN</code> Public: often no / Private: yes Coverage uploads Set to be explicit. <code>GOOGLE_ANALYTICS_ID</code> Optional GA4 measurement ID for docs analytics Used by MkDocs Material via <code>!ENV</code> in <code>mkdocs.yml</code> (e.g., <code>G-XXXXXXXXXX</code>). If unset/empty, analytics are disabled. <code>DOCKER_USERNAME</code> / <code>DOCKER_TOKEN</code> Optional Future Docker image publishing Not required yet. <code>GH_PAT</code> No Cross-repo automation (not standard) Avoid storing if unused. <p>Environment (optional): <code>TENETS_DEBUG</code>, <code>TENETS_CACHE_DIRECTORY</code>.</p>"},{"location":"DEPLOYMENT/#google-analytics-optional","title":"Google Analytics (optional)","text":"<p>MkDocs Material analytics are wired to an environment variable:</p> <ul> <li>In <code>mkdocs.yml</code>: <code>extra.analytics.property: !ENV [GOOGLE_ANALYTICS_ID, \"\"]</code></li> <li>Provide a GA4 Measurement ID (format <code>G-XXXXXXXXXX</code>). If the variable is unset or empty, analytics are disabled automatically.</li> </ul> <p>Local usage</p> Bash<pre><code># bash / Git Bash / WSL\nexport GOOGLE_ANALYTICS_ID=G-XXXXXXXXXX\nmkdocs serve\n</code></pre> PowerShell<pre><code># PowerShell\n$env:GOOGLE_ANALYTICS_ID = 'G-XXXXXXXXXX'\nmkdocs serve\n</code></pre> <p>GitHub Actions (recommended)</p> YAML<pre><code>jobs:\n   docs:\n      runs-on: ubuntu-latest\n      env:\n         GOOGLE_ANALYTICS_ID: ${{ secrets.GOOGLE_ANALYTICS_ID }}\n      steps:\n         - uses: actions/checkout@v4\n         - uses: actions/setup-python@v5\n            with:\n               python-version: '3.12'\n         - run: pip install -e '.[docs]'\n         - run: mkdocs build --clean\n</code></pre> <p>Store your GA4 Measurement ID as a repository secret named <code>GOOGLE_ANALYTICS_ID</code>. The docs build will inject it at build time; if not present, analytics are off.</p>"},{"location":"DEPLOYMENT/#with-specific-features","title":"With specific features","text":"<p>pip install tenets[ml]  # ML features pip install tenets[viz]  # Visualization pip install tenets[all]  # Everything Text Only<pre><code>### 2. Development Installation\n\n```bash\n# From source\ngit clone https://github.com/jddunn/tenets.git\ncd tenets\npip install -e \".[dev]\"\n</code></pre></p>"},{"location":"DEPLOYMENT/#3-docker-container","title":"3. Docker Container","text":"Bash<pre><code># Pull from Docker Hub\ndocker pull tenets/tenets:latest\n\n# Run command\ndocker run --rm -v $(pwd):/workspace tenets/tenets make-context \"query\" .\n\n# Interactive shell\ndocker run -it --rm -v $(pwd):/workspace tenets/tenets bash\n</code></pre>"},{"location":"DEPLOYMENT/#4-standalone-binary","title":"4. Standalone Binary","text":"<p>Download from GitHub Releases:</p> Bash<pre><code># Linux/macOS\ncurl -L https://github.com/jddunn/tenets/releases/latest/download/tenets-linux -o tenets\nchmod +x tenets\n./tenets --version\n\n# Windows\n# Download tenets-windows.exe from releases page\n</code></pre>"},{"location":"DEPLOYMENT/#pypi-publishing","title":"PyPI Publishing","text":""},{"location":"DEPLOYMENT/#first-time-setup","title":"First-Time Setup","text":"<ol> <li>Create PyPI account:</li> <li>Register at pypi.org</li> <li> <p>Enable 2FA (required)</p> </li> <li> <p>Configure trusted publishing:</p> </li> <li>Go to your project settings on PyPI</li> <li>Add GitHub Actions as trusted publisher:<ul> <li>Owner: <code>jddunn</code></li> <li>Repository: <code>tenets</code></li> <li>Workflow: <code>release.yml</code></li> <li>Environment: <code>pypi</code></li> </ul> </li> </ol>"},{"location":"DEPLOYMENT/#manual-publishing-emergency-only","title":"Manual Publishing (Emergency Only)","text":"Bash<pre><code># Build distribution\npython -m build\n\n# Check package\ntwine check dist/*\n\n# Upload to TestPyPI first\ntwine upload --repository testpypi dist/*\n\n# Test installation\npip install --index-url https://test.pypi.org/simple/ tenets\n\n# Upload to PyPI\ntwine upload dist/*\n</code></pre>"},{"location":"DEPLOYMENT/#docker-deployment","title":"Docker Deployment","text":""},{"location":"DEPLOYMENT/#building-images","title":"Building Images","text":"Docker<pre><code># Dockerfile\nFROM python:3.11-slim\n\n# Install system dependencies\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    git \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\n# Create non-root user\nRUN useradd -m -s /bin/bash tenets\n\n# Set working directory\nWORKDIR /app\n\n# Install tenets\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\nCOPY . .\nRUN pip install --no-cache-dir -e .\n\n# Switch to non-root user\nUSER tenets\n\n# Set entrypoint\nENTRYPOINT [\"tenets\"]\n</code></pre>"},{"location":"DEPLOYMENT/#multi-architecture-build","title":"Multi-Architecture Build","text":"Bash<pre><code># Setup buildx\ndocker buildx create --use\n\n# Build for multiple platforms\ndocker buildx build \\\n  --platform linux/amd64,linux/arm64 \\\n  --tag tenets/tenets:latest \\\n  --tag tenets/tenets:v0.1.0 \\\n  --push .\n</code></pre>"},{"location":"DEPLOYMENT/#docker-compose","title":"Docker Compose","text":"YAML<pre><code># docker-compose.yml\nversion: '3.8'\n\nservices:\n  tenets:\n    image: tenets/tenets:latest\n    volumes:\n      - .:/workspace\n      - ~/.tenets:/home/tenets/.tenets\n    working_dir: /workspace\n    environment:\n      - TENETS_LOG_LEVEL=INFO\n    command: make-context \"implement feature\" .\n</code></pre>"},{"location":"DEPLOYMENT/#binary-distribution","title":"Binary Distribution","text":""},{"location":"DEPLOYMENT/#building-binaries","title":"Building Binaries","text":"Bash<pre><code># Install PyInstaller\npip install pyinstaller\n\n# Build for current platform\npyinstaller \\\n  --onefile \\\n  --name tenets \\\n  --add-data \"tenets:tenets\" \\\n  --hidden-import tenets.core \\\n  --hidden-import tenets.models \\\n  --hidden-import tenets.utils \\\n  tenets/__main__.py\n\n# Output in dist/tenets\n</code></pre>"},{"location":"DEPLOYMENT/#cross-platform-building","title":"Cross-Platform Building","text":"<p>Use GitHub Actions for multi-platform builds: - Linux: Ubuntu runner - macOS: macOS runner - Windows: Windows runner</p>"},{"location":"DEPLOYMENT/#code-signing-optional","title":"Code Signing (Optional)","text":"Bash<pre><code># macOS\ncodesign --deep --force --verify --verbose \\\n  --sign \"Developer ID Application: Your Name\" \\\n  dist/tenets\n\n# Windows (using signtool)\nsigntool sign /t http://timestamp.digicert.com dist/tenets.exe\n</code></pre>"},{"location":"DEPLOYMENT/#documentation-deployment_1","title":"Documentation Deployment","text":""},{"location":"DEPLOYMENT/#building-documentation","title":"Building Documentation","text":"Bash<pre><code># Install dependencies\npip install -e \".[docs]\"\n\n# Build docs\nmkdocs build\n\n# Test locally\nmkdocs serve\n</code></pre>"},{"location":"DEPLOYMENT/#versioned-documentation","title":"Versioned Documentation","text":"Bash<pre><code># Deploy new version\nmike deploy --push --update-aliases 0.1.0 latest\n\n# Deploy development docs\nmike deploy --push dev\n\n# Set default version\nmike set-default --push latest\n</code></pre>"},{"location":"DEPLOYMENT/#github-pages-setup","title":"GitHub Pages Setup","text":"<ol> <li>Enable GitHub Pages in repository settings</li> <li>Set source to <code>gh-pages</code> branch</li> <li>Documentation auto-deploys on release</li> </ol>"},{"location":"DEPLOYMENT/#security-considerations","title":"Security Considerations","text":""},{"location":"DEPLOYMENT/#release-security","title":"Release Security","text":"<ol> <li> <p>Sign commits and tags:    Bash<pre><code>git config --global commit.gpgsign true\ngit config --global tag.gpgsign true\n</code></pre></p> </li> <li> <p>Verify dependencies:    Bash<pre><code># Check for vulnerabilities\nsafety check\n\n# Audit dependencies\npip-audit\n</code></pre></p> </li> <li> <p>Scan for secrets:    Bash<pre><code># Pre-release scan\ndetect-secrets scan --all-files\n</code></pre></p> </li> </ol>"},{"location":"DEPLOYMENT/#deployment-security","title":"Deployment Security","text":"<ol> <li> <p>Use minimal base images:    Docker<pre><code>FROM python:3.11-slim  # Not full python image\n</code></pre></p> </li> <li> <p>Run as non-root:    Docker<pre><code>USER nobody\n</code></pre></p> </li> <li> <p>Scan images:    Bash<pre><code># Scan for vulnerabilities\ndocker scan tenets/tenets:latest\n</code></pre></p> </li> </ol>"},{"location":"DEPLOYMENT/#monitoring--maintenance","title":"Monitoring &amp; Maintenance","text":""},{"location":"DEPLOYMENT/#release-monitoring","title":"Release Monitoring","text":"<ol> <li>PyPI Statistics:</li> <li>Check download stats</li> <li> <p>Monitor for unusual activity</p> </li> <li> <p>GitHub Insights:</p> </li> <li>Track clone/download metrics</li> <li> <p>Monitor issue trends</p> </li> <li> <p>Error Tracking:</p> </li> <li>Set up Sentry (optional)</li> <li>Monitor GitHub issues</li> </ol>"},{"location":"DEPLOYMENT/#maintenance-tasks","title":"Maintenance Tasks","text":""},{"location":"DEPLOYMENT/#weekly","title":"Weekly","text":"<ul> <li>Review and triage issues</li> <li>Check for security advisories</li> <li>Update dependencies</li> </ul>"},{"location":"DEPLOYMENT/#monthly","title":"Monthly","text":"<ul> <li>Review performance metrics</li> <li>Update documentation</li> <li>Clean up old releases</li> </ul>"},{"location":"DEPLOYMENT/#quarterly","title":"Quarterly","text":"<ul> <li>Major dependency updates</li> <li>Security audit</li> <li>Performance benchmarking</li> </ul>"},{"location":"DEPLOYMENT/#rollback-procedure","title":"Rollback Procedure","text":"<p>If a release has critical issues:</p> <ol> <li> <p>Yank from PyPI (last resort):    Bash<pre><code># This prevents new installations\n# Existing installations continue to work\ntwine yank tenets==0.1.0\n</code></pre></p> </li> <li> <p>Create hotfix:    Bash<pre><code>git checkout -b hotfix/critical-bug\n# Fix issue\ngit commit -m \"fix: critical bug in analyzer\"\ncz bump --increment PATCH\ngit push origin hotfix/critical-bug\n</code></pre></p> </li> <li> <p>Fast-track release:</p> </li> <li>Create PR with hotfix</li> <li>Bypass normal review (emergency)</li> <li>Merge and tag immediately</li> </ol>"},{"location":"DEPLOYMENT/#deployment-environments","title":"Deployment Environments","text":""},{"location":"DEPLOYMENT/#development","title":"Development","text":"Bash<pre><code>pip install -e \".[dev]\"\nexport TENETS_ENV=development\n</code></pre>"},{"location":"DEPLOYMENT/#staging","title":"Staging","text":"Bash<pre><code>pip install tenets==0.1.0rc1  # Release candidate\nexport TENETS_ENV=staging\n</code></pre>"},{"location":"DEPLOYMENT/#production","title":"Production","text":"Bash<pre><code>pip install tenets==0.1.0\nexport TENETS_ENV=production\n</code></pre>"},{"location":"DEPLOYMENT/#troubleshooting_1","title":"Troubleshooting","text":""},{"location":"DEPLOYMENT/#common-issues","title":"Common Issues","text":"<ol> <li>PyPI upload fails:</li> <li>Check PyPI status</li> <li>Verify credentials</li> <li> <p>Ensure version doesn't exist</p> </li> <li> <p>Docker build fails:</p> </li> <li>Clear builder cache</li> <li>Check Docker Hub limits</li> <li> <p>Verify multi-arch support</p> </li> <li> <p>Documentation not updating:</p> </li> <li>Check GitHub Pages settings</li> <li>Verify mike configuration</li> <li>Clear browser cache</li> </ol>"},{"location":"DEPLOYMENT/#getting-help","title":"Getting Help","text":"<ul> <li>GitHub Issues for bugs</li> <li>Discussions for questions</li> <li>team@tenets.dev for security issues</li> </ul> <p>Remember: Every release should make developers' lives easier. \ud83d\ude80</p>"},{"location":"DEVELOPMENT/","title":"Development Guide","text":"<p>This guide provides instructions for setting up your development environment, running tests, and contributing to the Tenets project.</p>"},{"location":"DEVELOPMENT/#1-initial-setup","title":"1. Initial Setup","text":""},{"location":"DEVELOPMENT/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.9+</li> <li>Git</li> <li>An activated Python virtual environment (e.g., <code>venv</code>, <code>conda</code>).</li> </ul>"},{"location":"DEVELOPMENT/#fork-and-clone","title":"Fork and Clone","text":"<ol> <li>Fork the repository on GitHub.</li> <li>Clone your fork locally:    Bash<pre><code>git clone https://github.com/jddunn/tenets.git\ncd tenets\n</code></pre></li> </ol>"},{"location":"DEVELOPMENT/#install-dependencies","title":"Install Dependencies","text":"<p>Install the project in \"editable\" mode along with all development dependencies. This allows you to modify the source code and have the changes immediately reflected.</p> <p>Bash<pre><code>pip install -e \".[all,dev]\"\n</code></pre> This command installs everything needed for development, including core dependencies, optional features (<code>all</code>), and development tools (<code>dev</code>).</p>"},{"location":"DEVELOPMENT/#set-up-pre-commit-hooks","title":"Set up Pre-Commit Hooks","text":"<p>This project uses <code>pre-commit</code> to automatically run linters and formatters before each commit.</p> Bash<pre><code>pre-commit install\n</code></pre>"},{"location":"DEVELOPMENT/#alternative-installs","title":"Alternative Installs","text":"<p>If you only need core + dev tooling (faster): Bash<pre><code>pip install -e \".[dev]\"\n</code></pre> If you need a minimal footprint for quick iteration (no optional extras): Bash<pre><code>pip install -e .\n</code></pre></p>"},{"location":"DEVELOPMENT/#verifying-the-cli","title":"Verifying the CLI","text":"Bash<pre><code>tenets --version\ntenets --help | head\n</code></pre> <p>If the command is not found, ensure your virtualenv is activated and that the <code>scripts</code> (Windows) or <code>bin</code> (Unix) directory is on PATH.</p>"},{"location":"DEVELOPMENT/#11-building-distribution-artifacts-optional","title":"1.1 Building Distribution Artifacts (Optional)","text":"<p>You typically do NOT need to build wheels / sdists for day\u2011to\u2011day development; the editable install auto-reflects code edits. Build only when testing packaging or release steps.</p> Bash<pre><code>python -m build               # creates dist/*.whl and dist/*.tar.gz\npip install --force-reinstall dist/tenets-*.whl  # sanity check install\n</code></pre> <p>To inspect what went into the wheel: Bash<pre><code>unzip -l dist/tenets-*.whl | grep analysis/implementations | head\n</code></pre></p>"},{"location":"DEVELOPMENT/#12-clean-environment-tasks","title":"1.2 Clean Environment Tasks","text":"Bash<pre><code>pip cache purge        # optional: clear wheel cache\nfind . -name \"__pycache__\" -exec rm -rf {} +\nrm -rf .pytest_cache .ruff_cache .mypy_cache build dist *.egg-info\n</code></pre>"},{"location":"DEVELOPMENT/#13-using-poetry-instead-of-pip-optional","title":"1.3 Using Poetry Instead of pip (Optional)","text":"<p>Poetry can manage the virtual environment and extras if you prefer: Bash<pre><code>poetry install -E all -E dev   # full feature + dev toolchain\npoetry run pytest              # run tests\npoetry run tenets --help       # invoke CLI\n</code></pre> Update dependencies: Bash<pre><code>poetry update\n</code></pre> Add a new optional dependency (example): Bash<pre><code>poetry add --optional rich\n</code></pre> Text Only<pre><code>## 2. Running Tests\n\nThe test suite uses `pytest`. We have a comprehensive configuration in `pytest.ini` that handles most settings automatically.\n\n### Running All Tests\nTo run the entire test suite:\n```bash\npytest\n</code></pre></p>"},{"location":"DEVELOPMENT/#running-tests-with-coverage","title":"Running Tests with Coverage","text":"<p>To generate a test coverage report: Bash<pre><code>pytest --cov\n</code></pre> This command is configured in <code>pytest.ini</code> to: - Measure coverage for the <code>tenets</code> package. - Generate reports in the terminal, as XML (<code>coverage.xml</code>), and as a detailed HTML report (<code>htmlcov/</code>). - Fail the build if coverage drops below 70%.</p> <p>To view the interactive HTML report: Bash<pre><code># On macOS\nopen htmlcov/index.html\n\n# On Windows\nstart htmlcov/index.html\n\n# On Linux\nxdg-open htmlcov/index.html\n</code></pre></p>"},{"location":"DEVELOPMENT/#3-required--optional-secrets","title":"3. Required / Optional Secrets","text":"<p>Configure these in GitHub: Settings \u2192 Secrets and variables \u2192 Actions.</p> Secret Required? Purpose Notes <code>PYPI_API_TOKEN</code> Yes* Upload package in <code>release.yml</code> *If using PyPI Trusted Publishing you can omit and approve first publication manually. Keep token while bootstrapping. <code>CODECOV_TOKEN</code> Yes (private repo) / No (public) Coverage uploads in CI Public repos sometimes auto-detect; set to be explicit. <code>DOCKER_USERNAME</code> Optional Auth for Docker image push (if enabled) Only needed if/when container publishing is turned on. <code>DOCKER_TOKEN</code> Optional Password / token for Docker Hub Pair with username. <code>GH_PAT</code> No Only for advanced workflows (e.g. cross\u2011repo automation) Not needed for standard release pipeline. <p>Additional environment driven configs (rarely needed): | Variable | Effect | |----------|-------| | <code>TENETS_CACHE_DIRECTORY</code> | Override default cache directory | | <code>TENETS_DEBUG</code> | Enables verbose debug logging when <code>true</code> |</p> <p>Security tips: - Grant least privilege (PyPI token scoped to project if possible) - Rotate any credentials annually or on role changes - Prefer Trusted Publishing over long\u2011lived API tokens once stable</p>"},{"location":"DEVELOPMENT/#4-code-style-and-linting","title":"4. Code Style and Linting","text":"<p>We use <code>ruff</code> for linting and formatting. The pre-commit hook runs it automatically, but you can also run it manually:</p> Bash<pre><code># Check for linting errors\nruff check .\n\n# Automatically fix linting errors\nruff check . --fix\n\n# Format the code\nruff format .\n</code></pre>"},{"location":"DEVELOPMENT/#5-building-documentation","title":"5. Building Documentation","text":"<p>The documentation is built using MkDocs with the Material theme.</p>"},{"location":"DEVELOPMENT/#installing-documentation-dependencies","title":"Installing Documentation Dependencies","text":"Bash<pre><code># Install MkDocs and theme\npip install mkdocs mkdocs-material\n\n# Or if you installed with dev dependencies, it's already included:\npip install -e \".[dev]\"\n</code></pre>"},{"location":"DEVELOPMENT/#serving-documentation-locally","title":"Serving Documentation Locally","text":""},{"location":"DEVELOPMENT/#fast-development-mode-recommended-for-editing-docs","title":"FAST Development Mode (Recommended for editing docs)","text":"Bash<pre><code># Use the lightweight dev config with dirty reload for FASTEST iteration\nmkdocs serve -f mkdocs.dev.yml --dirtyreload\n\n# Without dirty reload (still faster than full build)\nmkdocs serve -f mkdocs.dev.yml\n</code></pre> <p>mkdocs.dev.yml differences: - Disables heavy plugins: No API generation, no mkdocstrings, no minification - Faster rebuilds: Skips expensive operations - Dirty reload: Only rebuilds changed pages (not entire site) - Perfect for: Writing/editing documentation content</p>"},{"location":"DEVELOPMENT/#full-production-mode-for-testing-final-output","title":"Full Production Mode (for testing final output)","text":"Bash<pre><code># Full build with all features including API docs generation\nmkdocs serve\n\n# Serve on a different port\nmkdocs serve -a localhost:8080\n\n# Serve with verbose output for debugging\nmkdocs serve --verbose\n\n# With clean rebuild\nmkdocs serve --clean\n</code></pre> <p>The development server includes: - Live reload: Changes to docs files automatically refresh the browser - API docs generation: Auto-generates from Python docstrings - Full theme features: All navigation and search features enabled</p>"},{"location":"DEVELOPMENT/#building-static-documentation","title":"Building Static Documentation","text":"Bash<pre><code># Build the static site to site/ directory\nmkdocs build\n\n# Build with strict mode (fails on warnings)\nmkdocs build --strict\n\n# Build with verbose output\nmkdocs build --verbose\n\n# Clean build (removes old files first)\nmkdocs build --clean\n</code></pre>"},{"location":"DEVELOPMENT/#documentation-structure","title":"Documentation Structure","text":"Text Only<pre><code>docs/\n\u251c\u2500\u2500 index.md           # Homepage\n\u251c\u2500\u2500 overrides/        # Custom HTML templates\n\u2502   \u2514\u2500\u2500 home.html     # Custom homepage\n\u251c\u2500\u2500 styles/           # Custom CSS\n\u2502   \u251c\u2500\u2500 main.css\n\u2502   \u251c\u2500\u2500 search.css\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 assets/           # Images and screenshots\n\u2502   \u2514\u2500\u2500 images/\n\u2514\u2500\u2500 *.md             # Documentation pages\n</code></pre>"},{"location":"DEVELOPMENT/#api-documentation-generation","title":"API Documentation Generation","text":"<p>The API documentation is auto-generated from Python docstrings using <code>mkdocstrings</code> and <code>gen-files</code> plugins.</p>"},{"location":"DEVELOPMENT/#how-it-works","title":"How it works:","text":"<ol> <li><code>docs/gen_api.py</code> script runs during build:</li> <li>Scans all Python modules in <code>tenets/</code></li> <li>Generates markdown files with <code>:::</code> mkdocstrings syntax</li> <li> <p>Creates navigation structure in <code>api/</code> directory</p> </li> <li> <p><code>mkdocstrings</code> plugin processes the generated files:</p> </li> <li>Extracts docstrings from Python code</li> <li>Renders them as formatted documentation</li> <li>Includes type hints, parameters, returns, examples</li> </ol>"},{"location":"DEVELOPMENT/#regenerating-api-docs","title":"Regenerating API docs:","text":"Bash<pre><code># Full build with API generation (automatic)\nmkdocs build\n\n# Or serve with API generation\nmkdocs serve  # Uses mkdocs.yml which has gen-files enabled\n\n# Skip API generation for faster dev\nmkdocs serve -f mkdocs.dev.yml --dirtyreload\n</code></pre>"},{"location":"DEVELOPMENT/#writing-good-docstrings-for-api-docs","title":"Writing Good Docstrings for API docs:","text":"Python<pre><code>def example_function(param1: str, param2: int = 0) -&gt; bool:\n    \"\"\"Short summary of what this function does.\n\n    Longer description with more details about the function's\n    behavior, use cases, and any important notes.\n\n    Args:\n        param1: Description of first parameter\n        param2: Description of second parameter (default: 0)\n\n    Returns:\n        Description of return value\n\n    Raises:\n        ValueError: When something goes wrong\n\n    Example:\n        &gt;&gt;&gt; example_function(\"test\", 42)\n        True\n    \"\"\"\n</code></pre>"},{"location":"DEVELOPMENT/#making-documentation-changes","title":"Making Documentation Changes","text":"<ol> <li>For content/markdown: Edit files in <code>docs/</code> directory</li> <li>For API docs: Update docstrings in Python source files</li> <li>Preview changes:</li> <li>Fast: <code>mkdocs serve -f mkdocs.dev.yml --dirtyreload</code></li> <li>Full: <code>mkdocs serve</code></li> <li>Test the build: <code>mkdocs build --strict</code></li> <li>Check for broken links in the browser console</li> </ol>"},{"location":"DEVELOPMENT/#deploying-documentation","title":"Deploying Documentation","text":"Bash<pre><code># Deploy to GitHub Pages (requires push permissions)\nmkdocs gh-deploy\n\n# Deploy with custom commit message\nmkdocs gh-deploy -m \"Update documentation\"\n\n# Deploy without pushing (dry run)\nmkdocs gh-deploy --no-push\n</code></pre> <p>The site will be available at <code>https://[username].github.io/tenets/</code>.</p>"},{"location":"DEVELOPMENT/#2-making-changes","title":"2. Making Changes","text":"<p>Follow the coding standards: - Write clean, readable code - Add comprehensive docstrings (Google style) - Include type hints for all functions - Write tests for new functionality</p>"},{"location":"DEVELOPMENT/#3-committing-changes","title":"3. Committing Changes","text":"<p>We use Conventional Commits:</p> Bash<pre><code># Interactive commit\nmake commit  # or: cz commit\n\n# Manual commit (must follow format)\ngit commit -m \"feat(analyzer): add support for Rust AST parsing\"\n</code></pre> <p>Commit types: - <code>feat</code>: New feature - <code>fix</code>: Bug fix - <code>docs</code>: Documentation changes - <code>style</code>: Code style changes (formatting) - <code>refactor</code>: Code refactoring - <code>perf</code>: Performance improvements - <code>test</code>: Test additions or changes - <code>chore</code>: Maintenance tasks</p>"},{"location":"DEVELOPMENT/#4-running-tests","title":"4. Running Tests","text":"Bash<pre><code># Run all tests\nmake test\n\n# Run fast tests only\nmake test-fast\n\n# Run specific test file\npytest tests/test_analyzer.py\n\n# Run with coverage\npytest --cov=tenets --cov-report=html\n</code></pre>"},{"location":"DEVELOPMENT/#5-code-quality-checks","title":"5. Code Quality Checks","text":"Bash<pre><code># Run all checks\nmake lint\n\n# Auto-format code\nmake format\n\n# Individual tools\nblack .\nisort .\nruff check .\nmypy tenets --strict\nbandit -r tenets\n</code></pre>"},{"location":"DEVELOPMENT/#6-pushing-changes","title":"6. Pushing Changes","text":"Bash<pre><code># Pre-commit hooks will run automatically\ngit push origin feature/your-feature-name\n</code></pre>"},{"location":"DEVELOPMENT/#7-creating-a-pull-request","title":"7. Creating a Pull Request","text":"<ol> <li>Go to GitHub and create a PR</li> <li>Fill out the PR template</li> <li>Ensure all CI checks pass</li> <li>Request review from maintainers</li> </ol>"},{"location":"DEVELOPMENT/#testing","title":"Testing","text":""},{"location":"DEVELOPMENT/#test-structure","title":"Test Structure","text":"Text Only<pre><code>tests/\n\u251c\u2500\u2500 unit/              # Unit tests\n\u2502   \u251c\u2500\u2500 test_analyzer.py\n\u2502   \u251c\u2500\u2500 test_nlp.py\n\u2502   \u2514\u2500\u2500 test_scanner.py\n\u251c\u2500\u2500 integration/       # Integration tests\n\u2502   \u251c\u2500\u2500 test_cli.py\n\u2502   \u2514\u2500\u2500 test_workflow.py\n\u251c\u2500\u2500 fixtures/         # Test data\n\u2502   \u2514\u2500\u2500 sample_repo/\n\u2514\u2500\u2500 conftest.py      # Pytest configuration\n</code></pre>"},{"location":"DEVELOPMENT/#writing-tests","title":"Writing Tests","text":"Python<pre><code>\"\"\"Test module for analyzer functionality.\"\"\"\n\nimport pytest\nfrom tenets.core.analysis import CodeAnalyzer\n\n\nclass TestCodeAnalyzer:\n    \"\"\"Test suite for CodeAnalyzer.\"\"\"\n\n    @pytest.fixture\n    def analyzer(self):\n        \"\"\"Create analyzer instance.\"\"\"\n        return CodeAnalyzer()\n\n    def test_analyze_python_file(self, analyzer, tmp_path):\n        \"\"\"Test Python file analysis.\"\"\"\n        # Create test file\n        test_file = tmp_path / \"test.py\"\n        test_file.write_text(\"def hello():\\n    return 'world'\")\n\n        # Analyze\n        result = analyzer.analyze_file(test_file)\n\n        # Assertions\n        assert result.language == \"python\"\n        assert len(result.functions) == 1\n        assert result.functions[0][\"name\"] == \"hello\"\n</code></pre>"},{"location":"DEVELOPMENT/#test-markers","title":"Test Markers","text":"Bash<pre><code># Run only unit tests\npytest -m unit\n\n# Skip slow tests\npytest -m \"not slow\"\n\n# Run tests requiring git\npytest -m requires_git\n</code></pre>"},{"location":"DEVELOPMENT/#code-quality","title":"Code Quality","text":""},{"location":"DEVELOPMENT/#style-guide","title":"Style Guide","text":"<p>We follow PEP 8 with these modifications: - Line length: 100 characters - Use Black for formatting - Use Google-style docstrings</p>"},{"location":"DEVELOPMENT/#type-hints","title":"Type Hints","text":"<p>All functions must have type hints:</p> Python<pre><code>from typing import List, Optional, Dict, Any\n\n\ndef analyze_files(\n    paths: List[Path],\n    deep: bool = False,\n    max_workers: Optional[int] = None\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Analyze multiple files in parallel.\n\n    Args:\n        paths: List of file paths to analyze\n        deep: Whether to perform deep analysis\n        max_workers: Maximum number of parallel workers\n\n    Returns:\n        Dictionary containing analysis results\n    \"\"\"\n    ...\n</code></pre>"},{"location":"DEVELOPMENT/#docstrings","title":"Docstrings","text":"<p>Use Google-style docstrings:</p> Python<pre><code>def calculate_relevance(\n    file: FileAnalysis,\n    prompt: PromptContext,\n    algorithm: str = \"balanced\"\n) -&gt; float:\n    \"\"\"\n    Calculate relevance score for a file.\n\n    Uses multi-factor scoring to determine how relevant a file is\n    to the given prompt context.\n\n    Args:\n        file: Analyzed file data\n        prompt: Parsed prompt context\n        algorithm: Ranking algorithm to use\n\n    Returns:\n        Relevance score between 0.0 and 1.0\n\n    Raises:\n        ValueError: If algorithm is not recognized\n\n    Example:\n        &gt;&gt;&gt; relevance = calculate_relevance(file, prompt, \"thorough\")\n        &gt;&gt;&gt; print(f\"Relevance: {relevance:.2f}\")\n        0.85\n    \"\"\"\n    ...\n</code></pre>"},{"location":"DEVELOPMENT/#documentation","title":"Documentation","text":""},{"location":"DEVELOPMENT/#building-documentation","title":"Building Documentation","text":"Bash<pre><code># Build docs\nmake docs\n\n# Serve locally\nmake serve-docs\n# Visit http://localhost:8000\n</code></pre>"},{"location":"DEVELOPMENT/#writing-documentation","title":"Writing Documentation","text":"<ol> <li>API Documentation: Auto-generated from docstrings</li> <li>User Guides: Written in Markdown in <code>docs/</code></li> <li>Examples: Include code examples in docstrings</li> </ol>"},{"location":"DEVELOPMENT/#documentation-standards","title":"Documentation Standards","text":"<ul> <li>Use clear, concise language</li> <li>Include code examples</li> <li>Add diagrams where helpful</li> <li>Keep it up-to-date with code changes</li> </ul>"},{"location":"DEVELOPMENT/#debugging","title":"Debugging","text":""},{"location":"DEVELOPMENT/#debug-mode","title":"Debug Mode","text":"Bash<pre><code># Enable debug logging\nexport TENETS_DEBUG=true\ntenets make-context \"test\" . --verbose\n\n# Or in code\nimport logging\nlogging.basicConfig(level=logging.DEBUG)\n</code></pre>"},{"location":"DEVELOPMENT/#using-vs-code","title":"Using VS Code","text":"<p><code>.vscode/launch.json</code>: JSON<pre><code>{\n    \"version\": \"0.2.0\",\n    \"configurations\": [\n        {\n            \"name\": \"Debug tenets CLI\",\n            \"type\": \"python\",\n            \"request\": \"launch\",\n            \"module\": \"tenets.cli.main\",\n            \"args\": [\"make-context\", \"test query\", \".\"],\n            \"console\": \"integratedTerminal\"\n        }\n    ]\n}\n</code></pre></p>"},{"location":"DEVELOPMENT/#common-issues","title":"Common Issues","text":"<ol> <li>Import errors: Ensure you've installed in development mode (<code>pip install -e .</code>)</li> <li>Type errors: Run <code>mypy</code> to catch type issues</li> <li>Test failures: Check if you need to install optional dependencies</li> </ol>"},{"location":"DEVELOPMENT/#contributing-guidelines","title":"Contributing Guidelines","text":""},{"location":"DEVELOPMENT/#before-you-start","title":"Before You Start","text":"<ol> <li>Check existing issues and PRs</li> <li>Open an issue to discuss large changes</li> <li>Read the architecture documentation</li> </ol>"},{"location":"DEVELOPMENT/#code-review-checklist","title":"Code Review Checklist","text":"<ul> <li> Tests pass locally</li> <li> Code is formatted (black, isort)</li> <li> Type hints are present</li> <li> Docstrings are complete</li> <li> Tests cover new functionality</li> <li> Documentation is updated</li> <li> Commit messages follow convention</li> <li> No security issues (bandit)</li> </ul>"},{"location":"DEVELOPMENT/#getting-help","title":"Getting Help","text":"<ul> <li>Open an issue for bugs</li> <li>Start a discussion for features</li> <li>Join our Discord (coming soon)</li> <li>Email: team@tenets.dev</li> </ul>"},{"location":"DEVELOPMENT/#release--versioning","title":"Release &amp; Versioning","text":"<p>Releases are automated. Merging conventional commits into <code>main</code> (from PRs) is all you normally do.</p>"},{"location":"DEVELOPMENT/#branch-model","title":"Branch Model","text":"Branch Purpose <code>dev</code> (or feature branches) Integration / iterative work <code>main</code> Always releasable; auto-versioned on merge"},{"location":"DEVELOPMENT/#workflows-high-level","title":"Workflows (high level)","text":"<ol> <li>PR merged into <code>main</code>.</li> <li><code>version-bump.yml</code> runs:<ul> <li>Collects commits since last tag</li> <li>Determines next version:</li> <li>Major: commit body contains <code>BREAKING CHANGE:</code> or type suffixed with <code>!</code></li> <li>Minor: at least one <code>feat:</code> or <code>perf:</code> commit (performance treated as minor to signal impact)</li> <li>Patch: any <code>fix</code>, <code>refactor</code>, <code>chore</code> (unless a higher bump already chosen)</li> <li>Skip: only docs / test / style commits (no release)</li> <li>Updates <code>pyproject.toml</code></li> <li>Appends a section to <code>CHANGELOG.md</code> grouping commits (Features / Performance / Fixes / Refactoring / Chore)</li> <li>Commits with message <code>chore(release): vX.Y.Z</code> and creates annotated tag <code>vX.Y.Z</code></li> </ul> </li> <li>Tag push triggers <code>release.yml</code>:<ul> <li>Builds wheel + sdist</li> <li>Publishes to PyPI (token or Trusted Publishing)</li> <li>(Optional) Builds &amp; publishes Docker image (future enablement)</li> <li>Deploys docs (if configured) / updates site</li> </ul> </li> <li><code>release-drafter</code> (config) ensures GitHub Release notes reflect categorized changes (either via draft or final publish depending on config state).</li> </ol> <p>You do NOT run <code>cz bump</code> manually during normal flow; the workflow handles versioning.</p>"},{"location":"DEVELOPMENT/#conventional-commit-expectations","title":"Conventional Commit Expectations","text":"<p>Use clear scopes where possible: Text Only<pre><code>feat(ranking): add semantic similarity signal\nfix(cli): prevent crash on empty directory\nperf(analyzer): cache parsed ASTs\nrefactor(config): simplify loading logic\ndocs: update quickstart for --copy flag\n</code></pre></p> <p>Edge cases: - Multiple commit types: highest precedence decides (major &gt; minor &gt; patch) - Mixed docs + fix: still releases (fix wins) - Only docs/test/style: skipped; no tag produced</p>"},{"location":"DEVELOPMENT/#first-release-bootstrap","title":"First Release (Bootstrap)","text":"<p>If no existing tag: 1. Merge initial feature set into <code>main</code> 2. Push a commit with <code>feat: initial release</code> (or similar) 3. Workflow sets version to <code>0.1.0</code> (or bump logic starting point defined in workflow)</p> <p>If you need a different starting version (e.g. <code>0.3.0</code>): create an annotated tag manually once, then subsequent merges resume automation.</p>"},{"location":"DEVELOPMENT/#manual--emergency-release","title":"Manual / Emergency Release","text":"<p>Only when automation is blocked: Bash<pre><code>git checkout main &amp;&amp; git pull\ncz bump --increment PATCH  # or MINOR / MAJOR\ngit push &amp;&amp; git push --tags\n</code></pre> Monitor <code>release.yml</code>. After resolution, revert to automated flow.</p>"},{"location":"DEVELOPMENT/#verifying-a-release","title":"Verifying a Release","text":"<p>After automation completes: Bash<pre><code>pip install --no-cache-dir tenets==&lt;new_version&gt;\ntenets --version\n</code></pre> Smoke test a core command: Bash<pre><code>tenets distill \"smoke\" . --max-tokens 2000 --mode fast --stats || true\n</code></pre></p>"},{"location":"DEVELOPMENT/#troubleshooting","title":"Troubleshooting","text":"Symptom Likely Cause Fix No new tag after merge Only docs/test/style commits Land a non-skipped commit (e.g. fix) Wrong bump size Commit type misclassified Amend / add corrective commit (e.g. feat) PyPI publish failed Missing / invalid <code>PYPI_API_TOKEN</code> or Trusted Publishing not approved yet Add token or approve in PyPI UI Changelog missing section Commit type not in allowed list Ensure conventional type used Duplicate release notes Manual tag + automated tag Avoid manual tagging except emergencies"},{"location":"DEVELOPMENT/#philosophy","title":"Philosophy","text":"<p>Keep <code>main</code> always shippable. Small, frequent releases reduce risk and keep context fresh for users.</p>"},{"location":"DEVELOPMENT/#advanced-topics","title":"Advanced Topics","text":""},{"location":"DEVELOPMENT/#adding-a-new-language-analyzer","title":"Adding a New Language Analyzer","text":"<ol> <li> <p>Create analyzer in <code>tenets/core/analysis/</code>:    Python<pre><code>class RustAnalyzer(LanguageAnalyzer):\n    language_name = \"rust\"\n\n    def extract_imports(self, content: str) -&gt; List[Import]:\n        # Implementation\n        ...\n</code></pre></p> </li> <li> <p>Register in <code>analysis/analyzer.py</code>:    Python<pre><code>analyzers['.rs'] = RustAnalyzer()\n</code></pre></p> </li> <li> <p>Add tests in <code>tests/unit/test_rust_analyzer.py</code></p> </li> </ol>"},{"location":"DEVELOPMENT/#creating-custom-ranking-algorithms","title":"Creating Custom Ranking Algorithms","text":"<ol> <li> <p>Implement algorithm:    Python<pre><code>class SecurityRanking:\n    def score_file(self, file, prompt):\n        # Custom scoring logic\n        ...\n</code></pre></p> </li> <li> <p>Register algorithm:    Python<pre><code>@register_algorithm(\"security\")\nclass SecurityRanking:\n    ...\n</code></pre></p> </li> <li> <p>Document usage in <code>docs/api.md</code></p> </li> </ol> <p>Happy coding! \ud83d\ude80 Remember: context is everything.</p>"},{"location":"SECURITY/","title":"Security Policy","text":""},{"location":"SECURITY/#supported-versions","title":"Supported Versions","text":"<p>The project is pre-1.0; security fixes are applied to the latest released version. Older versions may not receive backports.</p>"},{"location":"SECURITY/#reporting-a-vulnerability","title":"Reporting a Vulnerability","text":"<p>Email: security@tenets.dev (or team@tenets.dev if unreachable)</p> <p>Please include: - Description of the issue - Steps to reproduce / proof-of-concept - Potential impact / affected components - Your environment (OS, Python, tenets version)</p> <p>We aim to acknowledge within 3 business days and provide a remediation ETA after triage.</p>"},{"location":"SECURITY/#responsible-disclosure","title":"Responsible Disclosure","text":"<p>Do not open public issues for exploitable vulnerabilities. Use the private email above. We will coordinate disclosure and credit (if desired) after a fix is released.</p>"},{"location":"SECURITY/#scope","title":"Scope","text":"<p>Tenets runs locally. Primary concerns: - Arbitrary code execution via file parsing - Directory traversal / path injection - Insecure temporary file handling - Leakage of private repository data beyond intended output</p> <p>Out of scope: - Issues requiring malicious local user privilege escalation - Vulnerabilities in optional third-party dependencies (report upstream)</p>"},{"location":"SECURITY/#security-best-practices-users","title":"Security Best Practices (Users)","text":"<ul> <li>Pin versions in production workflows</li> <li>Run latest patch release</li> <li>Review output before sharing externally</li> <li>Avoid running against untrusted repositories without isolation (use containers)</li> </ul>"},{"location":"SECURITY/#patching-process","title":"Patching Process","text":"<ol> <li>Triage &amp; reproduce</li> <li>Develop fix in private branch</li> <li>Add regression tests</li> <li>Coordinate release (patch version bump)</li> <li>Publish advisory in CHANGELOG / release notes</li> </ol>"},{"location":"SECURITY/#contact","title":"Contact","text":"<p>security@tenets.dev</p>"},{"location":"TESTING/","title":"Testing","text":""},{"location":"TESTING/#quick-start","title":"Quick Start","text":"Bash<pre><code># One-liner (editable install + test deps + coverage helpers)\npip install -e '.[test]' pytest pytest-cov\n\n# Run all tests (quiet)\npytest -q\n\n# Run with coverage + fail if below threshold (adjust as policy evolves)\npytest --cov=tenets --cov-report=term-missing --cov-fail-under=70\n\n# Generate XML (CI) + HTML\npytest --cov=tenets --cov-report=xml --cov-report=html\n\n# Open HTML (macOS/Linux)\nopen htmlcov/index.html || xdg-open htmlcov/index.html || true\n\n# Specific test file / test\npytest tests/core/analysis/test_analyzer.py::test_basic_python_analysis -q\n\n# Pattern match\npytest -k analyzer -q\n\n# Parallel (if pytest-xdist installed)\npytest -n auto\n</code></pre> <p>Optional feature extras (install before running related tests): Bash<pre><code>pip install -e '.[light]'   # BM25 / TF-IDF / YAKE ranking tests\npip install -e '.[viz]'     # Visualization tests\npip install -e '.[ml]'      # Embedding / semantic tests (heavy)\n</code></pre></p>"},{"location":"TESTING/#test-structure","title":"Test Structure","text":"Text Only<pre><code>tests/\n\u251c\u2500\u2500 conftest.py              # Shared fixtures\n\u251c\u2500\u2500 test_config.py           # Config tests\n\u251c\u2500\u2500 test_tenets.py           # Main module tests\n\u251c\u2500\u2500 core/\n\u2502   \u251c\u2500\u2500 analysis/           # Code analysis tests\n\u2502   \u251c\u2500\u2500 distiller/          # Context distillation tests\n\u2502   \u251c\u2500\u2500 git/                # Git integration tests\n\u2502   \u251c\u2500\u2500 prompt/             # Prompt parsing tests\n\u2502   \u251c\u2500\u2500 ranker/             # File ranking tests\n\u2502   \u251c\u2500\u2500 session/            # Session management tests\n\u2502   \u2514\u2500\u2500 summarizer/         # Summarization tests\n\u251c\u2500\u2500 storage/\n\u2502   \u251c\u2500\u2500 test_cache.py       # Caching system tests\n\u2502   \u251c\u2500\u2500 test_session_db.py  # Session persistence tests\n\u2502   \u2514\u2500\u2500 test_sqlite.py      # SQLite utilities tests\n\u2514\u2500\u2500 utils/\n    \u251c\u2500\u2500 test_scanner.py     # File scanning tests\n    \u251c\u2500\u2500 test_tokens.py      # Token counting tests\n    \u2514\u2500\u2500 test_logger.py      # Logging tests\n</code></pre>"},{"location":"TESTING/#running-tests","title":"Running Tests","text":""},{"location":"TESTING/#by-category","title":"By Category","text":"Bash<pre><code># Unit tests only\npytest -m unit\n\n# Integration tests\npytest -m integration\n\n# Skip slow tests\npytest -m \"not slow\"\n\n# Tests requiring git\npytest -m requires_git\n\n# Tests requiring ML dependencies\npytest -m requires_ml\n</code></pre>"},{"location":"TESTING/#coverage-reports","title":"Coverage Reports","text":"Bash<pre><code># Terminal report\npytest --cov=tenets --cov-report=term-missing\n\n# Enforce minimum (CI/local gate)\npytest --cov=tenets --cov-report=term-missing --cov-fail-under=80\n\n# HTML report\npytest --cov=tenets --cov-report=html\n\n# XML for CI services (Codecov)\npytest --cov=tenets --cov-report=xml\n</code></pre>"},{"location":"TESTING/#debug-mode","title":"Debug Mode","text":"Bash<pre><code># Show print statements\npytest -s\n\n# Stop on first failure\npytest -x\n\n# Drop into debugger on failure\npytest --pdb\n\n# Verbose output\npytest -vv\n</code></pre>"},{"location":"TESTING/#writing-tests","title":"Writing Tests","text":""},{"location":"TESTING/#basic-test","title":"Basic Test","text":"Python<pre><code>def test_feature(config, analyzer):\n    \"\"\"Test feature description.\"\"\"\n    result = analyzer.analyze_file(Path(\"test.py\"))\n    assert result.language == \"python\"\n</code></pre>"},{"location":"TESTING/#using-fixtures","title":"Using Fixtures","text":"Python<pre><code>@pytest.fixture\ndef temp_project(tmp_path):\n    \"\"\"Create temporary project structure.\"\"\"\n    (tmp_path / \"src\").mkdir()\n    (tmp_path / \"src/main.py\").write_text(\"print('hello')\")\n    return tmp_path\n\ndef test_with_project(temp_project):\n    files = list(temp_project.glob(\"**/*.py\"))\n    assert len(files) == 1\n</code></pre>"},{"location":"TESTING/#mocking","title":"Mocking","text":"Python<pre><code>from unittest.mock import Mock, patch\n\ndef test_with_mock():\n    with patch('tenets.utils.tokens.count_tokens') as mock_count:\n        mock_count.return_value = 100\n        # test code\n</code></pre>"},{"location":"TESTING/#parametrized-tests","title":"Parametrized Tests","text":"Python<pre><code>@pytest.mark.parametrize(\"input,expected\", [\n    (\"test.py\", \"python\"),\n    (\"test.js\", \"javascript\"),\n    (\"test.go\", \"go\"),\n])\ndef test_language_detection(analyzer, input, expected):\n    assert analyzer._detect_language(Path(input)) == expected\n</code></pre>"},{"location":"TESTING/#test-markers","title":"Test Markers","text":"<p>Add to test functions:</p> Python<pre><code>@pytest.mark.slow\ndef test_heavy_operation():\n    pass\n\n@pytest.mark.requires_git\ndef test_git_features():\n    pass\n\n@pytest.mark.skipif(not HAS_TIKTOKEN, reason=\"tiktoken not installed\")\ndef test_token_counting():\n    pass\n</code></pre>"},{"location":"TESTING/#ci-integration","title":"CI Integration","text":"YAML<pre><code># .github/workflows/test.yml\n- name: Run tests\n  run: |\n    pytest --cov=tenets --cov-report=xml\n\n- name: Upload coverage\n  uses: codecov/codecov-action@v3\n  with:\n    file: ./coverage.xml\n</code></pre>"},{"location":"TESTING/#pre-commit-hook","title":"Pre-commit Hook","text":"YAML<pre><code># .pre-commit-config.yaml\n- repo: local\n  hooks:\n    - id: tests\n      name: tests\n      entry: pytest\n      language: system\n      pass_filenames: false\n      always_run: true\n</code></pre>"},{"location":"TESTING/#release-test-checklist","title":"Release Test Checklist","text":"<p>Before tagging a release:</p> Bash<pre><code># 1. Clean environment\nrm -rf .venv dist build *.egg-info &amp;&amp; python -m venv .venv &amp;&amp; source .venv/bin/activate\n\n# 2. Install with all needed extras for full test surface\npip install -e '.[all,test]' pytest pytest-cov\n\n# 3. Lint / type (if tools configured)\n# ruff check .\n# mypy tenets\n\n# 4. Run tests with coverage gate\npytest --cov=tenets --cov-report=term-missing --cov-fail-under=80\n\n# 5. Spot-check critical CLI commands\nfor cmd in \\\n  \"distill 'smoke test' --stats\" \\\n  \"instill 'example tenet'\" \\\n  \"session create release-smoke\" \\\n  \"config cache-stats\"; do\n  echo \"tenets $cmd\"; tenets $cmd || exit 1; done\n\n# 6. Build sdist/wheel\npython -m build\n\n# 7. Install built artifact in fresh venv &amp; re-smoke\npython -m venv verify &amp;&amp; source verify/bin/activate &amp;&amp; pip install dist/*.whl &amp;&amp; tenets version\n</code></pre> <p>Minimal CHANGELOG update + version bump in <code>tenets/__init__.py</code> must precede tagging.</p>"},{"location":"TESTING/#performance-testing","title":"Performance Testing","text":"Bash<pre><code># Benchmark tests\npytest tests/performance/ --benchmark-only\n\n# Profile slow tests\npytest --durations=10\n</code></pre>"},{"location":"TESTING/#troubleshooting","title":"Troubleshooting","text":""},{"location":"TESTING/#common-issues","title":"Common Issues","text":"<p>Import errors: Ensure package is installed with test extras: Bash<pre><code>pip install -e \".[test]\"\n</code></pre></p> <p>Slow tests: Use parallel execution: Bash<pre><code>pytest -n auto\n</code></pre></p> <p>Flaky tests: Re-run failures: Bash<pre><code>pytest --reruns 3\n</code></pre></p> <p>Memory issues: Run tests in chunks: Bash<pre><code>pytest tests/core/\npytest tests/storage/\npytest tests/utils/\n</code></pre></p>"},{"location":"TESTING/#coverage-goals","title":"Coverage Goals","text":"<ul> <li>Overall: &gt;80%</li> <li>Core logic: &gt;90%</li> <li>Error paths: &gt;70%</li> <li>Utils: &gt;85%</li> </ul> <p>Check current coverage: Bash<pre><code>pytest --cov=tenets --cov-report=term-missing | grep TOTAL\n</code></pre></p>"},{"location":"VIZ_CHEATSHEET/","title":"Tenets Viz Deps Command Cheat Sheet","text":""},{"location":"VIZ_CHEATSHEET/#installation","title":"Installation","text":"Bash<pre><code>pip install tenets[viz]  # Install visualization dependencies\n</code></pre>"},{"location":"VIZ_CHEATSHEET/#basic-commands","title":"Basic Commands","text":""},{"location":"VIZ_CHEATSHEET/#simple-usage","title":"Simple Usage","text":"Bash<pre><code>tenets viz deps                     # Auto-detect project, show ASCII tree\ntenets viz deps .                   # Analyze current directory\ntenets viz deps src/                # Analyze specific directory\n</code></pre>"},{"location":"VIZ_CHEATSHEET/#output-formats","title":"Output Formats","text":"Bash<pre><code>tenets viz deps --format ascii      # Terminal tree (default)\ntenets viz deps --format svg --output arch.svg     # Scalable vector graphics\ntenets viz deps --format png --output arch.png     # PNG image\ntenets viz deps --format html --output deps.html   # Interactive HTML\ntenets viz deps --format dot --output graph.dot    # Graphviz DOT\ntenets viz deps --format json --output data.json   # Raw JSON data\n</code></pre>"},{"location":"VIZ_CHEATSHEET/#aggregation-levels","title":"Aggregation Levels","text":"Bash<pre><code>tenets viz deps --level file        # Individual file dependencies (detailed)\ntenets viz deps --level module      # Module-level aggregation (recommended)\ntenets viz deps --level package     # Package-level view (high-level)\n</code></pre>"},{"location":"VIZ_CHEATSHEET/#clustering-options","title":"Clustering Options","text":"Bash<pre><code>tenets viz deps --cluster-by directory   # Group by directory structure\ntenets viz deps --cluster-by module      # Group by module\ntenets viz deps --cluster-by package     # Group by package\n</code></pre>"},{"location":"VIZ_CHEATSHEET/#layout-algorithms","title":"Layout Algorithms","text":"Bash<pre><code>tenets viz deps --layout hierarchical   # Tree-like layout (default)\ntenets viz deps --layout circular       # Circular/radial layout\ntenets viz deps --layout shell          # Concentric circles\ntenets viz deps --layout kamada         # Force-directed layout\n</code></pre>"},{"location":"VIZ_CHEATSHEET/#filtering","title":"Filtering","text":"Bash<pre><code># Include specific patterns\ntenets viz deps --include \"*.py\"                    # Only Python files\ntenets viz deps --include \"*.js,*.jsx\"              # JavaScript files\ntenets viz deps --include \"src/**/*.py\"             # Python in src/\n\n# Exclude patterns\ntenets viz deps --exclude \"*test*\"                  # No test files\ntenets viz deps --exclude \"*.min.js,node_modules\"   # Skip minified and deps\n\n# Combined\ntenets viz deps --include \"*.py\" --exclude \"*test*\"\n</code></pre>"},{"location":"VIZ_CHEATSHEET/#node-limiting","title":"Node Limiting","text":"Bash<pre><code>tenets viz deps --max-nodes 50      # Show only top 50 most connected nodes\ntenets viz deps --max-nodes 100     # Useful for large projects\n</code></pre>"},{"location":"VIZ_CHEATSHEET/#real-world-examples","title":"Real-World Examples","text":""},{"location":"VIZ_CHEATSHEET/#for-documentation","title":"For Documentation","text":"Bash<pre><code># Clean architecture diagram for docs\ntenets viz deps . --level package --format svg --output docs/architecture.svg\n\n# Module overview with clustering\ntenets viz deps . --level module --cluster-by directory --format png --output modules.png\n</code></pre>"},{"location":"VIZ_CHEATSHEET/#for-code-review","title":"For Code Review","text":"Bash<pre><code># Interactive exploration\ntenets viz deps . --level module --format html --output review.html\n\n# Focused on specific subsystem\ntenets viz deps src/api --include \"*.py\" --format svg --output api_deps.svg\n</code></pre>"},{"location":"VIZ_CHEATSHEET/#for-refactoring","title":"For Refactoring","text":"Bash<pre><code># Find circular dependencies\ntenets viz deps . --layout circular --format html --output circular_deps.html\n\n# Identify tightly coupled modules\ntenets viz deps . --level module --layout circular --max-nodes 50 --output coupling.svg\n</code></pre>"},{"location":"VIZ_CHEATSHEET/#for-large-projects","title":"For Large Projects","text":"Bash<pre><code># Top-level overview\ntenets viz deps . --level package --max-nodes 20 --format svg --output overview.svg\n\n# Most connected files\ntenets viz deps . --max-nodes 100 --format html --output top100.html\n\n# Specific subsystem deep dive\ntenets viz deps backend/ --level module --cluster-by module --format html -o backend.html\n</code></pre>"},{"location":"VIZ_CHEATSHEET/#project-type-auto-detection","title":"Project Type Auto-Detection","text":"<p>The command automatically detects: - Python: Packages, Django, Flask, FastAPI - JavaScript/TypeScript: Node.js, React, Vue, Angular - Java: Maven, Gradle, Spring - Go: Go modules - Rust: Cargo projects - Ruby: Rails, Gems - PHP: Laravel, Composer - And more...</p>"},{"location":"VIZ_CHEATSHEET/#tips","title":"Tips","text":"<ol> <li>Start Simple: Use <code>tenets viz deps</code> first to see what's detected</li> <li>Use Levels: Start with <code>--level package</code> for overview, drill down to <code>module</code> or <code>file</code></li> <li>Interactive HTML: Best for exploration, use <code>--format html</code></li> <li>Filter Noise: Use <code>--exclude \"*test*,*mock*\"</code> to focus on core code</li> <li>Save Time: Use <code>--max-nodes</code> for large codebases</li> <li>Documentation: SVG format scales well for docs</li> <li>Clustering: Helps organize complex graphs visually</li> </ol>"},{"location":"VIZ_CHEATSHEET/#troubleshooting","title":"Troubleshooting","text":"Bash<pre><code># Check if dependencies are installed\npython -c \"import networkx, matplotlib, graphviz, plotly\" 2&gt;/dev/null &amp;&amp; echo \"All deps OK\" || echo \"Run: pip install tenets[viz]\"\n\n# Debug mode\nTENETS_LOG_LEVEL=DEBUG tenets viz deps . 2&gt;&amp;1 | grep -E \"(Detected|Found|Analyzing)\"\n\n# If graph is too large\ntenets viz deps . --max-nodes 50 --level module  # Reduce nodes and aggregate\n</code></pre>"},{"location":"VIZ_CHEATSHEET/#output-examples","title":"Output Examples","text":""},{"location":"VIZ_CHEATSHEET/#ascii-tree-default","title":"ASCII Tree (default)","text":"Text Only<pre><code>Dependency Graph:\n==================================================\n\nmain.py\n  \u2514\u2500&gt; utils.py\n  \u2514\u2500&gt; config.py\n  \u2514\u2500&gt; models.py\n\nutils.py\n  \u2514\u2500&gt; config.py\n\nmodels.py\n  \u2514\u2500&gt; utils.py\n</code></pre>"},{"location":"VIZ_CHEATSHEET/#what-you-get","title":"What You Get","text":"<ul> <li>Project Info: Auto-detected type, languages, frameworks</li> <li>Entry Points: Identified main files (main.py, index.js, etc.)</li> <li>Dependency Graph: Visual representation of code relationships</li> <li>Multiple Views: File, module, or package level perspectives</li> </ul>"},{"location":"best-practices/","title":"Best Practices","text":""},{"location":"best-practices/#repository-setup","title":"Repository Setup","text":""},{"location":"best-practices/#clean-working-directory","title":"Clean Working Directory","text":"<p>Always run Tenets on a clean working directory for accurate results: Bash<pre><code>git status  # Ensure no uncommitted changes\ntenets examine\n</code></pre></p>"},{"location":"best-practices/#gitignore-configuration","title":"Gitignore Configuration","text":"<p>Ensure your <code>.gitignore</code> is properly configured to exclude: - Build artifacts - Node modules - Virtual environments - Temporary files</p>"},{"location":"best-practices/#command-usage","title":"Command Usage","text":""},{"location":"best-practices/#examine-command","title":"Examine Command","text":"<ul> <li>Use <code>--format</code> for different output formats</li> <li>Filter by language with <code>--language</code></li> <li>Focus on specific paths for targeted analysis</li> </ul>"},{"location":"best-practices/#chronicle-command","title":"Chronicle Command","text":"<ul> <li>Use time ranges appropriate to your project's activity</li> <li>Filter by author for team member contributions</li> <li>Combine with <code>--pattern</code> for specific file analysis</li> </ul>"},{"location":"best-practices/#distill-command","title":"Distill Command","text":"<ul> <li>Run after significant development milestones</li> <li>Use to generate weekly or monthly insights</li> <li>Combine with chronicle for historical context</li> </ul>"},{"location":"best-practices/#performance-optimization","title":"Performance Optimization","text":""},{"location":"best-practices/#large-repositories","title":"Large Repositories","text":"<p>For repositories with many files: Bash<pre><code># Focus on specific directories\ntenets examine src/ --depth 3\n\n# Exclude certain patterns\ntenets examine --exclude \"**/test/**\"\n</code></pre></p>"},{"location":"best-practices/#memory-management","title":"Memory Management","text":"<ul> <li>Use <code>--batch-size</code> for large analyses</li> <li>Enable streaming output with <code>--stream</code></li> </ul>"},{"location":"best-practices/#integration","title":"Integration","text":""},{"location":"best-practices/#cicd-pipeline","title":"CI/CD Pipeline","text":"<p>Add Tenets to your CI pipeline: YAML<pre><code>- name: Code Analysis\n  run: |\n    pip install tenets\n    tenets examine --format json &gt; analysis.json\n</code></pre></p>"},{"location":"best-practices/#pre-commit-hooks","title":"Pre-commit Hooks","text":"<p>Use Tenets in pre-commit hooks: YAML<pre><code>repos:\n  - repo: local\n    hooks:\n      - id: tenets-check\n        name: Tenets Analysis\n        entry: tenets examine --quick\n        language: system\n        pass_filenames: false\n</code></pre></p>"},{"location":"best-practices/#team-collaboration","title":"Team Collaboration","text":""},{"location":"best-practices/#sharing-reports","title":"Sharing Reports","text":"<ul> <li>Generate HTML reports for stakeholder review</li> <li>Export JSON for further processing</li> <li>Use visualizations for architecture discussions</li> </ul>"},{"location":"best-practices/#code-reviews","title":"Code Reviews","text":"<p>Use Tenets output to: - Identify complex areas needing review - Track ownership changes - Monitor technical debt</p>"},{"location":"best-practices/#next-steps","title":"Next Steps","text":"<ul> <li>See Examples for real-world scenarios</li> <li>Review CLI Reference for all options</li> <li>Check Configuration for customization</li> </ul>"},{"location":"docs/","title":"Documentation","text":"<p>Welcome to the Tenets documentation hub. Explore guides and references below.</p> <ul> <li>Quick Start: Get started fast</li> <li>Supported Languages: List</li> <li>CLI Reference: Commands</li> <li>Configuration: Config guide</li> <li> <p>Architecture: System overview</p> </li> <li> <p>API Reference: Browse API</p> </li> </ul> <p>If you were looking for the homepage, go to /.</p>"},{"location":"examples/","title":"Examples","text":""},{"location":"examples/#basic-analysis-examples","title":"Basic Analysis Examples","text":""},{"location":"examples/#analyzing-a-python-project","title":"Analyzing a Python Project","text":"Bash<pre><code># Basic examination\ntenets examine my_python_project/\n\n# With specific focus\ntenets examine my_python_project/ --language python --depth 3\n\n# Output to JSON\ntenets examine my_python_project/ --format json &gt; analysis.json\n</code></pre>"},{"location":"examples/#analyzing-a-javascripttypescript-project","title":"Analyzing a JavaScript/TypeScript Project","text":"Bash<pre><code># Examine with TypeScript support\ntenets examine frontend/ --language typescript\n\n# Exclude node_modules\ntenets examine frontend/ --exclude \"**/node_modules/**\"\n</code></pre>"},{"location":"examples/#chronicle-examples","title":"Chronicle Examples","text":""},{"location":"examples/#team-contribution-analysis","title":"Team Contribution Analysis","text":"Bash<pre><code># Last month's team activity\ntenets chronicle --days 30 --format table\n\n# Specific developer's contributions\ntenets chronicle --author \"Jane Doe\" --days 90\n\n# Focus on feature branch\ntenets chronicle --branch feature/new-ui --days 14\n</code></pre>"},{"location":"examples/#release-analysis","title":"Release Analysis","text":"Bash<pre><code># Changes between releases\ntenets chronicle --from v1.0.0 --to v2.0.0\n\n# Recent hotfixes\ntenets chronicle --pattern \"**/hotfix/**\" --days 7\n</code></pre>"},{"location":"examples/#distill-examples","title":"Distill Examples","text":""},{"location":"examples/#project-insights","title":"Project Insights","text":"Bash<pre><code># Generate comprehensive insights\ntenets distill --comprehensive\n\n# Quick summary\ntenets distill --quick\n\n# Export for reporting\ntenets distill --format markdown &gt; insights.md\n</code></pre>"},{"location":"examples/#visualization-examples","title":"Visualization Examples","text":""},{"location":"examples/#architecture-visualization","title":"Architecture Visualization","text":"Bash<pre><code># Interactive HTML graph\ntenets viz --output architecture.html\n\n# Include all relationships\ntenets viz --include-all --output full-graph.html\n\n# Focus on core modules\ntenets viz --filter \"core/**\" --output core-modules.html\n</code></pre>"},{"location":"examples/#momentum-tracking","title":"Momentum Tracking","text":""},{"location":"examples/#development-velocity","title":"Development Velocity","text":"Bash<pre><code># Weekly momentum report\ntenets momentum --period week\n\n# Monthly trends\ntenets momentum --period month --format chart\n\n# Team momentum\ntenets momentum --team --days 30\n</code></pre>"},{"location":"examples/#advanced-combinations","title":"Advanced Combinations","text":""},{"location":"examples/#pre-release-audit","title":"Pre-Release Audit","text":"Bash<pre><code># Full pre-release analysis\ntenets examine --comprehensive &gt; examine-report.txt\ntenets chronicle --days 30 &gt; chronicle-report.txt\ntenets distill --format json &gt; insights.json\ntenets viz --output release-architecture.html\n</code></pre>"},{"location":"examples/#technical-debt-assessment","title":"Technical Debt Assessment","text":"Bash<pre><code># Identify complex areas\ntenets examine --metric complexity --threshold high\n\n# Find stale code\ntenets chronicle --stale --days 180\n\n# Ownership gaps\ntenets examine --ownership --unowned\n</code></pre>"},{"location":"examples/#team-performance-review","title":"Team Performance Review","text":"Bash<pre><code># Individual contributions\nfor author in \"Alice\" \"Bob\" \"Charlie\"; do\n  tenets chronicle --author \"$author\" --days 90 &gt; \"$author-report.txt\"\ndone\n\n# Team visualization\ntenets viz --team --output team-collaboration.html\n</code></pre>"},{"location":"examples/#integration-examples","title":"Integration Examples","text":""},{"location":"examples/#github-actions","title":"GitHub Actions","text":"YAML<pre><code>name: Code Analysis\non: [push, pull_request]\n\njobs:\n  analyze:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2\n      - name: Install Tenets\n        run: pip install tenets\n      - name: Run Analysis\n        run: |\n          tenets examine --format json &gt; examine.json\n          tenets chronicle --days 7 --format json &gt; chronicle.json\n      - name: Upload Results\n        uses: actions/upload-artifact@v2\n        with:\n          name: analysis-results\n          path: |\n            examine.json\n            chronicle.json\n</code></pre>"},{"location":"examples/#pre-commit-hook","title":"Pre-commit Hook","text":"YAML<pre><code># .pre-commit-config.yaml\nrepos:\n  - repo: local\n    hooks:\n      - id: tenets-complexity\n        name: Check Code Complexity\n        entry: tenets examine --metric complexity --fail-on high\n        language: system\n        pass_filenames: false\n</code></pre>"},{"location":"examples/#next-steps","title":"Next Steps","text":"<ul> <li>Review Best Practices for optimal usage</li> <li>See CLI Reference for all available options</li> <li>Check Configuration for customization options</li> </ul>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.9 or higher</li> <li>Git</li> </ul>"},{"location":"installation/#install-from-pypi","title":"Install from PyPI","text":"Bash<pre><code>pip install tenets\n</code></pre>"},{"location":"installation/#install-from-source","title":"Install from Source","text":"Bash<pre><code>git clone https://github.com/yourusername/tenets.git\ncd tenets\npip install -e .\n</code></pre>"},{"location":"installation/#verify-installation","title":"Verify Installation","text":"Bash<pre><code>tenets --version\n</code></pre>"},{"location":"installation/#next-steps","title":"Next Steps","text":"<p>After installation, see Quick Start to get started with your first analysis.</p>"},{"location":"quickstart/","title":"Quickstart","text":""},{"location":"quickstart/#real-world-flow-system-instruction--tenets--sessions","title":"Real-world flow: System instruction + Tenets + Sessions","text":"Bash<pre><code># Create a working session\ntenets session create auth-refresh\n\n# Add guiding principles (tenets)\ntenets tenet add \"Prefer small, safe diffs\" --priority high --category style\ntenets tenet add \"Always validate user input\" --priority critical --category security\n\n# Apply tenets for this session\ntenets instill --session auth-refresh\n\n# Set a global system instruction\ntenets system-instruction set \"You are a senior engineer. Add tests and document trade-offs.\" --enable\n\n# Build context with transformations for token efficiency\ntenets distill \"add OAuth2 refresh tokens\" --session auth-refresh --remove-comments --condense\n\n# Pin files as you learn what matters\ntenets instill --session auth-refresh --add-file src/auth/service.py --add-folder src/auth/routes\ntenets instill --session auth-refresh --list-pinned\n</code></pre> <p>See also: CLI &gt; System Instruction Commands, Tenet Commands, and Instill.</p>"},{"location":"quickstart/#quick-start","title":"Quick Start","text":"<p>Get productive with Tenets in under 60 seconds.</p>"},{"location":"quickstart/#1-install","title":"1. Install","text":"Bash<pre><code>pip install tenets\n</code></pre>"},{"location":"quickstart/#2-generate-context-cli","title":"2. Generate Context (CLI)","text":"Bash<pre><code>tenets distill \"add optimistic locking to order updates\"\n</code></pre> <p>Copy straight to your clipboard:</p> Bash<pre><code>tenets distill \"refactor payment flow\" --copy\n</code></pre> <p>Or enable auto-copy in <code>tenets.toml</code>:</p> TOML<pre><code>[output]\ncopy_on_distill = true\n</code></pre>"},{"location":"quickstart/#3-refine","title":"3. Refine","text":"<p>Pin or force-include critical files:</p> Bash<pre><code># Build context for investigation\ntenets distill \"investigate cache stampede\"\n\n# Pin files are managed through instill command for sessions\ntenets instill --add-file cache/*.py --add-file config/settings.py\n</code></pre> <p>Exclude noise:</p> Bash<pre><code>tenets distill \"debug webhook\" --exclude \"**/migrations/**,**/tests/**\"\n</code></pre>"},{"location":"quickstart/#4-python-api","title":"4. Python API","text":"Python<pre><code>from tenets import Tenets\n\ntenets = Tenets()\nresult = tenets.distill(\n    prompt=\"implement bulk import\",\n    max_tokens=80_000,\n)\nprint(result.token_count, \"tokens\")\n# Copy is done via CLI flag --copy or config setting\n</code></pre>"},{"location":"quickstart/#5-sessions-iterate","title":"5. Sessions (Iterate)","text":"Python<pre><code>tenets = Tenets()\n# Sessions are managed through distill parameters\nfirst = tenets.distill(\"trace 500 errors in checkout\", session_name=\"checkout-fixes\")\nsecond = tenets.distill(\"add instrumentation around payment retries\", session_name=\"checkout-fixes\")\n</code></pre>"},{"location":"quickstart/#6-visualization--insight","title":"6. Visualization &amp; Insight","text":"Bash<pre><code># Complexity &amp; hotspots\ntenets examine . --show-details --hotspots\n\n# Dependency graph (Interactive HTML)\ntenets viz deps --format html --output deps.html\n</code></pre>"},{"location":"quickstart/#7-next","title":"7. Next","text":"<ul> <li>See full CLI options: CLI Reference</li> <li>Tune ranking &amp; tokens: Configuration</li> <li>Dive deeper: Architecture</li> </ul>"},{"location":"supported-languages/","title":"Supported Languages","text":"<p>Tenets ships with first-class analyzers for a broad set of ecosystems. Each analyzer extracts structural signals (definitions, imports, dependencies) that feed ranking.</p> Language / Tech Analyzer Class Extensions Notes Python PythonAnalyzer .py AST parsing, imports, class/function graph JavaScript / TypeScript* JavaScriptAnalyzer .js, .jsx, .ts, .tsx Lightweight regex/heuristic (TypeScript treated as JS for now) Java JavaAnalyzer .java Package &amp; import extraction Go GoAnalyzer .go Import graph, function signatures C# CSharpAnalyzer .cs Namespace &amp; using directives (great for Unity scripts) C / C++ CppAnalyzer .c, .h, .cpp, .hpp Include graph detection Rust RustAnalyzer .rs Module/use extraction Scala ScalaAnalyzer .scala Object/class/trait discovery Kotlin KotlinAnalyzer .kt, .kts Package &amp; import extraction Swift SwiftAnalyzer .swift Import/use lines PHP PhpAnalyzer .php Namespace/use detection Ruby RubyAnalyzer .rb Class/module defs Dart DartAnalyzer .dart Import and class/function capture GDScript (Godot) GDScriptAnalyzer .gd Signals + extends parsing HTML HTMLAnalyzer .html, .htm Link/script/style references CSS CSSAnalyzer .css @import and rule summarization Generic Text GenericAnalyzer * (fallback) Used when no specific analyzer matches <p>*TypeScript currently leverages the JavaScript analyzer (roadmap: richer TS-specific parsing).</p>"},{"location":"supported-languages/#detection-rules","title":"Detection Rules","text":"<p>File extension matching selects the analyzer. Unsupported files fall back to the generic analyzer supplying minimal term frequency and path heuristics.</p>"},{"location":"supported-languages/#adding-a-new-language","title":"Adding a New Language","text":"<ol> <li>Subclass <code>LanguageAnalyzer</code> in <code>tenets/core/analysis/implementations</code></li> <li>Implement <code>match(path)</code> and <code>analyze(content)</code></li> <li>Register in the analyzer registry (if dynamic) or import to ensure discovery</li> <li>Add tests under <code>tests/core/analysis/implementations</code></li> <li>Update this page</li> </ol>"},{"location":"supported-languages/#roadmap","title":"Roadmap","text":"<p>Planned enhancements:</p> <ul> <li>Deeper TypeScript semantic model</li> <li>SQL schema/introspection analyzer</li> <li>Proto / gRPC IDL support</li> <li>Framework-aware weighting (Django, Rails, Spring) optional modules</li> </ul> <p>Got a priority? Open an issue or PR.</p>"},{"location":"api/","title":"API Reference","text":"<p>This reference is generated from Python docstrings using mkdocstrings. It reflects the code in this repository.</p>"},{"location":"api/#package-tenets","title":"Package: tenets","text":""},{"location":"api/#tenets","title":"tenets","text":"<p>Tenets - Context that feeds your prompts.</p> <p>Tenets is a code intelligence platform that analyzes codebases locally to surface relevant files, track development velocity, and build optimal context for both human understanding and AI pair programming - all without making any LLM API calls.</p> <p>This package provides:</p> Example <p>Basic usage for context extraction:</p> <p>from tenets import Tenets ten = Tenets() result = ten.distill(\"implement OAuth2 authentication\") print(result.context)</p> <p>With tenet system:</p> <p>ten.add_tenet(\"Always use type hints in Python\", priority=\"high\") ten.instill_tenets() result = ten.distill(\"add user model\")  # Context now includes tenets</p>"},{"location":"api/#tenets-classes","title":"Classes","text":""},{"location":"api/#tenets.TenetsConfig","title":"TenetsConfig  <code>dataclass</code>","text":"Python<pre><code>TenetsConfig(config_file: Optional[Path] = None, project_root: Optional[Path] = None, max_tokens: int = 100000, version: str = '0.1.0', debug: bool = False, quiet: bool = False, scanner: ScannerConfig = ScannerConfig(), ranking: RankingConfig = RankingConfig(), summarizer: SummarizerConfig = SummarizerConfig(), tenet: TenetConfig = TenetConfig(), cache: CacheConfig = CacheConfig(), output: OutputConfig = OutputConfig(), git: GitConfig = GitConfig(), llm: LLMConfig = LLMConfig(), nlp: NLPConfig = NLPConfig(), custom: Dict[str, Any] = dict())\n</code></pre> <p>Main configuration for the Tenets system with LLM and NLP support.</p> <p>This is the root configuration object that contains all subsystem configs and global settings. It handles loading from files, environment variables, and provides sensible defaults.</p> <p>Attributes:</p> Name Type Description <code>config_file</code> <code>Optional[Path]</code> <p>Path to configuration file (if any)</p> <code>project_root</code> <code>Optional[Path]</code> <p>Root directory of the project</p> <code>max_tokens</code> <code>int</code> <p>Default maximum tokens for context</p> <code>version</code> <code>str</code> <p>Tenets version (for compatibility checking)</p> <code>debug</code> <code>bool</code> <p>Enable debug mode</p> <code>quiet</code> <code>bool</code> <p>Suppress non-essential output</p> <code>scanner</code> <code>ScannerConfig</code> <p>Scanner subsystem configuration</p> <code>ranking</code> <code>RankingConfig</code> <p>Ranking subsystem configuration</p> <code>summarizer</code> <code>SummarizerConfig</code> <p>Summarizer subsystem configuration</p> <code>tenet</code> <code>TenetConfig</code> <p>Tenet subsystem configuration</p> <code>cache</code> <code>CacheConfig</code> <p>Cache subsystem configuration</p> <code>output</code> <code>OutputConfig</code> <p>Output formatting configuration</p> <code>git</code> <code>GitConfig</code> <p>Git integration configuration</p> <code>llm</code> <code>LLMConfig</code> <p>LLM integration configuration</p> <code>nlp</code> <code>NLPConfig</code> <p>NLP system configuration</p> <code>custom</code> <code>Dict[str, Any]</code> <p>Custom user configuration</p>"},{"location":"api/#tenets.TenetsConfig-attributes","title":"Attributes","text":""},{"location":"api/#tenets.TenetsConfig.exclude_minified","title":"exclude_minified  <code>property</code> <code>writable</code>","text":"Python<pre><code>exclude_minified: bool\n</code></pre> <p>Get exclude_minified setting from scanner config.</p>"},{"location":"api/#tenets.TenetsConfig.minified_patterns","title":"minified_patterns  <code>property</code> <code>writable</code>","text":"Python<pre><code>minified_patterns: List[str]\n</code></pre> <p>Get minified patterns from scanner config.</p>"},{"location":"api/#tenets.TenetsConfig.build_directory_patterns","title":"build_directory_patterns  <code>property</code> <code>writable</code>","text":"Python<pre><code>build_directory_patterns: List[str]\n</code></pre> <p>Get build directory patterns from scanner config.</p>"},{"location":"api/#tenets.TenetsConfig.cache_dir","title":"cache_dir  <code>property</code> <code>writable</code>","text":"Python<pre><code>cache_dir: Path\n</code></pre> <p>Get the cache directory path.</p>"},{"location":"api/#tenets.TenetsConfig.scanner_workers","title":"scanner_workers  <code>property</code>","text":"Python<pre><code>scanner_workers: int\n</code></pre> <p>Get number of scanner workers.</p>"},{"location":"api/#tenets.TenetsConfig.ranking_workers","title":"ranking_workers  <code>property</code>","text":"Python<pre><code>ranking_workers: int\n</code></pre> <p>Get number of ranking workers.</p>"},{"location":"api/#tenets.TenetsConfig.ranking_algorithm","title":"ranking_algorithm  <code>property</code>","text":"Python<pre><code>ranking_algorithm: str\n</code></pre> <p>Get the ranking algorithm.</p>"},{"location":"api/#tenets.TenetsConfig.summarizer_mode","title":"summarizer_mode  <code>property</code>","text":"Python<pre><code>summarizer_mode: str\n</code></pre> <p>Get the default summarizer mode.</p>"},{"location":"api/#tenets.TenetsConfig.summarizer_ratio","title":"summarizer_ratio  <code>property</code>","text":"Python<pre><code>summarizer_ratio: float\n</code></pre> <p>Get the default summarization target ratio.</p>"},{"location":"api/#tenets.TenetsConfig.respect_gitignore","title":"respect_gitignore  <code>property</code> <code>writable</code>","text":"Python<pre><code>respect_gitignore: bool\n</code></pre> <p>Whether to respect .gitignore files.</p>"},{"location":"api/#tenets.TenetsConfig.follow_symlinks","title":"follow_symlinks  <code>property</code> <code>writable</code>","text":"Python<pre><code>follow_symlinks: bool\n</code></pre> <p>Whether to follow symbolic links.</p>"},{"location":"api/#tenets.TenetsConfig.additional_ignore_patterns","title":"additional_ignore_patterns  <code>property</code> <code>writable</code>","text":"Python<pre><code>additional_ignore_patterns: List[str]\n</code></pre> <p>Get additional ignore patterns.</p>"},{"location":"api/#tenets.TenetsConfig.auto_instill_tenets","title":"auto_instill_tenets  <code>property</code> <code>writable</code>","text":"Python<pre><code>auto_instill_tenets: bool\n</code></pre> <p>Whether to automatically instill tenets.</p>"},{"location":"api/#tenets.TenetsConfig.max_tenets_per_context","title":"max_tenets_per_context  <code>property</code> <code>writable</code>","text":"Python<pre><code>max_tenets_per_context: int\n</code></pre> <p>Maximum tenets to inject per context.</p>"},{"location":"api/#tenets.TenetsConfig.tenet_injection_config","title":"tenet_injection_config  <code>property</code>","text":"Python<pre><code>tenet_injection_config: Dict[str, Any]\n</code></pre> <p>Get tenet injection configuration.</p>"},{"location":"api/#tenets.TenetsConfig.cache_ttl_days","title":"cache_ttl_days  <code>property</code> <code>writable</code>","text":"Python<pre><code>cache_ttl_days: int\n</code></pre> <p>Cache time-to-live in days.</p>"},{"location":"api/#tenets.TenetsConfig.max_cache_size_mb","title":"max_cache_size_mb  <code>property</code> <code>writable</code>","text":"Python<pre><code>max_cache_size_mb: int\n</code></pre> <p>Maximum cache size in megabytes.</p>"},{"location":"api/#tenets.TenetsConfig.llm_enabled","title":"llm_enabled  <code>property</code> <code>writable</code>","text":"Python<pre><code>llm_enabled: bool\n</code></pre> <p>Whether LLM features are enabled.</p>"},{"location":"api/#tenets.TenetsConfig.llm_provider","title":"llm_provider  <code>property</code> <code>writable</code>","text":"Python<pre><code>llm_provider: str\n</code></pre> <p>Get the current LLM provider.</p>"},{"location":"api/#tenets.TenetsConfig.nlp_enabled","title":"nlp_enabled  <code>property</code> <code>writable</code>","text":"Python<pre><code>nlp_enabled: bool\n</code></pre> <p>Whether NLP features are enabled.</p>"},{"location":"api/#tenets.TenetsConfig.nlp_embeddings_enabled","title":"nlp_embeddings_enabled  <code>property</code> <code>writable</code>","text":"Python<pre><code>nlp_embeddings_enabled: bool\n</code></pre> <p>Whether NLP embeddings are enabled.</p>"},{"location":"api/#tenets.TenetsConfig-functions","title":"Functions","text":""},{"location":"api/#tenets.TenetsConfig.to_dict","title":"to_dict","text":"Python<pre><code>to_dict() -&gt; Dict[str, Any]\n</code></pre> <p>Convert configuration to dictionary.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary representation of configuration</p> Source code in <code>tenets/config.py</code> Python<pre><code>def to_dict(self) -&gt; Dict[str, Any]:\n    \"\"\"Convert configuration to dictionary.\n\n    Returns:\n        Dictionary representation of configuration\n    \"\"\"\n\n    def _as_serializable(obj):\n        if isinstance(obj, Path):\n            return str(obj)\n        if isinstance(obj, dict):\n            return {k: _as_serializable(v) for k, v in obj.items()}\n        if isinstance(obj, list):\n            return [_as_serializable(v) for v in obj]\n        return obj\n\n    data = {\n        \"max_tokens\": self.max_tokens,\n        \"version\": self.version,\n        \"debug\": self.debug,\n        \"quiet\": self.quiet,\n        \"scanner\": asdict(self.scanner),\n        \"ranking\": asdict(self.ranking),\n        \"summarizer\": asdict(self.summarizer),\n        \"tenet\": asdict(self.tenet),\n        \"cache\": asdict(self.cache),\n        \"output\": asdict(self.output),\n        \"git\": asdict(self.git),\n        \"llm\": asdict(self.llm),\n        \"nlp\": asdict(self.nlp),\n        \"custom\": self.custom,\n    }\n    return _as_serializable(data)\n</code></pre>"},{"location":"api/#tenets.TenetsConfig.save","title":"save","text":"Python<pre><code>save(path: Optional[Path] = None)\n</code></pre> <p>Save configuration to file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Optional[Path]</code> <p>Path to save to (uses config_file if not specified)</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no path specified and config_file not set</p> Source code in <code>tenets/config.py</code> Python<pre><code>def save(self, path: Optional[Path] = None):\n    \"\"\"Save configuration to file.\n\n    Args:\n        path: Path to save to (uses config_file if not specified)\n\n    Raises:\n        ValueError: If no path specified and config_file not set\n    \"\"\"\n    # Only allow implicit save to config_file if it was explicitly provided\n    if path is None:\n        if not self.config_file or self._config_file_discovered:\n            raise ValueError(\"No path specified for saving configuration\")\n        save_path = self.config_file\n    else:\n        save_path = path\n\n    save_path = Path(save_path)\n    config_dict = self.to_dict()\n\n    # Remove version from saved config (managed by package)\n    config_dict.pop(\"version\", None)\n\n    with open(save_path, \"w\") as f:\n        if save_path.suffix == \".json\":\n            json.dump(config_dict, f, indent=2)\n        else:\n            _ensure_yaml_imported()  # Import yaml when needed\n            yaml.dump(config_dict, f, default_flow_style=False, sort_keys=False)\n\n    self._logger.info(f\"Configuration saved to {save_path}\")\n</code></pre>"},{"location":"api/#tenets.TenetsConfig.get_llm_api_key","title":"get_llm_api_key","text":"Python<pre><code>get_llm_api_key(provider: Optional[str] = None) -&gt; Optional[str]\n</code></pre> <p>Get LLM API key for a provider.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>Optional[str]</code> <p>Provider name (uses default if not specified)</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>API key or None</p> Source code in <code>tenets/config.py</code> Python<pre><code>def get_llm_api_key(self, provider: Optional[str] = None) -&gt; Optional[str]:\n    \"\"\"Get LLM API key for a provider.\n\n    Args:\n        provider: Provider name (uses default if not specified)\n\n    Returns:\n        API key or None\n    \"\"\"\n    return self.llm.get_api_key(provider)\n</code></pre>"},{"location":"api/#tenets.TenetsConfig.get_llm_model","title":"get_llm_model","text":"Python<pre><code>get_llm_model(task: str = 'default', provider: Optional[str] = None) -&gt; str\n</code></pre> <p>Get LLM model for a specific task.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>str</code> <p>Task type</p> <code>'default'</code> <code>provider</code> <code>Optional[str]</code> <p>Provider name (uses default if not specified)</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Model name</p> Source code in <code>tenets/config.py</code> Python<pre><code>def get_llm_model(self, task: str = \"default\", provider: Optional[str] = None) -&gt; str:\n    \"\"\"Get LLM model for a specific task.\n\n    Args:\n        task: Task type\n        provider: Provider name (uses default if not specified)\n\n    Returns:\n        Model name\n    \"\"\"\n    return self.llm.get_model(task, provider)\n</code></pre>"},{"location":"api/#tenets.CodeAnalyzer","title":"CodeAnalyzer","text":"Python<pre><code>CodeAnalyzer(config: TenetsConfig)\n</code></pre> <p>Main code analysis orchestrator.</p> <p>Coordinates language-specific analyzers and provides a unified interface for analyzing source code files. Handles caching, parallel processing, analyzer selection, and fallback strategies.</p> <p>Attributes:</p> Name Type Description <code>config</code> <p>TenetsConfig instance for configuration</p> <code>logger</code> <p>Logger instance for logging</p> <code>cache</code> <p>AnalysisCache for caching analysis results</p> <code>analyzers</code> <p>Dictionary mapping file extensions to analyzer instances</p> <code>stats</code> <p>Analysis statistics and metrics</p> <p>Initialize the code analyzer.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>TenetsConfig</code> <p>Tenets configuration object</p> required Source code in <code>tenets/core/analysis/analyzer.py</code> Python<pre><code>def __init__(self, config: TenetsConfig):\n    \"\"\"Initialize the code analyzer.\n\n    Args:\n        config: Tenets configuration object\n    \"\"\"\n    self.config = config\n    self.logger = get_logger(__name__)\n\n    # Initialize cache if enabled\n    self.cache = None\n    if config.cache.enabled:\n        self.cache = AnalysisCache(config.cache.directory)\n        self.logger.info(f\"Cache initialized at {config.cache.directory}\")\n\n    # Initialize language analyzers\n    self.analyzers = self._initialize_analyzers()\n\n    # Thread pool for parallel analysis\n    self._executor = concurrent.futures.ThreadPoolExecutor(max_workers=config.scanner.workers)\n\n    # Analysis statistics\n    self.stats = {\n        \"files_analyzed\": 0,\n        \"cache_hits\": 0,\n        \"cache_misses\": 0,\n        \"errors\": 0,\n        \"total_time\": 0,\n        \"languages\": {},\n    }\n\n    self.logger.info(f\"CodeAnalyzer initialized with {len(self.analyzers)} language analyzers\")\n</code></pre>"},{"location":"api/#tenets.CodeAnalyzer-functions","title":"Functions","text":""},{"location":"api/#tenets.CodeAnalyzer.analyze_file","title":"analyze_file","text":"Python<pre><code>analyze_file(file_path: Path, deep: bool = False, extract_keywords: bool = True, use_cache: bool = True, progress_callback: Optional[Callable] = None) -&gt; FileAnalysis\n</code></pre> <p>Analyze a single file.</p> <p>Performs language-specific analysis on a file, extracting imports, structure, complexity metrics, and other relevant information.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>Path to the file to analyze</p> required <code>deep</code> <code>bool</code> <p>Whether to perform deep analysis (AST parsing, etc.)</p> <code>False</code> <code>extract_keywords</code> <code>bool</code> <p>Whether to extract keywords from content</p> <code>True</code> <code>use_cache</code> <code>bool</code> <p>Whether to use cached results if available</p> <code>True</code> <code>progress_callback</code> <code>Optional[Callable]</code> <p>Optional callback for progress updates</p> <code>None</code> <p>Returns:</p> Type Description <code>FileAnalysis</code> <p>FileAnalysis object with complete analysis results</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If file doesn't exist</p> <code>PermissionError</code> <p>If file cannot be read</p> Source code in <code>tenets/core/analysis/analyzer.py</code> Python<pre><code>def analyze_file(\n    self,\n    file_path: Path,\n    deep: bool = False,\n    extract_keywords: bool = True,\n    use_cache: bool = True,\n    progress_callback: Optional[Callable] = None,\n) -&gt; FileAnalysis:\n    \"\"\"Analyze a single file.\n\n    Performs language-specific analysis on a file, extracting imports,\n    structure, complexity metrics, and other relevant information.\n\n    Args:\n        file_path: Path to the file to analyze\n        deep: Whether to perform deep analysis (AST parsing, etc.)\n        extract_keywords: Whether to extract keywords from content\n        use_cache: Whether to use cached results if available\n        progress_callback: Optional callback for progress updates\n\n    Returns:\n        FileAnalysis object with complete analysis results\n\n    Raises:\n        FileNotFoundError: If file doesn't exist\n        PermissionError: If file cannot be read\n    \"\"\"\n    file_path = Path(file_path)\n\n    # Check cache first\n    if use_cache and self.cache:\n        cached_analysis = self.cache.get_file_analysis(file_path)\n        if cached_analysis:\n            self.stats[\"cache_hits\"] += 1\n            self.logger.debug(f\"Cache hit for {file_path}\")\n\n            if progress_callback:\n                progress_callback(\"cache_hit\", file_path)\n\n            return cached_analysis\n        else:\n            self.stats[\"cache_misses\"] += 1\n\n    self.logger.debug(f\"Analyzing file: {file_path}\")\n\n    try:\n        # Read file content\n        content = self._read_file_content(file_path)\n\n        # Create base analysis\n        analysis = FileAnalysis(\n            path=str(file_path),\n            content=content,\n            size=file_path.stat().st_size,\n            lines=content.count(\"\\n\") + 1,\n            language=self._detect_language(file_path),\n            file_name=file_path.name,\n            file_extension=file_path.suffix,\n            last_modified=datetime.fromtimestamp(file_path.stat().st_mtime),\n            hash=self._calculate_file_hash(content),\n        )\n\n        # Get appropriate analyzer\n        analyzer = self._get_analyzer(file_path)\n\n        if analyzer is None and deep:\n            analyzer = GenericAnalyzer()\n\n        if analyzer and deep:\n            try:\n                # Run language-specific analysis\n                self.logger.debug(f\"Running {analyzer.language_name} analyzer on {file_path}\")\n                analysis_results = analyzer.analyze(content, file_path)\n\n                # Update analysis object with results\n                # Collect results\n                imports = analysis_results.get(\"imports\", [])\n                analysis.imports = imports\n                analysis.exports = analysis_results.get(\"exports\", [])\n                structure = analysis_results.get(\"structure\", CodeStructure())\n                # Ensure imports are accessible via structure as well for downstream tools\n                try:\n                    if hasattr(structure, \"imports\"):\n                        # Only set if empty to respect analyzers that already populate it\n                        if not getattr(structure, \"imports\", None):\n                            structure.imports = imports\n                except Exception:\n                    # Be defensive; never fail analysis due to structure syncing\n                    pass\n                analysis.structure = structure\n                analysis.complexity = analysis_results.get(\"complexity\", ComplexityMetrics())\n\n                # Extract additional information\n                if analysis.structure:\n                    analysis.classes = analysis.structure.classes\n                    analysis.functions = analysis.structure.functions\n                    analysis.modules = getattr(analysis.structure, \"modules\", [])\n\n            except Exception as e:\n                self.logger.warning(f\"Language-specific analysis failed for {file_path}: {e}\")\n                analysis.error = str(e)\n                self.stats[\"errors\"] += 1\n\n        # Extract keywords if requested\n        if extract_keywords:\n            analysis.keywords = self._extract_keywords(content, analysis.language)\n\n        # Add code quality metrics\n        analysis.quality_score = self._calculate_quality_score(analysis)\n\n        # Cache the result\n        if use_cache and self.cache and not analysis.error:\n            try:\n                self.cache.put_file_analysis(file_path, analysis)\n            except Exception as e:\n                self.logger.debug(f\"Failed to write analysis cache for {file_path}: {e}\")\n                analysis.error = \"Cache write error\"\n\n        # Update statistics\n        self.stats[\"files_analyzed\"] += 1\n        self.stats[\"languages\"][analysis.language] = (\n            self.stats[\"languages\"].get(analysis.language, 0) + 1\n        )\n\n        if progress_callback:\n            progress_callback(\"analyzed\", file_path)\n\n        return analysis\n\n    except FileNotFoundError:\n        # Propagate not found to satisfy tests expecting exception\n        self.logger.error(f\"File not found: {file_path}\")\n        raise\n    except Exception as e:\n        self.logger.error(f\"Failed to analyze {file_path}: {e}\")\n        self.stats[\"errors\"] += 1\n\n        return FileAnalysis(\n            path=str(file_path),\n            error=str(e),\n            file_name=file_path.name,\n            file_extension=file_path.suffix,\n        )\n</code></pre>"},{"location":"api/#tenets.CodeAnalyzer.analyze_files","title":"analyze_files","text":"Python<pre><code>analyze_files(file_paths: list[Path], deep: bool = False, parallel: bool = True, progress_callback: Optional[Callable] = None) -&gt; list[FileAnalysis]\n</code></pre> <p>Analyze multiple files.</p> <p>Parameters:</p> Name Type Description Default <code>file_paths</code> <code>list[Path]</code> <p>List of file paths to analyze</p> required <code>deep</code> <code>bool</code> <p>Whether to perform deep analysis</p> <code>False</code> <code>parallel</code> <code>bool</code> <p>Whether to analyze files in parallel</p> <code>True</code> <code>progress_callback</code> <code>Optional[Callable]</code> <p>Optional callback for progress updates</p> <code>None</code> <p>Returns:</p> Type Description <code>list[FileAnalysis]</code> <p>List of FileAnalysis objects</p> Source code in <code>tenets/core/analysis/analyzer.py</code> Python<pre><code>def analyze_files(\n    self,\n    file_paths: list[Path],\n    deep: bool = False,\n    parallel: bool = True,\n    progress_callback: Optional[Callable] = None,\n) -&gt; list[FileAnalysis]:\n    \"\"\"Analyze multiple files.\n\n    Args:\n        file_paths: List of file paths to analyze\n        deep: Whether to perform deep analysis\n        parallel: Whether to analyze files in parallel\n        progress_callback: Optional callback for progress updates\n\n    Returns:\n        List of FileAnalysis objects\n    \"\"\"\n    self.logger.info(f\"Analyzing {len(file_paths)} files (parallel={parallel})\")\n\n    if parallel and len(file_paths) &gt; 1:\n        # Parallel analysis\n        futures = []\n        for file_path in file_paths:\n            future = self._executor.submit(\n                self.analyze_file, file_path, deep=deep, progress_callback=progress_callback\n            )\n            futures.append((future, file_path))\n\n        # Collect results\n        results = []\n        for future, file_path in futures:\n            try:\n                result = future.result(timeout=self.config.scanner.timeout)\n                results.append(result)\n            except concurrent.futures.TimeoutError:\n                self.logger.warning(f\"Analysis timeout for {file_path}\")\n                results.append(FileAnalysis(path=str(file_path), error=\"Analysis timeout\"))\n            except Exception as e:\n                self.logger.warning(f\"Failed to analyze {file_path}: {e}\")\n                results.append(FileAnalysis(path=str(file_path), error=str(e)))\n\n        return results\n    else:\n        # Sequential analysis\n        results = []\n        for i, file_path in enumerate(file_paths):\n            result = self.analyze_file(file_path, deep=deep)\n            results.append(result)\n\n            if progress_callback:\n                progress_callback(i + 1, len(file_paths))\n\n        return results\n</code></pre>"},{"location":"api/#tenets.CodeAnalyzer.analyze_project","title":"analyze_project","text":"Python<pre><code>analyze_project(project_path: Path, patterns: Optional[list[str]] = None, exclude_patterns: Optional[list[str]] = None, deep: bool = True, parallel: bool = True, progress_callback: Optional[Callable] = None) -&gt; ProjectAnalysis\n</code></pre> <p>Analyze an entire project.</p> <p>Parameters:</p> Name Type Description Default <code>project_path</code> <code>Path</code> <p>Path to the project root</p> required <code>patterns</code> <code>Optional[list[str]]</code> <p>File patterns to include (e.g., ['.py', '.js'])</p> <code>None</code> <code>exclude_patterns</code> <code>Optional[list[str]]</code> <p>File patterns to exclude</p> <code>None</code> <code>deep</code> <code>bool</code> <p>Whether to perform deep analysis</p> <code>True</code> <code>parallel</code> <code>bool</code> <p>Whether to analyze files in parallel</p> <code>True</code> <code>progress_callback</code> <code>Optional[Callable]</code> <p>Optional callback for progress updates</p> <code>None</code> <p>Returns:</p> Type Description <code>ProjectAnalysis</code> <p>ProjectAnalysis object with complete project analysis</p> Source code in <code>tenets/core/analysis/analyzer.py</code> Python<pre><code>def analyze_project(\n    self,\n    project_path: Path,\n    patterns: Optional[list[str]] = None,\n    exclude_patterns: Optional[list[str]] = None,\n    deep: bool = True,\n    parallel: bool = True,\n    progress_callback: Optional[Callable] = None,\n) -&gt; ProjectAnalysis:\n    \"\"\"Analyze an entire project.\n\n    Args:\n        project_path: Path to the project root\n        patterns: File patterns to include (e.g., ['*.py', '*.js'])\n        exclude_patterns: File patterns to exclude\n        deep: Whether to perform deep analysis\n        parallel: Whether to analyze files in parallel\n        progress_callback: Optional callback for progress updates\n\n    Returns:\n        ProjectAnalysis object with complete project analysis\n    \"\"\"\n    self.logger.info(f\"Analyzing project: {project_path}\")\n\n    # Collect files to analyze\n    files = self._collect_project_files(project_path, patterns, exclude_patterns)\n\n    self.logger.info(f\"Found {len(files)} files to analyze\")\n\n    # Analyze all files\n    file_analyses = self.analyze_files(\n        files, deep=deep, parallel=parallel, progress_callback=progress_callback\n    )\n\n    # Build project analysis\n    project_analysis = ProjectAnalysis(\n        path=str(project_path),\n        name=project_path.name,\n        files=file_analyses,\n        total_files=len(file_analyses),\n        analyzed_files=len([f for f in file_analyses if not f.error]),\n        failed_files=len([f for f in file_analyses if f.error]),\n    )\n\n    # Calculate project-level metrics\n    self._calculate_project_metrics(project_analysis)\n\n    # Build dependency graph\n    project_analysis.dependency_graph = self._build_dependency_graph(file_analyses)\n\n    # Detect project type and framework\n    project_analysis.project_type = self._detect_project_type(project_path, file_analyses)\n    project_analysis.frameworks = self._detect_frameworks(file_analyses)\n\n    # Generate summary\n    project_analysis.summary = self._generate_project_summary(project_analysis)\n\n    return project_analysis\n</code></pre>"},{"location":"api/#tenets.CodeAnalyzer.generate_report","title":"generate_report","text":"Python<pre><code>generate_report(analysis: Union[FileAnalysis, ProjectAnalysis, list[FileAnalysis]], format: str = 'json', output_path: Optional[Path] = None) -&gt; AnalysisReport\n</code></pre> <p>Generate an analysis report.</p> <p>Parameters:</p> Name Type Description Default <code>analysis</code> <code>Union[FileAnalysis, ProjectAnalysis, list[FileAnalysis]]</code> <p>Analysis results to report on</p> required <code>format</code> <code>str</code> <p>Report format ('json', 'html', 'markdown', 'csv')</p> <code>'json'</code> <code>output_path</code> <code>Optional[Path]</code> <p>Optional path to save the report</p> <code>None</code> <p>Returns:</p> Type Description <code>AnalysisReport</code> <p>AnalysisReport object</p> Source code in <code>tenets/core/analysis/analyzer.py</code> Python<pre><code>def generate_report(\n    self,\n    analysis: Union[FileAnalysis, ProjectAnalysis, list[FileAnalysis]],\n    format: str = \"json\",\n    output_path: Optional[Path] = None,\n) -&gt; AnalysisReport:\n    \"\"\"Generate an analysis report.\n\n    Args:\n        analysis: Analysis results to report on\n        format: Report format ('json', 'html', 'markdown', 'csv')\n        output_path: Optional path to save the report\n\n    Returns:\n        AnalysisReport object\n    \"\"\"\n    self.logger.info(f\"Generating {format} report\")\n\n    report = AnalysisReport(\n        timestamp=datetime.now(), format=format, statistics=self.stats.copy()\n    )\n\n    # Generate report content based on format\n    if format == \"json\":\n        report.content = self._generate_json_report(analysis)\n    elif format == \"html\":\n        report.content = self._generate_html_report(analysis)\n    elif format == \"markdown\":\n        report.content = self._generate_markdown_report(analysis)\n    elif format == \"csv\":\n        report.content = self._generate_csv_report(analysis)\n    else:\n        raise ValueError(f\"Unsupported report format: {format}\")\n\n    # Save report if output path provided\n    if output_path:\n        output_path = Path(output_path)\n        output_path.parent.mkdir(parents=True, exist_ok=True)\n\n        if format in [\"json\", \"csv\"]:\n            output_path.write_text(report.content)\n        else:\n            output_path.write_text(report.content, encoding=\"utf-8\")\n\n        self.logger.info(f\"Report saved to {output_path}\")\n        report.output_path = str(output_path)\n\n    return report\n</code></pre>"},{"location":"api/#tenets.CodeAnalyzer.shutdown","title":"shutdown","text":"Python<pre><code>shutdown()\n</code></pre> <p>Shutdown the analyzer and clean up resources.</p> Source code in <code>tenets/core/analysis/analyzer.py</code> Python<pre><code>def shutdown(self):\n    \"\"\"Shutdown the analyzer and clean up resources.\"\"\"\n    self._executor.shutdown(wait=True)\n\n    if self.cache:\n        self.cache.close()\n\n    self.logger.info(\"CodeAnalyzer shutdown complete\")\n    self.logger.info(f\"Analysis statistics: {self.stats}\")\n</code></pre>"},{"location":"api/#tenets.Distiller","title":"Distiller","text":"Python<pre><code>Distiller(config: TenetsConfig)\n</code></pre> <p>Orchestrates context extraction from codebases.</p> <p>The Distiller is the main engine that powers the 'distill' command. It coordinates all the components to extract the most relevant context based on a user's prompt.</p> <p>Initialize the distiller with configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>TenetsConfig</code> <p>Tenets configuration</p> required Source code in <code>tenets/core/distiller/distiller.py</code> Python<pre><code>def __init__(self, config: TenetsConfig):\n    \"\"\"Initialize the distiller with configuration.\n\n    Args:\n        config: Tenets configuration\n    \"\"\"\n    self.config = config\n    self.logger = get_logger(__name__)\n\n    # Log multiprocessing configuration\n    import os\n\n    from tenets.utils.multiprocessing import get_ranking_workers, get_scanner_workers\n\n    cpu_count = os.cpu_count() or 1\n    scanner_workers = get_scanner_workers(config)\n    ranking_workers = get_ranking_workers(config)\n    self.logger.info(\n        f\"Distiller initialized (CPU cores: {cpu_count}, \"\n        f\"scanner workers: {scanner_workers}, \"\n        f\"ranking workers: {ranking_workers}, \"\n        f\"ML enabled: {config.ranking.use_ml})\"\n    )\n\n    # Initialize components\n    self.scanner = FileScanner(config)\n    self.analyzer = CodeAnalyzer(config)\n    self.ranker = RelevanceRanker(config)\n    self.parser = PromptParser(config)\n    self.git = GitAnalyzer(config)\n    self.aggregator = ContextAggregator(config)\n    self.optimizer = TokenOptimizer(config)\n    self.formatter = ContextFormatter(config)\n</code></pre>"},{"location":"api/#tenets.Distiller-functions","title":"Functions","text":""},{"location":"api/#tenets.Distiller.distill","title":"distill","text":"Python<pre><code>distill(prompt: str, paths: Optional[Union[str, Path, List[Path]]] = None, *, format: str = 'markdown', model: Optional[str] = None, max_tokens: Optional[int] = None, mode: str = 'balanced', include_git: bool = True, session_name: Optional[str] = None, include_patterns: Optional[List[str]] = None, exclude_patterns: Optional[List[str]] = None, full: bool = False, condense: bool = False, remove_comments: bool = False, pinned_files: Optional[List[Path]] = None, include_tests: Optional[bool] = None, docstring_weight: Optional[float] = None, summarize_imports: bool = True) -&gt; ContextResult\n</code></pre> <p>Distill relevant context from codebase based on prompt.</p> <p>This is the main method that extracts, ranks, and aggregates the most relevant files and information for a given prompt.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The user's query or task description</p> required <code>paths</code> <code>Optional[Union[str, Path, List[Path]]]</code> <p>Paths to analyze (default: current directory)</p> <code>None</code> <code>format</code> <code>str</code> <p>Output format (markdown, xml, json)</p> <code>'markdown'</code> <code>model</code> <code>Optional[str]</code> <p>Target LLM model for token counting</p> <code>None</code> <code>max_tokens</code> <code>Optional[int]</code> <p>Maximum tokens for context</p> <code>None</code> <code>mode</code> <code>str</code> <p>Analysis mode (fast, balanced, thorough)</p> <code>'balanced'</code> <code>include_git</code> <code>bool</code> <p>Whether to include git context</p> <code>True</code> <code>session_name</code> <code>Optional[str]</code> <p>Session name for stateful context</p> <code>None</code> <code>include_patterns</code> <code>Optional[List[str]]</code> <p>File patterns to include</p> <code>None</code> <code>exclude_patterns</code> <code>Optional[List[str]]</code> <p>File patterns to exclude</p> <code>None</code> <p>Returns:</p> Type Description <code>ContextResult</code> <p>ContextResult with the distilled context</p> Example <p>distiller = Distiller(config) result = distiller.distill( ...     \"implement OAuth2 authentication\", ...     paths=\"./src\", ...     mode=\"thorough\", ...     max_tokens=50000 ... ) print(result.context)</p> Source code in <code>tenets/core/distiller/distiller.py</code> Python<pre><code>def distill(\n    self,\n    prompt: str,\n    paths: Optional[Union[str, Path, List[Path]]] = None,\n    *,  # Force keyword-only arguments for clarity\n    format: str = \"markdown\",\n    model: Optional[str] = None,\n    max_tokens: Optional[int] = None,\n    mode: str = \"balanced\",\n    include_git: bool = True,\n    session_name: Optional[str] = None,\n    include_patterns: Optional[List[str]] = None,\n    exclude_patterns: Optional[List[str]] = None,\n    full: bool = False,\n    condense: bool = False,\n    remove_comments: bool = False,\n    pinned_files: Optional[List[Path]] = None,\n    include_tests: Optional[bool] = None,\n    docstring_weight: Optional[float] = None,\n    summarize_imports: bool = True,\n) -&gt; ContextResult:\n    \"\"\"Distill relevant context from codebase based on prompt.\n\n    This is the main method that extracts, ranks, and aggregates\n    the most relevant files and information for a given prompt.\n\n    Args:\n        prompt: The user's query or task description\n        paths: Paths to analyze (default: current directory)\n        format: Output format (markdown, xml, json)\n        model: Target LLM model for token counting\n        max_tokens: Maximum tokens for context\n        mode: Analysis mode (fast, balanced, thorough)\n        include_git: Whether to include git context\n        session_name: Session name for stateful context\n        include_patterns: File patterns to include\n        exclude_patterns: File patterns to exclude\n\n    Returns:\n        ContextResult with the distilled context\n\n    Example:\n        &gt;&gt;&gt; distiller = Distiller(config)\n        &gt;&gt;&gt; result = distiller.distill(\n        ...     \"implement OAuth2 authentication\",\n        ...     paths=\"./src\",\n        ...     mode=\"thorough\",\n        ...     max_tokens=50000\n        ... )\n        &gt;&gt;&gt; print(result.context)\n    \"\"\"\n    import time\n\n    start_time = time.time()\n    self.logger.info(f\"Distilling context for: {prompt[:100]}...\")\n\n    # 1. Parse and understand the prompt\n    parse_start = time.time()\n    prompt_context = self._parse_prompt(prompt)\n    self.logger.debug(f\"Prompt parsing took {time.time() - parse_start:.2f}s\")\n\n    # Override test inclusion if explicitly specified\n    if include_tests is not None:\n        prompt_context.include_tests = include_tests\n        self.logger.debug(f\"Override: test inclusion set to {include_tests}\")\n\n    # 2. Determine paths to analyze\n    paths = self._normalize_paths(paths)\n\n    # 3. Discover relevant files\n    discover_start = time.time()\n    files = self._discover_files(\n        paths=paths,\n        prompt_context=prompt_context,\n        include_patterns=include_patterns,\n        exclude_patterns=exclude_patterns,\n    )\n    self.logger.debug(f\"File discovery took {time.time() - discover_start:.2f}s\")\n\n    # 4. Analyze files for structure and content\n    # Prepend pinned files (avoid duplicates) while preserving original discovery order\n    if pinned_files:\n        # Preserve the explicit order given by the caller (tests rely on this)\n        # Do NOT filter by existence \u2013 tests pass synthetic Paths.\n        pinned_strs = [str(p) for p in pinned_files]\n        pinned_set = set(pinned_strs)\n        ordered: List[Path] = []\n        # First, add pinned files (re-using the discovered Path object if present\n        # so downstream identity / patch assertions still work).\n        discovered_map = {str(f): f for f in files}\n        for p_str, p_obj in zip(pinned_strs, pinned_files):\n            if p_str in discovered_map:\n                f = discovered_map[p_str]\n            else:\n                f = p_obj  # fallback to provided Path\n            if f not in ordered:\n                ordered.append(f)\n        # Then append remaining discovered files preserving original discovery order.\n        for f in files:\n            if str(f) not in pinned_set and f not in ordered:\n                ordered.append(f)\n        files = ordered\n\n    analyzed_files = self._analyze_files(files=files, mode=mode, prompt_context=prompt_context)\n\n    # 5. Rank files by relevance\n    rank_start = time.time()\n    ranked_files = self._rank_files(\n        files=analyzed_files, prompt_context=prompt_context, mode=mode\n    )\n    self.logger.debug(f\"File ranking took {time.time() - rank_start:.2f}s\")\n\n    # 6. Add git context if requested\n    git_context = None\n    if include_git:\n        git_context = self._get_git_context(\n            paths=paths, prompt_context=prompt_context, files=ranked_files\n        )\n\n    # 7. Aggregate files within token budget\n    aggregate_start = time.time()\n    aggregated = self._aggregate_files(\n        files=ranked_files,\n        prompt_context=prompt_context,\n        max_tokens=max_tokens or self.config.max_tokens,\n        model=model,\n        git_context=git_context,\n        full=full,\n        condense=condense,\n        remove_comments=remove_comments,\n        docstring_weight=docstring_weight,\n        summarize_imports=summarize_imports,\n    )\n    self.logger.debug(f\"File aggregation took {time.time() - aggregate_start:.2f}s\")\n\n    # 8. Format the output\n    formatted = self._format_output(\n        aggregated=aggregated,\n        format=format,\n        prompt_context=prompt_context,\n        session_name=session_name,\n    )\n\n    # 9. Build final result with debug information\n    metadata = {\n        \"mode\": mode,\n        \"files_analyzed\": len(files),\n        \"files_included\": len(aggregated[\"included_files\"]),\n        \"model\": model,\n        \"session\": session_name,\n        \"prompt\": prompt,\n        \"full_mode\": full,\n        \"condense\": condense,\n        \"remove_comments\": remove_comments,\n        # Include the aggregated data for _build_result to use\n        \"included_files\": aggregated[\"included_files\"],\n        \"total_tokens\": aggregated.get(\"total_tokens\", 0),\n    }\n\n    # Add debug information for verbose mode\n    # Add prompt parsing details\n    metadata[\"prompt_context\"] = {\n        \"task_type\": prompt_context.task_type,\n        \"intent\": prompt_context.intent,\n        \"keywords\": prompt_context.keywords,\n        \"synonyms\": getattr(prompt_context, \"synonyms\", []),\n        \"entities\": prompt_context.entities,\n    }\n\n    # Expose NLP normalization metrics if available from parser\n    try:\n        if (\n            isinstance(prompt_context.metadata, dict)\n            and \"nlp_normalization\" in prompt_context.metadata\n        ):\n            metadata[\"nlp_normalization\"] = prompt_context.metadata[\"nlp_normalization\"]\n    except Exception:\n        pass\n\n    # Add ranking details\n    metadata[\"ranking_details\"] = {\n        \"algorithm\": mode,\n        \"threshold\": self.config.ranking.threshold,\n        \"files_ranked\": len(analyzed_files),\n        \"files_above_threshold\": len(ranked_files),\n        \"top_files\": [\n            {\n                \"path\": str(f.path),\n                \"score\": f.relevance_score,\n                \"match_details\": {\n                    \"keywords_matched\": getattr(f, \"keywords_matched\", []),\n                    \"semantic_score\": getattr(f, \"semantic_score\", 0),\n                },\n            }\n            for f in ranked_files[:10]  # Top 10 files\n        ],\n    }\n\n    # Add aggregation details\n    metadata[\"aggregation_details\"] = {\n        \"strategy\": aggregated.get(\"strategy\", \"unknown\"),\n        \"min_relevance\": aggregated.get(\"min_relevance\", 0),\n        \"files_considered\": len(ranked_files),\n        \"files_rejected\": len(ranked_files) - len(aggregated[\"included_files\"]),\n        \"rejection_reasons\": aggregated.get(\"rejection_reasons\", {}),\n    }\n\n    return self._build_result(\n        formatted=formatted,\n        metadata=metadata,\n    )\n</code></pre>"},{"location":"api/#tenets.Instiller","title":"Instiller","text":"Python<pre><code>Instiller(config: TenetsConfig)\n</code></pre> <p>Main orchestrator for tenet instillation with smart injection.</p> <p>The Instiller manages the entire process of injecting tenets into context, including: - Tracking injection history per session - Analyzing context complexity - Determining optimal injection frequency - Selecting appropriate tenets - Applying injection strategies - Recording metrics and effectiveness</p> <p>It supports multiple injection modes: - Always: Inject into every context - Periodic: Inject every Nth distillation - Adaptive: Smart injection based on complexity and session - Manual: Only inject when explicitly requested</p> <p>Initialize the Instiller.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>TenetsConfig</code> <p>Configuration object</p> required Source code in <code>tenets/core/instiller/instiller.py</code> Python<pre><code>def __init__(self, config: TenetsConfig):\n    \"\"\"Initialize the Instiller.\n\n    Args:\n        config: Configuration object\n    \"\"\"\n    self.config = config\n    self.logger = get_logger(__name__)\n\n    # Core components\n    self.manager = TenetManager(config)\n    self.injector = TenetInjector(config.tenet.injection_config)\n    self.complexity_analyzer = ComplexityAnalyzer(config)\n    self.metrics_tracker = MetricsTracker()\n\n    # Session tracking\n    self.session_histories: Dict[str, InjectionHistory] = {}\n\n    # Load histories only when cache is enabled to avoid test cross-contamination\n    try:\n        if getattr(self.config.cache, \"enabled\", False):\n            self._load_session_histories()\n    except Exception:\n        pass\n    # Track which sessions had system instruction injected (tests expect this map)\n    self.system_instruction_injected: Dict[str, bool] = {}\n    # Do NOT seed from persisted histories: once-per-session should apply\n    # only within the lifetime of this Instiller instance. Persisted\n    # histories are still maintained for analytics but must not block\n    # first injection in fresh instances (tests rely on this behavior).\n\n    self._load_session_histories()\n\n    # Cache for results\n    self._cache: Dict[str, InstillationResult] = {}\n\n    self.logger.info(\"Instiller initialized with smart injection capabilities\")\n</code></pre>"},{"location":"api/#tenets.Instiller-functions","title":"Functions","text":""},{"location":"api/#tenets.Instiller.inject_system_instruction","title":"inject_system_instruction","text":"Python<pre><code>inject_system_instruction(content: str, format: str = 'markdown', session: Optional[str] = None) -&gt; Tuple[str, Dict[str, Any]]\n</code></pre> <p>Inject system instruction (system prompt) according to config.</p> <p>Behavior: - If system instruction is disabled or empty, return unchanged. - If session provided and once-per-session is enabled, inject only on first distill. - If no session, inject on every distill. - Placement controlled by system_instruction_position. - Formatting controlled by system_instruction_format.</p> <p>Returns modified content and metadata about injection.</p> Source code in <code>tenets/core/instiller/instiller.py</code> Python<pre><code>def inject_system_instruction(\n    self,\n    content: str,\n    format: str = \"markdown\",\n    session: Optional[str] = None,\n) -&gt; Tuple[str, Dict[str, Any]]:\n    \"\"\"Inject system instruction (system prompt) according to config.\n\n    Behavior:\n    - If system instruction is disabled or empty, return unchanged.\n    - If session provided and once-per-session is enabled, inject only on first distill.\n    - If no session, inject on every distill.\n    - Placement controlled by system_instruction_position.\n    - Formatting controlled by system_instruction_format.\n\n    Returns modified content and metadata about injection.\n    \"\"\"\n    cfg = self.config.tenet\n    meta: Dict[str, Any] = {\n        \"system_instruction_enabled\": cfg.system_instruction_enabled,\n        \"system_instruction_injected\": False,\n    }\n\n    if not cfg.system_instruction_enabled or not cfg.system_instruction:\n        meta[\"reason\"] = \"disabled_or_empty\"\n        return content, meta\n\n    # Session-aware check: only once per session\n    if session and getattr(cfg, \"system_instruction_once_per_session\", False):\n        # Respect once-per-session within this instance and, when allowed,\n        # across instances via persisted history.\n        already = self.system_instruction_injected.get(session, False)\n        # Only consult persisted histories when policy allows it\n        if not already and self._should_respect_persisted_once_per_session():\n            hist = self.session_histories.get(session)\n            already = bool(hist and getattr(hist, \"system_instruction_injected\", False))\n        if already:\n            meta[\"reason\"] = \"already_injected_in_session\"\n            return content, meta\n\n    # Mark as injecting now that we've passed guards\n    meta[\"system_instruction_injected\"] = True\n\n    instruction = cfg.system_instruction\n    formatted_instr = self._format_system_instruction(\n        instruction, cfg.system_instruction_format\n    )\n\n    # Optional label and separator\n    label = getattr(cfg, \"system_instruction_label\", None) or \"\ud83c\udfaf System Context\"\n    separator = getattr(cfg, \"system_instruction_separator\", \"\\n---\\n\\n\")\n\n    # Build final block per format\n    if cfg.system_instruction_format == \"markdown\":\n        formatted_block = f\"## {label}\\n\\n{instruction.strip()}\"\n    elif cfg.system_instruction_format == \"plain\":\n        formatted_block = f\"{label}\\n\\n{instruction.strip()}\"\n    elif cfg.system_instruction_format == \"comment\":\n        # For injected content, wrap as HTML comment so it embeds safely in text\n        formatted_block = f\"&lt;!-- {instruction.strip()} --&gt;\"\n    elif cfg.system_instruction_format == \"xml\":\n        # Integration tests expect hyphenated tag name here\n        formatted_block = f\"&lt;system-instruction&gt;{instruction.strip()}&lt;/system-instruction&gt;\"\n    else:\n        # xml or comment, rely on formatter\n        formatted_block = formatted_instr\n\n    # Determine position\n    if cfg.system_instruction_position == \"top\":\n        modified = formatted_block + separator + content\n        position = \"top\"\n    elif cfg.system_instruction_position == \"after_header\":\n        # After first markdown header or beginning if not found\n        try:\n            import re\n\n            # Match first Markdown header line\n            header_match = re.search(r\"^#+\\s+.*$\", content, flags=re.MULTILINE)\n        except Exception:\n            header_match = None\n        if header_match:\n            idx = header_match.end()\n            modified = content[:idx] + \"\\n\\n\" + formatted_block + content[idx:]\n            position = \"after_header\"\n        else:\n            modified = formatted_block + separator + content\n            position = \"top_fallback\"\n    elif cfg.system_instruction_position == \"before_content\":\n        # Before first non-empty line\n        lines = content.splitlines()\n        i = 0\n        while i &lt; len(lines) and not lines[i].strip():\n            i += 1\n        prefix = \"\\n\".join(lines[:i])\n        suffix = \"\\n\".join(lines[i:])\n\n        between = \"\\n\" if suffix else \"\"\n\n        modified = (\n            prefix\n            + (\"\\n\" if prefix else \"\")\n            + formatted_instr\n            + (\"\\n\" if suffix else \"\")\n            + suffix\n        )\n        position = \"before_content\"\n    else:\n        modified = formatted_block + separator + content\n        position = \"top_default\"\n\n    # Compute token increase (original first, then modified) so patched mocks match\n    orig_tokens = estimate_tokens(content)\n\n    meta.update(\n        {\n            \"system_instruction_position\": position,\n            \"token_increase\": estimate_tokens(modified) - orig_tokens,\n        }\n    )\n\n    # Persist info in metadata when enabled\n    if getattr(cfg, \"system_instruction_persist_in_context\", False):\n        meta[\"system_instruction_persisted\"] = True\n        meta[\"system_instruction_content\"] = instruction\n\n    # Mark as injected for this session and persist history if applicable\n    if session:\n        # Mark in the session map immediately (tests assert this)\n        self.system_instruction_injected[session] = True\n        # Also update history record if present\n        if session not in self.session_histories:\n            self.session_histories[session] = InjectionHistory(session_id=session)\n        hist = self.session_histories[session]\n        hist.system_instruction_injected = True\n        hist.updated_at = datetime.now()\n        # Best-effort save\n        try:\n            self._save_session_histories()\n        except Exception:\n            pass\n    return modified, meta\n</code></pre>"},{"location":"api/#tenets.Instiller.instill","title":"instill","text":"Python<pre><code>instill(context: Union[str, ContextResult], session: Optional[str] = None, force: bool = False, strategy: Optional[str] = None, max_tenets: Optional[int] = None, check_frequency: bool = True, inject_system_instruction: Optional[bool] = None) -&gt; Union[str, ContextResult]\n</code></pre> <p>Instill tenets into context with smart injection.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>Union[str, ContextResult]</code> <p>Context to inject tenets into</p> required <code>session</code> <code>Optional[str]</code> <p>Session identifier for tracking</p> <code>None</code> <code>force</code> <code>bool</code> <p>Force injection regardless of frequency settings</p> <code>False</code> <code>strategy</code> <code>Optional[str]</code> <p>Override injection strategy</p> <code>None</code> <code>max_tenets</code> <code>Optional[int]</code> <p>Override maximum tenets</p> <code>None</code> <code>check_frequency</code> <code>bool</code> <p>Whether to check injection frequency</p> <code>True</code> <p>Returns:</p> Type Description <code>Union[str, ContextResult]</code> <p>Modified context with tenets injected (if applicable)</p> Source code in <code>tenets/core/instiller/instiller.py</code> Python<pre><code>def instill(\n    self,\n    context: Union[str, ContextResult],\n    session: Optional[str] = None,\n    force: bool = False,\n    strategy: Optional[str] = None,\n    max_tenets: Optional[int] = None,\n    check_frequency: bool = True,\n    inject_system_instruction: Optional[bool] = None,\n) -&gt; Union[str, ContextResult]:\n    \"\"\"Instill tenets into context with smart injection.\n\n    Args:\n        context: Context to inject tenets into\n        session: Session identifier for tracking\n        force: Force injection regardless of frequency settings\n        strategy: Override injection strategy\n        max_tenets: Override maximum tenets\n        check_frequency: Whether to check injection frequency\n\n    Returns:\n        Modified context with tenets injected (if applicable)\n    \"\"\"\n    start_time = time.time()\n\n    # Track session if provided\n    if session:\n        if session not in self.session_histories:\n            self.session_histories[session] = InjectionHistory(session_id=session)\n        history = self.session_histories[session]\n        history.total_distills += 1\n    else:\n        history = None\n\n    # Extract text and format\n    if isinstance(context, ContextResult):\n        text = context.context\n        format_type = context.format\n        is_context_result = True\n    else:\n        text = context\n        format_type = \"markdown\"\n        is_context_result = False\n\n    # Analyze complexity using the analyzer (tests patch this)\n    try:\n        complexity = float(\n            self.complexity_analyzer.analyze(context if is_context_result else text)\n        )\n    except Exception:\n        # Fallback lightweight heuristic\n        try:\n            text_len = len(text)\n        except Exception:\n            text_len = 0\n        complexity = min(1.0, max(0.0, text_len / 20000.0))\n    try:\n        self.logger.debug(f\"Context complexity: {complexity:.2f}\")\n    except Exception:\n        self.logger.debug(\"Context complexity computed\")\n\n    # Optionally inject system instruction before tenets (when enabled)\n    sys_meta: Dict[str, Any] = {}\n    sys_injected_text: Optional[str] = None\n    # Determine whether to inject system instruction based on flag and config\n    sys_should = None\n    if inject_system_instruction is True:\n        sys_should = True\n    elif inject_system_instruction is False:\n        sys_should = False\n    else:\n        sys_should = bool(\n            self.config.tenet.system_instruction_enabled\n            and self.config.tenet.system_instruction\n        )\n\n    if sys_should:\n        modified_text, meta = self.inject_system_instruction(\n            text, format=format_type, session=session\n        )\n        # If actually injected, update text and tracking map\n        if meta.get(\"system_instruction_injected\"):\n            text = modified_text\n            sys_injected_text = modified_text\n            sys_meta = meta\n            if session:\n                self.system_instruction_injected[session] = True\n    # Analyze complexity\n    complexity = self.complexity_analyzer.analyze(context)\n    self.logger.debug(f\"Context complexity: {complexity:.2f}\")\n\n    # Check if we should inject\n    should_inject = force\n    skip_reason = None\n\n    if not force and check_frequency:\n        if history:\n            should_inject, reason = history.should_inject(\n                frequency=self.config.tenet.injection_frequency,\n                interval=self.config.tenet.injection_interval,\n                complexity=complexity,\n                complexity_threshold=self.config.tenet.session_complexity_threshold,\n                min_session_length=self.config.tenet.min_session_length,\n            )\n        else:\n            # No session history \u2013 treat as new/unnamed session that needs tenets\n            freq = self.config.tenet.injection_frequency\n            if freq == \"always\":\n                should_inject, reason = True, \"always_mode_no_session\"\n            elif freq == \"manual\":\n                should_inject, reason = False, \"manual_mode_no_session\"\n            else:\n                # For periodic/adaptive without a session, INJECT to establish context\n                # Unnamed sessions are important - they need guiding principles\n                should_inject, reason = True, f\"unnamed_session_needs_tenets\"\n\n    if not force and check_frequency and history:\n        should_inject, reason = history.should_inject(\n            frequency=self.config.tenet.injection_frequency,\n            interval=self.config.tenet.injection_interval,\n            complexity=complexity,\n            complexity_threshold=self.config.tenet.session_complexity_threshold,\n            min_session_length=self.config.tenet.min_session_length,\n        )\n\n        if not should_inject:\n            skip_reason = reason\n            self.logger.debug(f\"Skipping injection: {reason}\")\n\n    # Record metrics even if skipping\n    if not should_inject:\n        self.metrics_tracker.record_instillation(\n            tenet_count=0,\n            token_increase=0,\n            strategy=\"skipped\",\n            session=session,\n            complexity=complexity,\n            skip_reason=skip_reason,\n        )\n\n        # Save histories\n        self._save_session_histories()\n\n        # If we injected a system instruction earlier, return the modified\n        # content and include system_instruction metadata as tests expect.\n        if sys_meta.get(\"system_instruction_injected\") and sys_injected_text is not None:\n            if is_context_result:\n                extra_meta: Dict[str, Any] = {\n                    \"system_instruction\": sys_meta,\n                    \"injection_complexity\": complexity,\n                }\n                modified_context = ContextResult(\n                    files=context.files,  # type: ignore[attr-defined]\n                    context=sys_injected_text,\n                    format=context.format,  # type: ignore[attr-defined]\n                    metadata={**context.metadata, **extra_meta},  # type: ignore[attr-defined]\n                )\n                return modified_context\n            else:\n                return sys_injected_text\n\n        return context  # Return unchanged when nothing was injected\n\n    # Get tenets for injection\n    tenets = self._get_tenets_for_instillation(\n        session=session,\n        force=force,\n        content_length=len(text),\n        max_tenets=max_tenets or self.config.tenet.max_per_context,\n        history=history,\n        complexity=complexity,\n    )\n\n    if not tenets:\n        self.logger.info(\"No tenets available for instillation\")\n        return context\n\n    # Determine injection strategy\n    if not strategy:\n        strategy = self._determine_injection_strategy(\n            content_length=len(text),\n            tenet_count=len(tenets),\n            format_type=format_type,\n            complexity=complexity,\n        )\n\n    self.logger.info(\n        f\"Instilling {len(tenets)} tenets using {strategy} strategy\"\n        f\"{f' for session {session}' if session else ''}\"\n    )\n\n    # Inject tenets - TenetInjector doesn't have a strategy parameter\n    modified_text, injection_metadata = self.injector.inject_tenets(\n        content=text, tenets=tenets, format=format_type, context_metadata={\"strategy\": strategy}\n    )\n\n    # Update tenet metrics\n    for tenet in tenets:\n        # Update metrics and status on the tenet\n        try:\n            tenet.metrics.update_injection()\n            tenet.instill()\n        except Exception:\n            pass\n        self.manager._save_tenet(tenet)\n        self.metrics_tracker.record_tenet_usage(tenet.id)\n\n    # Record injection in history\n    if history:\n        history.record_injection(tenets, complexity)\n\n        # Check for reinforcement\n        if (\n            self.config.tenet.reinforcement\n            and history.total_injections % self.config.tenet.reinforcement_interval == 0\n        ):\n            history.reinforcement_count += 1\n            self.logger.info(f\"Reinforcement injection #{history.reinforcement_count}\")\n\n    # Create result\n    result = InstillationResult(\n        tenets_instilled=tenets,\n        injection_positions=injection_metadata.get(\"injections\", []),\n        token_increase=injection_metadata.get(\"token_increase\", 0),\n        strategy_used=strategy,\n        session=session,\n        complexity_score=complexity,\n        metrics={\n            \"processing_time\": time.time() - start_time,\n            \"complexity\": complexity,\n            \"injection_metadata\": injection_metadata,\n        },\n    )\n\n    # Cache result\n    cache_key = f\"{session or 'global'}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n    self._cache[cache_key] = result\n\n    # Record metrics\n    self.metrics_tracker.record_instillation(\n        tenet_count=len(tenets),\n        token_increase=injection_metadata.get(\"token_increase\", 0),\n        strategy=strategy,\n        session=session,\n        complexity=complexity,\n    )\n\n    # Save histories\n    self._save_session_histories()\n\n    # Return modified context\n    if is_context_result:\n        # Merge system instruction metadata if present\n        extra_meta: Dict[str, Any] = {\n            \"tenet_instillation\": result.to_dict(),\n            \"tenets_injected\": [t.id for t in tenets],\n            \"injection_complexity\": complexity,\n        }\n        if sys_meta:\n            extra_meta[\"system_instruction\"] = sys_meta\n\n        modified_context = ContextResult(\n            files=context.files,\n            context=modified_text,\n            format=context.format,\n            metadata={**context.metadata, **extra_meta},\n        )\n        return modified_context\n    else:\n        return modified_text\n</code></pre>"},{"location":"api/#tenets.Instiller.get_session_stats","title":"get_session_stats","text":"Python<pre><code>get_session_stats(session: str) -&gt; Dict[str, Any]\n</code></pre> <p>Get statistics for a specific session.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>str</code> <p>Session identifier</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary of session statistics</p> Source code in <code>tenets/core/instiller/instiller.py</code> Python<pre><code>def get_session_stats(self, session: str) -&gt; Dict[str, Any]:\n    \"\"\"Get statistics for a specific session.\n\n    Args:\n        session: Session identifier\n\n    Returns:\n        Dictionary of session statistics\n    \"\"\"\n    if session not in self.session_histories:\n        return {\"error\": f\"No history for session: {session}\"}\n\n    history = self.session_histories[session]\n    stats = history.get_stats()\n\n    # Add metrics from tracker\n    session_metrics = self.metrics_tracker.session_metrics.get(session, {})\n    stats.update(session_metrics)\n\n    return stats\n</code></pre>"},{"location":"api/#tenets.Instiller.get_all_session_stats","title":"get_all_session_stats","text":"Python<pre><code>get_all_session_stats() -&gt; Dict[str, Dict[str, Any]]\n</code></pre> <p>Get statistics for all sessions.</p> <p>Returns:</p> Type Description <code>Dict[str, Dict[str, Any]]</code> <p>Dictionary mapping session IDs to stats</p> Source code in <code>tenets/core/instiller/instiller.py</code> Python<pre><code>def get_all_session_stats(self) -&gt; Dict[str, Dict[str, Any]]:\n    \"\"\"Get statistics for all sessions.\n\n    Returns:\n        Dictionary mapping session IDs to stats\n    \"\"\"\n    all_stats = {}\n\n    for session_id, history in self.session_histories.items():\n        all_stats[session_id] = history.get_stats()\n\n    return all_stats\n</code></pre>"},{"location":"api/#tenets.Instiller.analyze_effectiveness","title":"analyze_effectiveness","text":"Python<pre><code>analyze_effectiveness(session: Optional[str] = None) -&gt; Dict[str, Any]\n</code></pre> <p>Analyze the effectiveness of tenet instillation.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Optional[str]</code> <p>Optional session to analyze</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with analysis results and recommendations</p> Source code in <code>tenets/core/instiller/instiller.py</code> Python<pre><code>def analyze_effectiveness(self, session: Optional[str] = None) -&gt; Dict[str, Any]:\n    \"\"\"Analyze the effectiveness of tenet instillation.\n\n    Args:\n        session: Optional session to analyze\n\n    Returns:\n        Dictionary with analysis results and recommendations\n    \"\"\"\n    # Get tenet effectiveness from manager\n    tenet_analysis = self.manager.analyze_tenet_effectiveness()\n\n    # Get instillation metrics\n    metrics = self.metrics_tracker.get_metrics(session)\n\n    # Session-specific analysis\n    session_analysis = {}\n    if session and session in self.session_histories:\n        session_analysis = self.get_session_stats(session)\n\n    # Generate recommendations\n    recommendations = []\n\n    # Check injection frequency\n    if metrics.get(\"total_instillations\", 0) &gt; 0:\n        avg_complexity = metrics.get(\"avg_complexity\", 0.5)\n\n        if avg_complexity &gt; 0.7:\n            recommendations.append(\n                \"High average complexity detected. Consider reducing injection frequency \"\n                \"or using simpler tenets.\"\n            )\n        elif avg_complexity &lt; 0.3:\n            recommendations.append(\n                \"Low average complexity. You could increase injection frequency \"\n                \"for better reinforcement.\"\n            )\n\n    # Check skip reasons\n    skip_dist = metrics.get(\"skip_distribution\", {})\n    if skip_dist:\n        top_skip = max(skip_dist.items(), key=lambda x: x[1])\n        if \"session_too_short\" in top_skip[0]:\n            recommendations.append(\n                f\"Many skips due to short sessions. Consider reducing min_session_length \"\n                f\"(currently {self.config.tenet.min_session_length}).\"\n            )\n\n    # Check tenet usage\n    if tenet_analysis.get(\"need_reinforcement\"):\n        recommendations.append(\n            f\"Tenets needing reinforcement: {', '.join(tenet_analysis['need_reinforcement'][:3])}\"\n        )\n\n    return {\n        \"tenet_effectiveness\": tenet_analysis,\n        \"instillation_metrics\": metrics,\n        \"session_analysis\": session_analysis,\n        \"recommendations\": recommendations,\n        \"configuration\": {\n            \"injection_frequency\": self.config.tenet.injection_frequency,\n            \"injection_interval\": self.config.tenet.injection_interval,\n            \"complexity_threshold\": self.config.tenet.session_complexity_threshold,\n            \"min_session_length\": self.config.tenet.min_session_length,\n        },\n    }\n</code></pre>"},{"location":"api/#tenets.Instiller.export_instillation_history","title":"export_instillation_history","text":"Python<pre><code>export_instillation_history(output_path: Path, format: str = 'json', session: Optional[str] = None) -&gt; None\n</code></pre> <p>Export instillation history to file.</p> <p>Parameters:</p> Name Type Description Default <code>output_path</code> <code>Path</code> <p>Path to output file</p> required <code>format</code> <code>str</code> <p>Export format (json or csv)</p> <code>'json'</code> <code>session</code> <code>Optional[str]</code> <p>Optional session filter</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If format is not supported</p> Source code in <code>tenets/core/instiller/instiller.py</code> Python<pre><code>def export_instillation_history(\n    self,\n    output_path: Path,\n    format: str = \"json\",\n    session: Optional[str] = None,\n) -&gt; None:\n    \"\"\"Export instillation history to file.\n\n    Args:\n        output_path: Path to output file\n        format: Export format (json or csv)\n        session: Optional session filter\n\n    Raises:\n        ValueError: If format is not supported\n    \"\"\"\n    if format == \"json\":\n        # Export as JSON\n        data = {\n            \"exported_at\": datetime.now().isoformat(),\n            \"configuration\": {\n                \"injection_frequency\": self.config.tenet.injection_frequency,\n                \"injection_interval\": self.config.tenet.injection_interval,\n            },\n            \"metrics\": self.metrics_tracker.get_all_metrics(),\n            \"session_histories\": {},\n            \"cached_results\": {},\n        }\n\n        # Add session histories\n        for sid, history in self.session_histories.items():\n            if not session or sid == session:\n                data[\"session_histories\"][sid] = history.get_stats()\n\n        # Add cached results\n        for key, result in self._cache.items():\n            if not session or result.session == session:\n                data[\"cached_results\"][key] = result.to_dict()\n\n        with open(output_path, \"w\") as f:\n            json.dump(data, f, indent=2)\n\n    elif format == \"csv\":\n        # Export as CSV\n        import csv\n\n        rows = []\n        for record in self.metrics_tracker.instillations:\n            if not session or record.get(\"session\") == session:\n                rows.append(\n                    {\n                        \"Timestamp\": record[\"timestamp\"],\n                        \"Session\": record.get(\"session\", \"\"),\n                        \"Tenets\": record[\"tenet_count\"],\n                        \"Tokens\": record[\"token_increase\"],\n                        \"Strategy\": record[\"strategy\"],\n                        \"Complexity\": f\"{record.get('complexity', 0):.2f}\",\n                        \"Skip Reason\": record.get(\"skip_reason\", \"\"),\n                    }\n                )\n\n        if rows:\n            with open(output_path, \"w\", newline=\"\") as f:\n                writer = csv.DictWriter(f, fieldnames=rows[0].keys())\n                writer.writeheader()\n                writer.writerows(rows)\n        else:\n            # Create empty file with headers\n            with open(output_path, \"w\", newline=\"\") as f:\n                writer = csv.writer(f)\n                writer.writerow(\n                    [\n                        \"Timestamp\",\n                        \"Session\",\n                        \"Tenets\",\n                        \"Tokens\",\n                        \"Strategy\",\n                        \"Complexity\",\n                        \"Skip Reason\",\n                    ]\n                )\n\n    else:\n        raise ValueError(f\"Unsupported export format: {format}\")\n\n    self.logger.info(f\"Exported instillation history to {output_path}\")\n</code></pre>"},{"location":"api/#tenets.Instiller.reset_session_history","title":"reset_session_history","text":"Python<pre><code>reset_session_history(session: str) -&gt; bool\n</code></pre> <p>Reset injection history for a session.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>str</code> <p>Session identifier</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if reset, False if session not found</p> Source code in <code>tenets/core/instiller/instiller.py</code> Python<pre><code>def reset_session_history(self, session: str) -&gt; bool:\n    \"\"\"Reset injection history for a session.\n\n    Args:\n        session: Session identifier\n\n    Returns:\n        True if reset, False if session not found\n    \"\"\"\n    if session in self.session_histories:\n        self.session_histories[session] = InjectionHistory(session_id=session)\n        self._save_session_histories()\n        self.logger.info(f\"Reset injection history for session: {session}\")\n        return True\n    return False\n</code></pre>"},{"location":"api/#tenets.Instiller.clear_cache","title":"clear_cache","text":"Python<pre><code>clear_cache() -&gt; None\n</code></pre> <p>Clear the results cache.</p> Source code in <code>tenets/core/instiller/instiller.py</code> Python<pre><code>def clear_cache(self) -&gt; None:\n    \"\"\"Clear the results cache.\"\"\"\n    self._cache.clear()\n    self.logger.info(\"Cleared instillation results cache\")\n</code></pre>"},{"location":"api/#tenets.TenetManager","title":"TenetManager","text":"Python<pre><code>TenetManager(config: TenetsConfig)\n</code></pre> <p>Manages tenets throughout their lifecycle.</p> <p>Initialize the tenet manager.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>TenetsConfig</code> <p>Tenets configuration</p> required Source code in <code>tenets/core/instiller/manager.py</code> Python<pre><code>def __init__(self, config: TenetsConfig):\n    \"\"\"Initialize the tenet manager.\n\n    Args:\n        config: Tenets configuration\n    \"\"\"\n    self.config = config\n    self.logger = get_logger(__name__)\n\n    # Initialize storage\n    self.storage_path = Path(config.cache_dir) / \"tenets\"\n    self.storage_path.mkdir(parents=True, exist_ok=True)\n\n    self.db_path = self.storage_path / \"tenets.db\"\n    self._init_database()\n\n    # Cache for active tenets\n    self._tenet_cache: Dict[str, Tenet] = {}\n    self._load_active_tenets()\n</code></pre>"},{"location":"api/#tenets.TenetManager-functions","title":"Functions","text":""},{"location":"api/#tenets.TenetManager.add_tenet","title":"add_tenet","text":"Python<pre><code>add_tenet(content: Union[str, Tenet], priority: Union[str, Priority] = 'medium', category: Optional[Union[str, TenetCategory]] = None, session: Optional[str] = None, author: Optional[str] = None) -&gt; Tenet\n</code></pre> <p>Add a new tenet.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>Union[str, Tenet]</code> <p>The guiding principle text or a Tenet object</p> required <code>priority</code> <code>Union[str, Priority]</code> <p>Priority level (low, medium, high, critical)</p> <code>'medium'</code> <code>category</code> <code>Optional[Union[str, TenetCategory]]</code> <p>Category for organization</p> <code>None</code> <code>session</code> <code>Optional[str]</code> <p>Bind to specific session</p> <code>None</code> <code>author</code> <code>Optional[str]</code> <p>Who created the tenet</p> <code>None</code> <p>Returns:</p> Type Description <code>Tenet</code> <p>The created Tenet</p> Source code in <code>tenets/core/instiller/manager.py</code> Python<pre><code>def add_tenet(\n    self,\n    content: Union[str, Tenet],\n    priority: Union[str, Priority] = \"medium\",\n    category: Optional[Union[str, TenetCategory]] = None,\n    session: Optional[str] = None,\n    author: Optional[str] = None,\n) -&gt; Tenet:\n    \"\"\"Add a new tenet.\n\n    Args:\n        content: The guiding principle text or a Tenet object\n        priority: Priority level (low, medium, high, critical)\n        category: Category for organization\n        session: Bind to specific session\n        author: Who created the tenet\n\n    Returns:\n        The created Tenet\n    \"\"\"\n    # Check if content is already a Tenet object\n    if isinstance(content, Tenet):\n        tenet = content\n        # Update session bindings if a session was specified\n        if session and session not in (tenet.session_bindings or []):\n            if tenet.session_bindings:\n                tenet.session_bindings.append(session)\n            else:\n                tenet.session_bindings = [session]\n    else:\n        # Create tenet from string content\n        # Ensure content is a string before calling strip()\n        if not isinstance(content, str):\n            raise TypeError(\n                f\"Expected string or Tenet, got {type(content).__name__}: {content}\"\n            )\n        tenet = Tenet(\n            content=content.strip(),\n            priority=priority if isinstance(priority, Priority) else Priority(priority),\n            category=(\n                category\n                if isinstance(category, TenetCategory)\n                else (TenetCategory(category) if category else None)\n            ),\n            author=author,\n        )\n\n    # Bind to session if specified\n    if session:\n        tenet.bind_to_session(session)\n\n    # Save to database\n    self._save_tenet(tenet)\n\n    # Add to cache\n    self._tenet_cache[tenet.id] = tenet\n\n    self.logger.info(f\"Added tenet: {tenet.id} - {tenet.content[:50]}...\")\n\n    return tenet\n</code></pre>"},{"location":"api/#tenets.TenetManager.get_tenet","title":"get_tenet","text":"Python<pre><code>get_tenet(tenet_id: str) -&gt; Optional[Tenet]\n</code></pre> <p>Get a specific tenet by ID.</p> <p>Parameters:</p> Name Type Description Default <code>tenet_id</code> <code>str</code> <p>Tenet ID (can be partial)</p> required <p>Returns:</p> Type Description <code>Optional[Tenet]</code> <p>The Tenet or None if not found</p> Source code in <code>tenets/core/instiller/manager.py</code> Python<pre><code>def get_tenet(self, tenet_id: str) -&gt; Optional[Tenet]:\n    \"\"\"Get a specific tenet by ID.\n\n    Args:\n        tenet_id: Tenet ID (can be partial)\n\n    Returns:\n        The Tenet or None if not found\n    \"\"\"\n    # Try cache first\n    if tenet_id in self._tenet_cache:\n        return self._tenet_cache[tenet_id]\n\n    # Try partial match\n    for tid, tenet in self._tenet_cache.items():\n        if tid.startswith(tenet_id):\n            return tenet\n\n    # Try database\n    with sqlite3.connect(self.db_path) as conn:\n        conn.row_factory = sqlite3.Row\n        cursor = conn.execute(\"SELECT data FROM tenets WHERE id LIKE ?\", (f\"{tenet_id}%\",))\n        row = cursor.fetchone()\n\n        if row:\n            tenet = Tenet.from_dict(json.loads(row[\"data\"]))\n            self._tenet_cache[tenet.id] = tenet\n            return tenet\n\n    return None\n</code></pre>"},{"location":"api/#tenets.TenetManager.list_tenets","title":"list_tenets","text":"Python<pre><code>list_tenets(pending_only: bool = False, instilled_only: bool = False, session: Optional[str] = None, category: Optional[Union[str, TenetCategory]] = None) -&gt; List[Dict[str, Any]]\n</code></pre> <p>List tenets with filtering.</p> <p>Parameters:</p> Name Type Description Default <code>pending_only</code> <code>bool</code> <p>Only show pending tenets</p> <code>False</code> <code>instilled_only</code> <code>bool</code> <p>Only show instilled tenets</p> <code>False</code> <code>session</code> <code>Optional[str]</code> <p>Filter by session binding</p> <code>None</code> <code>category</code> <code>Optional[Union[str, TenetCategory]]</code> <p>Filter by category</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List of tenet dictionaries</p> Source code in <code>tenets/core/instiller/manager.py</code> Python<pre><code>def list_tenets(\n    self,\n    pending_only: bool = False,\n    instilled_only: bool = False,\n    session: Optional[str] = None,\n    category: Optional[Union[str, TenetCategory]] = None,\n) -&gt; List[Dict[str, Any]]:\n    \"\"\"List tenets with filtering.\n\n    Args:\n        pending_only: Only show pending tenets\n        instilled_only: Only show instilled tenets\n        session: Filter by session binding\n        category: Filter by category\n\n    Returns:\n        List of tenet dictionaries\n    \"\"\"\n    tenets = []\n\n    # Build query\n    query = \"SELECT data FROM tenets WHERE 1=1\"\n    params = []\n\n    if pending_only:\n        query += \" AND status = ?\"\n        params.append(TenetStatus.PENDING.value)\n    elif instilled_only:\n        query += \" AND status = ?\"\n        params.append(TenetStatus.INSTILLED.value)\n    else:\n        query += \" AND status != ?\"\n        params.append(TenetStatus.ARCHIVED.value)\n\n    if category:\n        cat_value = category if isinstance(category, str) else category.value\n        query += \" AND category = ?\"\n        params.append(cat_value)\n\n    query += \" ORDER BY created_at DESC\"\n\n    with sqlite3.connect(self.db_path) as conn:\n        conn.row_factory = sqlite3.Row\n        cursor = conn.execute(query, params)\n\n        for row in cursor:\n            tenet = Tenet.from_dict(json.loads(row[\"data\"]))\n\n            # Filter by session if specified\n            if session and not tenet.applies_to_session(session):\n                continue\n\n            tenet_dict = tenet.to_dict()\n            tenet_dict[\"instilled\"] = tenet.status == TenetStatus.INSTILLED\n            tenets.append(tenet_dict)\n\n    return tenets\n</code></pre>"},{"location":"api/#tenets.TenetManager.get_pending_tenets","title":"get_pending_tenets","text":"Python<pre><code>get_pending_tenets(session: Optional[str] = None) -&gt; List[Tenet]\n</code></pre> <p>Get all pending tenets.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Optional[str]</code> <p>Filter by session</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Tenet]</code> <p>List of pending Tenet objects</p> Source code in <code>tenets/core/instiller/manager.py</code> Python<pre><code>def get_pending_tenets(self, session: Optional[str] = None) -&gt; List[Tenet]:\n    \"\"\"Get all pending tenets.\n\n    Args:\n        session: Filter by session\n\n    Returns:\n        List of pending Tenet objects\n    \"\"\"\n    pending = []\n\n    for tenet in self._tenet_cache.values():\n        if tenet.status == TenetStatus.PENDING:\n            if not session or tenet.applies_to_session(session):\n                pending.append(tenet)\n\n    return sorted(pending, key=lambda t: (t.priority.weight, t.created_at), reverse=True)\n</code></pre>"},{"location":"api/#tenets.TenetManager.remove_tenet","title":"remove_tenet","text":"Python<pre><code>remove_tenet(tenet_id: str) -&gt; bool\n</code></pre> <p>Remove a tenet.</p> <p>Parameters:</p> Name Type Description Default <code>tenet_id</code> <code>str</code> <p>Tenet ID (can be partial)</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if removed, False if not found</p> Source code in <code>tenets/core/instiller/manager.py</code> Python<pre><code>def remove_tenet(self, tenet_id: str) -&gt; bool:\n    \"\"\"Remove a tenet.\n\n    Args:\n        tenet_id: Tenet ID (can be partial)\n\n    Returns:\n        True if removed, False if not found\n    \"\"\"\n    tenet = self.get_tenet(tenet_id)\n    if not tenet:\n        return False\n\n    # Archive instead of delete\n    tenet.archive()\n    self._save_tenet(tenet)\n\n    # Remove from cache\n    if tenet.id in self._tenet_cache:\n        del self._tenet_cache[tenet.id]\n\n    self.logger.info(f\"Archived tenet: {tenet.id}\")\n    return True\n</code></pre>"},{"location":"api/#tenets.TenetManager.instill_tenets","title":"instill_tenets","text":"Python<pre><code>instill_tenets(session: Optional[str] = None, force: bool = False) -&gt; Dict[str, Any]\n</code></pre> <p>Instill pending tenets.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Optional[str]</code> <p>Target session</p> <code>None</code> <code>force</code> <code>bool</code> <p>Re-instill even if already instilled</p> <code>False</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with results</p> Source code in <code>tenets/core/instiller/manager.py</code> Python<pre><code>def instill_tenets(self, session: Optional[str] = None, force: bool = False) -&gt; Dict[str, Any]:\n    \"\"\"Instill pending tenets.\n\n    Args:\n        session: Target session\n        force: Re-instill even if already instilled\n\n    Returns:\n        Dictionary with results\n    \"\"\"\n    tenets_to_instill = []\n\n    if force:\n        # Get all non-archived tenets\n        for tenet in self._tenet_cache.values():\n            if tenet.status != TenetStatus.ARCHIVED:\n                if not session or tenet.applies_to_session(session):\n                    tenets_to_instill.append(tenet)\n    else:\n        # Get only pending tenets\n        tenets_to_instill = self.get_pending_tenets(session)\n\n    # Sort by priority and creation date\n    tenets_to_instill.sort(key=lambda t: (t.priority.weight, t.created_at), reverse=True)\n\n    # Mark as instilled\n    instilled = []\n    for tenet in tenets_to_instill:\n        tenet.instill()\n        self._save_tenet(tenet)\n        instilled.append(tenet.content)\n\n    self.logger.info(f\"Instilled {len(instilled)} tenets\")\n\n    return {\n        \"count\": len(instilled),\n        \"tenets\": instilled,\n        \"session\": session,\n        \"strategy\": \"priority-based\",\n    }\n</code></pre>"},{"location":"api/#tenets.TenetManager.get_tenets_for_injection","title":"get_tenets_for_injection","text":"Python<pre><code>get_tenets_for_injection(context_length: int, session: Optional[str] = None, max_tenets: int = 5) -&gt; List[Tenet]\n</code></pre> <p>Get tenets ready for injection into context.</p> <p>Parameters:</p> Name Type Description Default <code>context_length</code> <code>int</code> <p>Current context length in tokens</p> required <code>session</code> <code>Optional[str]</code> <p>Current session</p> <code>None</code> <code>max_tenets</code> <code>int</code> <p>Maximum number of tenets to return</p> <code>5</code> <p>Returns:</p> Type Description <code>List[Tenet]</code> <p>List of tenets to inject</p> Source code in <code>tenets/core/instiller/manager.py</code> Python<pre><code>def get_tenets_for_injection(\n    self, context_length: int, session: Optional[str] = None, max_tenets: int = 5\n) -&gt; List[Tenet]:\n    \"\"\"Get tenets ready for injection into context.\n\n    Args:\n        context_length: Current context length in tokens\n        session: Current session\n        max_tenets: Maximum number of tenets to return\n\n    Returns:\n        List of tenets to inject\n    \"\"\"\n    candidates = []\n\n    # Get applicable tenets\n    for tenet in self._tenet_cache.values():\n        if tenet.status == TenetStatus.INSTILLED:\n            if not session or tenet.applies_to_session(session):\n                candidates.append(tenet)\n\n    # Sort by priority and need for reinforcement\n    candidates.sort(\n        key=lambda t: (\n            t.priority.weight,\n            t.metrics.reinforcement_needed,\n            -t.metrics.injection_count,  # Prefer less frequently injected\n        ),\n        reverse=True,\n    )\n\n    # Select tenets based on injection strategy\n    selected = []\n    for tenet in candidates:\n        if len(selected) &gt;= max_tenets:\n            break\n\n        if tenet.should_inject(context_length, len(selected)):\n            selected.append(tenet)\n\n            # Update metrics\n            tenet.metrics.update_injection()\n            self._save_tenet(tenet)\n\n    return selected\n</code></pre>"},{"location":"api/#tenets.TenetManager.export_tenets","title":"export_tenets","text":"Python<pre><code>export_tenets(format: str = 'yaml', session: Optional[str] = None, include_archived: bool = False) -&gt; str\n</code></pre> <p>Export tenets to YAML or JSON.</p> <p>Parameters:</p> Name Type Description Default <code>format</code> <code>str</code> <p>Export format (yaml or json)</p> <code>'yaml'</code> <code>session</code> <code>Optional[str]</code> <p>Filter by session</p> <code>None</code> <code>include_archived</code> <code>bool</code> <p>Include archived tenets</p> <code>False</code> <p>Returns:</p> Type Description <code>str</code> <p>Serialized tenets</p> Source code in <code>tenets/core/instiller/manager.py</code> Python<pre><code>def export_tenets(\n    self, format: str = \"yaml\", session: Optional[str] = None, include_archived: bool = False\n) -&gt; str:\n    \"\"\"Export tenets to YAML or JSON.\n\n    Args:\n        format: Export format (yaml or json)\n        session: Filter by session\n        include_archived: Include archived tenets\n\n    Returns:\n        Serialized tenets\n    \"\"\"\n    tenets_data = []\n\n    for tenet in self._tenet_cache.values():\n        if not include_archived and tenet.status == TenetStatus.ARCHIVED:\n            continue\n\n        if session and not tenet.applies_to_session(session):\n            continue\n\n        tenets_data.append(tenet.to_dict())\n\n    # Sort by creation date\n    tenets_data.sort(key=lambda t: t[\"created_at\"])\n\n    export_data = {\n        \"version\": \"1.0\",\n        \"exported_at\": datetime.now().isoformat(),\n        \"tenets\": tenets_data,\n    }\n\n    if format == \"yaml\":\n        return yaml.dump(export_data, default_flow_style=False, sort_keys=False)\n    else:\n        return json.dumps(export_data, indent=2)\n</code></pre>"},{"location":"api/#tenets.TenetManager.import_tenets","title":"import_tenets","text":"Python<pre><code>import_tenets(file_path: Union[str, Path], session: Optional[str] = None, override_priority: Optional[Priority] = None) -&gt; int\n</code></pre> <p>Import tenets from file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Union[str, Path]</code> <p>Path to import file</p> required <code>session</code> <code>Optional[str]</code> <p>Bind imported tenets to session</p> <code>None</code> <code>override_priority</code> <code>Optional[Priority]</code> <p>Override priority for all imported tenets</p> <code>None</code> <p>Returns:</p> Type Description <code>int</code> <p>Number of tenets imported</p> Source code in <code>tenets/core/instiller/manager.py</code> Python<pre><code>def import_tenets(\n    self,\n    file_path: Union[str, Path],\n    session: Optional[str] = None,\n    override_priority: Optional[Priority] = None,\n) -&gt; int:\n    \"\"\"Import tenets from file.\n\n    Args:\n        file_path: Path to import file\n        session: Bind imported tenets to session\n        override_priority: Override priority for all imported tenets\n\n    Returns:\n        Number of tenets imported\n    \"\"\"\n    file_path = Path(file_path)\n\n    if not file_path.exists():\n        raise FileNotFoundError(f\"Import file not found: {file_path}\")\n\n    # Load data\n    with open(file_path) as f:\n        if file_path.suffix in [\".yaml\", \".yml\"]:\n            data = yaml.safe_load(f)\n        else:\n            data = json.load(f)\n\n    # Import tenets\n    imported = 0\n    tenets = data.get(\"tenets\", [])\n\n    for tenet_data in tenets:\n        # Skip if already exists\n        if self.get_tenet(tenet_data.get(\"id\", \"\")):\n            continue\n\n        # Create new tenet\n        tenet = Tenet.from_dict(tenet_data)\n\n        # Override priority if requested\n        if override_priority:\n            tenet.priority = override_priority\n\n        # Bind to session if specified\n        if session:\n            tenet.bind_to_session(session)\n\n        # Reset status to pending\n        tenet.status = TenetStatus.PENDING\n        tenet.instilled_at = None\n\n        # Save\n        self._save_tenet(tenet)\n        self._tenet_cache[tenet.id] = tenet\n\n        imported += 1\n\n    self.logger.info(f\"Imported {imported} tenets from {file_path}\")\n    return imported\n</code></pre>"},{"location":"api/#tenets.TenetManager.create_collection","title":"create_collection","text":"Python<pre><code>create_collection(name: str, description: str = '', tenet_ids: Optional[List[str]] = None) -&gt; TenetCollection\n</code></pre> <p>Create a collection of related tenets.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Collection name</p> required <code>description</code> <code>str</code> <p>Collection description</p> <code>''</code> <code>tenet_ids</code> <code>Optional[List[str]]</code> <p>IDs of tenets to include</p> <code>None</code> <p>Returns:</p> Type Description <code>TenetCollection</code> <p>The created TenetCollection</p> Source code in <code>tenets/core/instiller/manager.py</code> Python<pre><code>def create_collection(\n    self, name: str, description: str = \"\", tenet_ids: Optional[List[str]] = None\n) -&gt; TenetCollection:\n    \"\"\"Create a collection of related tenets.\n\n    Args:\n        name: Collection name\n        description: Collection description\n        tenet_ids: IDs of tenets to include\n\n    Returns:\n        The created TenetCollection\n    \"\"\"\n    collection = TenetCollection(name=name, description=description)\n\n    if tenet_ids:\n        for tenet_id in tenet_ids:\n            if tenet := self.get_tenet(tenet_id):\n                collection.add_tenet(tenet)\n\n    # Save collection\n    collection_path = self.storage_path / f\"collection_{name.lower().replace(' ', '_')}.json\"\n    with open(collection_path, \"w\") as f:\n        json.dump(collection.to_dict(), f, indent=2)\n\n    return collection\n</code></pre>"},{"location":"api/#tenets.TenetManager.analyze_tenet_effectiveness","title":"analyze_tenet_effectiveness","text":"Python<pre><code>analyze_tenet_effectiveness() -&gt; Dict[str, Any]\n</code></pre> <p>Analyze effectiveness of tenets.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Analysis of tenet usage and effectiveness</p> Source code in <code>tenets/core/instiller/manager.py</code> Python<pre><code>def analyze_tenet_effectiveness(self) -&gt; Dict[str, Any]:\n    \"\"\"Analyze effectiveness of tenets.\n\n    Returns:\n        Analysis of tenet usage and effectiveness\n    \"\"\"\n    total_tenets = len(self._tenet_cache)\n\n    if total_tenets == 0:\n        return {\"total_tenets\": 0, \"status\": \"No tenets configured\"}\n\n    # Gather statistics\n    stats = {\n        \"total_tenets\": total_tenets,\n        \"by_status\": {},\n        \"by_priority\": {},\n        \"by_category\": {},\n        \"most_injected\": [],\n        \"least_effective\": [],\n        \"need_reinforcement\": [],\n    }\n\n    # Count by status\n    for status in TenetStatus:\n        count = sum(1 for t in self._tenet_cache.values() if t.status == status)\n        stats[\"by_status\"][status.value] = count\n\n    # Count by priority\n    for priority in Priority:\n        count = sum(1 for t in self._tenet_cache.values() if t.priority == priority)\n        stats[\"by_priority\"][priority.value] = count\n\n    # Count by category\n    category_counts = {}\n    for tenet in self._tenet_cache.values():\n        if tenet.category:\n            cat = tenet.category.value\n            category_counts[cat] = category_counts.get(cat, 0) + 1\n    stats[\"by_category\"] = category_counts\n\n    # Find most injected\n    sorted_by_injection = sorted(\n        self._tenet_cache.values(), key=lambda t: t.metrics.injection_count, reverse=True\n    )\n    stats[\"most_injected\"] = [\n        {\n            \"id\": t.id[:8],\n            \"content\": t.content[:50] + \"...\" if len(t.content) &gt; 50 else t.content,\n            \"count\": t.metrics.injection_count,\n        }\n        for t in sorted_by_injection[:5]\n    ]\n\n    # Find least effective\n    sorted_by_compliance = sorted(\n        [t for t in self._tenet_cache.values() if t.metrics.injection_count &gt; 0],\n        key=lambda t: t.metrics.compliance_score,\n    )\n    stats[\"least_effective\"] = [\n        {\n            \"id\": t.id[:8],\n            \"content\": t.content[:50] + \"...\" if len(t.content) &gt; 50 else t.content,\n            \"score\": t.metrics.compliance_score,\n        }\n        for t in sorted_by_compliance[:5]\n    ]\n\n    # Find those needing reinforcement\n    stats[\"need_reinforcement\"] = [\n        {\n            \"id\": t.id[:8],\n            \"content\": t.content[:50] + \"...\" if len(t.content) &gt; 50 else t.content,\n            \"priority\": t.priority.value,\n        }\n        for t in self._tenet_cache.values()\n        if t.metrics.reinforcement_needed\n    ]\n\n    return stats\n</code></pre>"},{"location":"api/#tenets.Tenets","title":"Tenets","text":"Python<pre><code>Tenets(config: Optional[Union[TenetsConfig, dict[str, Any], Path]] = None)\n</code></pre> <p>Main API interface for the Tenets system.</p> <p>This is the primary class that users interact with to access all Tenets functionality. It coordinates between the various subsystems (distiller, instiller, analyzer, etc.) to provide a unified interface.</p> <p>The Tenets class can be used both programmatically through Python and via the CLI. It maintains configuration, manages sessions, and orchestrates the various analysis and context generation operations.</p> <p>Attributes:</p> Name Type Description <code>config</code> <p>TenetsConfig instance containing all configuration</p> <code>distiller</code> <p>Distiller instance for context extraction</p> <code>instiller</code> <p>Instiller instance for tenet management</p> <code>tenet_manager</code> <p>Direct access to TenetManager for advanced operations</p> <code>logger</code> <p>Logger instance for this class</p> <code>_session</code> <p>Current session name if any</p> <code>_cache</code> <p>Internal cache for results</p> Example <p>from tenets import Tenets from pathlib import Path</p> <p>Initialize Tenets with configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Optional[Union[TenetsConfig, dict[str, Any], Path]]</code> <p>Can be: - TenetsConfig instance - Dictionary of configuration values - Path to configuration file - None (uses default configuration)</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If config format is invalid</p> <code>FileNotFoundError</code> <p>If config file path doesn't exist</p> Source code in <code>tenets/__init__.py</code> Python<pre><code>def __init__(self, config: Optional[Union[TenetsConfig, dict[str, Any], Path]] = None):\n    \"\"\"Initialize Tenets with configuration.\n\n    Args:\n        config: Can be:\n            - TenetsConfig instance\n            - Dictionary of configuration values\n            - Path to configuration file\n            - None (uses default configuration)\n\n    Raises:\n        ValueError: If config format is invalid\n        FileNotFoundError: If config file path doesn't exist\n    \"\"\"\n    # Handle different config input types\n    if config is None:\n        self.config = TenetsConfig()\n    elif isinstance(config, TenetsConfig):\n        self.config = config\n    elif isinstance(config, dict):\n        # Map common top-level aliases into nested config structure\n        cfg = TenetsConfig()\n        # Known top-level shortcuts used in docs/tests\n        if \"max_tokens\" in config:\n            cfg.max_tokens = int(config[\"max_tokens\"])  # type: ignore[arg-type]\n        if \"debug\" in config:\n            cfg.debug = bool(config[\"debug\"])  # type: ignore[arg-type]\n        if \"ranking_algorithm\" in config:\n            cfg.ranking.algorithm = str(config[\"ranking_algorithm\"])  # type: ignore[arg-type]\n        # Apply any nested sections if provided\n        if \"scanner\" in config and isinstance(config[\"scanner\"], dict):\n            cfg.scanner = type(cfg.scanner)(**config[\"scanner\"])  # type: ignore[call-arg]\n        if \"ranking\" in config and isinstance(config[\"ranking\"], dict):\n            cfg.ranking = type(cfg.ranking)(**config[\"ranking\"])  # type: ignore[call-arg]\n        if \"tenet\" in config and isinstance(config[\"tenet\"], dict):\n            cfg.tenet = type(cfg.tenet)(**config[\"tenet\"])  # type: ignore[call-arg]\n        if \"cache\" in config and isinstance(config[\"cache\"], dict):\n            cfg.cache = type(cfg.cache)(**config[\"cache\"])  # type: ignore[call-arg]\n        if \"output\" in config and isinstance(config[\"output\"], dict):\n            cfg.output = type(cfg.output)(**config[\"output\"])  # type: ignore[call-arg]\n        if \"git\" in config and isinstance(config[\"git\"], dict):\n            cfg.git = type(cfg.git)(**config[\"git\"])  # type: ignore[call-arg]\n        # Any other keys go to custom\n        for k, v in config.items():\n            if k not in {\n                \"max_tokens\",\n                \"debug\",\n                \"ranking_algorithm\",\n                \"scanner\",\n                \"ranking\",\n                \"tenet\",\n                \"cache\",\n                \"output\",\n                \"git\",\n            }:\n                cfg.custom[k] = v\n        self.config = cfg\n    elif isinstance(config, (str, Path)):\n        config_path = Path(config)\n        if not config_path.exists():\n            raise FileNotFoundError(f\"Config file not found: {config_path}\")\n        self.config = TenetsConfig(config_file=config_path)\n    else:\n        raise ValueError(f\"Invalid config type: {type(config)}\")\n\n    # Initialize logger (import locally to avoid circular import)\n    from tenets.utils.logger import get_logger\n\n    self.logger = get_logger(__name__)\n    self.logger.info(f\"Initializing Tenets v{__version__}\")\n\n    # Lazy-load core components to improve import performance\n    self._distiller = None\n    self._instiller = None\n    self._tenet_manager = None\n\n    # Session management\n    self._session = None\n    self._session_data = {}\n\n    # Internal cache\n    self._cache = {}\n\n    self.logger.info(\"Tenets initialization complete\")\n</code></pre>"},{"location":"api/#tenets.Tenets--initialize-with-default-config","title":"Initialize with default config","text":"<p>ten = Tenets()</p>"},{"location":"api/#tenets.Tenets--or-with-custom-config","title":"Or with custom config","text":"<p>from tenets.config import TenetsConfig config = TenetsConfig(max_tokens=150000, ranking_algorithm=\"thorough\") ten = Tenets(config=config)</p>"},{"location":"api/#tenets.Tenets--extract-context-uses-default-session-automatically","title":"Extract context (uses default session automatically)","text":"<p>result = ten.distill(\"implement user authentication\") print(f\"Generated {result.token_count} tokens of context\")</p>"},{"location":"api/#tenets.Tenets--generate-html-report","title":"Generate HTML report","text":"<p>result = ten.distill(\"review API endpoints\", format=\"html\") Path(\"api-review.html\").write_text(result.context)</p>"},{"location":"api/#tenets.Tenets--add-and-apply-tenets","title":"Add and apply tenets","text":"<p>ten.add_tenet(\"Use dependency injection\", priority=\"high\") ten.add_tenet(\"Follow RESTful conventions\", category=\"architecture\") ten.instill_tenets()</p>"},{"location":"api/#tenets.Tenets--pin-critical-files-for-priority-inclusion","title":"Pin critical files for priority inclusion","text":"<p>ten.pin_file(\"src/core/auth.py\") ten.pin_folder(\"src/api/endpoints\")</p>"},{"location":"api/#tenets.Tenets--work-with-named-sessions","title":"Work with named sessions","text":"<p>result = ten.distill( ...     \"implement OAuth2\", ...     session_name=\"oauth-feature\", ...     mode=\"thorough\" ... )</p>"},{"location":"api/#tenets.Tenets-attributes","title":"Attributes","text":""},{"location":"api/#tenets.Tenets.distiller","title":"distiller  <code>property</code>","text":"Python<pre><code>distiller\n</code></pre> <p>Lazy load distiller when needed.</p>"},{"location":"api/#tenets.Tenets.instiller","title":"instiller  <code>property</code>","text":"Python<pre><code>instiller\n</code></pre> <p>Lazy load instiller when needed.</p>"},{"location":"api/#tenets.Tenets.tenet_manager","title":"tenet_manager  <code>property</code>","text":"Python<pre><code>tenet_manager\n</code></pre> <p>Lazy load tenet manager when needed.</p>"},{"location":"api/#tenets.Tenets-functions","title":"Functions","text":""},{"location":"api/#tenets.Tenets.distill","title":"distill","text":"Python<pre><code>distill(prompt: str, files: Optional[Union[str, Path, list[Path]]] = None, *, format: str = 'markdown', model: Optional[str] = None, max_tokens: Optional[int] = None, mode: str = 'balanced', include_git: bool = True, session_name: Optional[str] = None, include_patterns: Optional[list[str]] = None, exclude_patterns: Optional[list[str]] = None, apply_tenets: Optional[bool] = None, full: bool = False, condense: bool = False, remove_comments: bool = False, include_tests: Optional[bool] = None, docstring_weight: Optional[float] = None, summarize_imports: bool = True) -&gt; ContextResult\n</code></pre> <p>Distill relevant context from codebase based on prompt.</p> <p>This is the main method for extracting context. It analyzes your codebase, finds relevant files, ranks them by importance, and aggregates them into an optimized context that fits within token limits.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>Your query or task description. Can be plain text or a URL    to a GitHub issue, JIRA ticket, etc.</p> required <code>files</code> <code>Optional[Union[str, Path, list[Path]]]</code> <p>Paths to analyze. Can be a single path, list of paths, or None   to use current directory</p> <code>None</code> <code>format</code> <code>str</code> <p>Output format - 'markdown', 'xml' (Claude), 'json', or 'html' (interactive report)</p> <code>'markdown'</code> <code>model</code> <code>Optional[str]</code> <p>Target LLM model for token counting (e.g., 'gpt-4o', 'claude-3-opus')</p> <code>None</code> <code>max_tokens</code> <code>Optional[int]</code> <p>Maximum tokens for context (overrides model default)</p> <code>None</code> <code>mode</code> <code>str</code> <p>Analysis mode - 'fast', 'balanced', or 'thorough'</p> <code>'balanced'</code> <code>include_git</code> <code>bool</code> <p>Whether to include git context (commits, contributors, etc.)</p> <code>True</code> <code>session_name</code> <code>Optional[str]</code> <p>Session name for stateful context building</p> <code>None</code> <code>include_patterns</code> <code>Optional[list[str]]</code> <p>File patterns to include (e.g., ['.py', '.js'])</p> <code>None</code> <code>exclude_patterns</code> <code>Optional[list[str]]</code> <p>File patterns to exclude (e.g., ['test_', '.backup'])</p> <code>None</code> <code>apply_tenets</code> <code>Optional[bool]</code> <p>Whether to apply tenets (None = use config default)</p> <code>None</code> <p>Returns:</p> Type Description <code>ContextResult</code> <p>ContextResult containing the generated context, metadata, and statistics.</p> <code>ContextResult</code> <p>The metadata field includes timing information when available: metadata['timing'] = {     'duration': 2.34,  # seconds     'formatted_duration': '2.34s',  # Human-readable duration string     'start_datetime': '2024-01-15T10:30:45',     'end_datetime': '2024-01-15T10:30:47' }</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If prompt is empty or invalid</p> <code>FileNotFoundError</code> <p>If specified files don't exist</p> Example Source code in <code>tenets/__init__.py</code> Python<pre><code>def distill(\n    self,\n    prompt: str,\n    files: Optional[Union[str, Path, list[Path]]] = None,\n    *,  # Force keyword-only arguments\n    format: str = \"markdown\",\n    model: Optional[str] = None,\n    max_tokens: Optional[int] = None,\n    mode: str = \"balanced\",\n    include_git: bool = True,\n    session_name: Optional[str] = None,\n    include_patterns: Optional[list[str]] = None,\n    exclude_patterns: Optional[list[str]] = None,\n    apply_tenets: Optional[bool] = None,\n    full: bool = False,\n    condense: bool = False,\n    remove_comments: bool = False,\n    include_tests: Optional[bool] = None,\n    docstring_weight: Optional[float] = None,\n    summarize_imports: bool = True,\n) -&gt; ContextResult:\n    \"\"\"Distill relevant context from codebase based on prompt.\n\n    This is the main method for extracting context. It analyzes your codebase,\n    finds relevant files, ranks them by importance, and aggregates them into\n    an optimized context that fits within token limits.\n\n    Args:\n        prompt: Your query or task description. Can be plain text or a URL\n               to a GitHub issue, JIRA ticket, etc.\n        files: Paths to analyze. Can be a single path, list of paths, or None\n              to use current directory\n        format: Output format - 'markdown', 'xml' (Claude), 'json', or 'html' (interactive report)\n        model: Target LLM model for token counting (e.g., 'gpt-4o', 'claude-3-opus')\n        max_tokens: Maximum tokens for context (overrides model default)\n        mode: Analysis mode - 'fast', 'balanced', or 'thorough'\n        include_git: Whether to include git context (commits, contributors, etc.)\n        session_name: Session name for stateful context building\n        include_patterns: File patterns to include (e.g., ['*.py', '*.js'])\n        exclude_patterns: File patterns to exclude (e.g., ['test_*', '*.backup'])\n        apply_tenets: Whether to apply tenets (None = use config default)\n\n    Returns:\n        ContextResult containing the generated context, metadata, and statistics.\n        The metadata field includes timing information when available:\n            metadata['timing'] = {\n                'duration': 2.34,  # seconds\n                'formatted_duration': '2.34s',  # Human-readable duration string\n                'start_datetime': '2024-01-15T10:30:45',\n                'end_datetime': '2024-01-15T10:30:47'\n            }\n\n    Raises:\n        ValueError: If prompt is empty or invalid\n        FileNotFoundError: If specified files don't exist\n\n    Example:\n        &gt;&gt;&gt; # Basic usage (uses default session automatically)\n        &gt;&gt;&gt; result = tenets.distill(\"implement OAuth2 authentication\")\n        &gt;&gt;&gt; print(result.context[:100])  # First 100 chars of context\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # With specific files and options\n        &gt;&gt;&gt; result = tenets.distill(\n        ...     \"add caching layer\",\n        ...     files=\"./src\",\n        ...     mode=\"thorough\",\n        ...     max_tokens=50000,\n        ...     include_patterns=[\"*.py\"],\n        ...     exclude_patterns=[\"test_*.py\"]\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Generate HTML report\n        &gt;&gt;&gt; result = tenets.distill(\n        ...     \"analyze authentication flow\",\n        ...     format=\"html\"\n        ... )\n        &gt;&gt;&gt; Path(\"report.html\").write_text(result.context)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # With session management\n        &gt;&gt;&gt; result = tenets.distill(\n        ...     \"implement validation\",\n        ...     session_name=\"validation-feature\"\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # From GitHub issue\n        &gt;&gt;&gt; result = tenets.distill(\"https://github.com/org/repo/issues/123\")\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Access timing information\n        &gt;&gt;&gt; result = tenets.distill(\"analyze performance\")\n        &gt;&gt;&gt; if 'timing' in result.metadata:\n        ...     print(f\"Analysis took {result.metadata['timing']['formatted_duration']}\")\n        ...     # Output: \"Analysis took 2.34s\"\n    \"\"\"\n    if not prompt:\n        raise ValueError(\"Prompt cannot be empty\")\n\n    self.logger.info(f\"Distilling context for: {prompt[:100]}...\")\n\n    # Use session if specified or default session\n    session = session_name or self._session\n\n    # Run distillation\n    pinned_files = []\n    try:\n        pf_map = self.config.custom.get(\"pinned_files\", {})\n        if session and pf_map and session in pf_map:\n            pinned_files = [Path(p) for p in pf_map[session] if Path(p).exists()]\n        # Supplement from session DB metadata\n        if session and not pinned_files:\n            try:\n                from tenets.storage.session_db import SessionDB\n\n                sdb = SessionDB(self.config)\n                rec = sdb.get_session(session)\n                if rec and rec.metadata.get(\"pinned_files\"):\n                    pinned_files = [\n                        Path(p)\n                        for p in rec.metadata.get(\"pinned_files\", [])\n                        if Path(p).exists()\n                    ]\n            except Exception:  # pragma: no cover\n                pass\n    except Exception:  # pragma: no cover\n        pinned_files = []\n    result = self.distiller.distill(\n        prompt=prompt,\n        paths=files,\n        format=format,\n        model=model,\n        max_tokens=max_tokens,\n        mode=mode,\n        include_git=include_git,\n        session_name=session,\n        include_patterns=include_patterns,\n        exclude_patterns=exclude_patterns,\n        full=full,\n        condense=condense,\n        remove_comments=remove_comments,\n        pinned_files=pinned_files or None,\n        include_tests=include_tests,\n        docstring_weight=docstring_weight,\n        summarize_imports=summarize_imports,\n    )\n\n    # Inject system instruction if configured (skip for HTML reports meant for humans)\n    if format.lower() != \"html\":\n        try:\n            modified, meta = self.instiller.inject_system_instruction(\n                result.context, format=result.format, session=session\n            )\n            if meta.get(\"system_instruction_injected\"):\n                result = ContextResult(\n                    files=result.files,\n                    context=modified,\n                    format=result.format,\n                    metadata={**result.metadata, \"system_instruction\": meta},\n                )\n        except Exception:\n            # Best-effort; don't fail distill if injection fails\n            pass\n\n    # Apply tenets if configured\n    should_apply_tenets = (\n        apply_tenets if apply_tenets is not None else self.config.auto_instill_tenets\n    )\n\n    pending = None\n    if should_apply_tenets:\n        try:\n            pending = self.tenet_manager.get_pending_tenets(session)\n        except Exception:\n            pending = []\n\n    def _has_real_pending(items) -&gt; bool:\n        try:\n            return isinstance(items, list) and len(items) &gt; 0\n        except Exception:\n            return False\n\n    if should_apply_tenets and _has_real_pending(pending):\n        self.logger.info(\"Applying tenets to context\")\n        result = self.instiller.instill(\n            context=result,\n            session=session,\n            max_tenets=self.config.max_tenets_per_context,\n            inject_system_instruction=False,  # Already injected above\n        )\n\n    # Cache result\n    cache_key = f\"{prompt[:50]}_{session or 'global'}\"\n    self._cache[cache_key] = result\n\n    return result\n</code></pre>"},{"location":"api/#tenets.Tenets.distill--basic-usage-uses-default-session-automatically","title":"Basic usage (uses default session automatically)","text":"<p>result = tenets.distill(\"implement OAuth2 authentication\") print(result.context[:100])  # First 100 chars of context</p>"},{"location":"api/#tenets.Tenets.distill--with-specific-files-and-options","title":"With specific files and options","text":"<p>result = tenets.distill( ...     \"add caching layer\", ...     files=\"./src\", ...     mode=\"thorough\", ...     max_tokens=50000, ...     include_patterns=[\".py\"], ...     exclude_patterns=[\"test_.py\"] ... )</p>"},{"location":"api/#tenets.Tenets.distill--generate-html-report","title":"Generate HTML report","text":"<p>result = tenets.distill( ...     \"analyze authentication flow\", ...     format=\"html\" ... ) Path(\"report.html\").write_text(result.context)</p>"},{"location":"api/#tenets.Tenets.distill--with-session-management","title":"With session management","text":"<p>result = tenets.distill( ...     \"implement validation\", ...     session_name=\"validation-feature\" ... )</p>"},{"location":"api/#tenets.Tenets.distill--from-github-issue","title":"From GitHub issue","text":"<p>result = tenets.distill(\"https://github.com/org/repo/issues/123\")</p>"},{"location":"api/#tenets.Tenets.distill--access-timing-information","title":"Access timing information","text":"<p>result = tenets.distill(\"analyze performance\") if 'timing' in result.metadata: ...     print(f\"Analysis took {result.metadata['timing']['formatted_duration']}\") ...     # Output: \"Analysis took 2.34s\"</p>"},{"location":"api/#tenets.Tenets.rank_files","title":"rank_files","text":"Python<pre><code>rank_files(prompt: str, paths: Optional[Union[str, Path, List[Path]]] = None, *, mode: str = 'balanced', include_patterns: Optional[List[str]] = None, exclude_patterns: Optional[List[str]] = None, include_tests: Optional[bool] = None, exclude_tests: bool = False, explain: bool = False) -&gt; RankResult\n</code></pre> <p>Rank files by relevance without generating full context.</p> <p>This method uses the same sophisticated ranking pipeline as distill() but returns only the ranked files without aggregating content. Perfect for understanding which files are relevant or for automation.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>Your query or task description</p> required <code>paths</code> <code>Optional[Union[str, Path, List[Path]]]</code> <p>Paths to analyze (default: current directory)</p> <code>None</code> <code>mode</code> <code>str</code> <p>Analysis mode - 'fast', 'balanced', or 'thorough'</p> <code>'balanced'</code> <code>include_patterns</code> <code>Optional[List[str]]</code> <p>File patterns to include</p> <code>None</code> <code>exclude_patterns</code> <code>Optional[List[str]]</code> <p>File patterns to exclude</p> <code>None</code> <code>include_tests</code> <code>Optional[bool]</code> <p>Whether to include test files</p> <code>None</code> <code>exclude_tests</code> <code>bool</code> <p>Whether to exclude test files</p> <code>False</code> <code>explain</code> <code>bool</code> <p>Whether to include ranking factor explanations</p> <code>False</code> <p>Returns:</p> Type Description <code>RankResult</code> <p>RankResult containing the ranked files and metadata</p> Example <p>result = ten.rank_files(\"fix summarizing truncation bug\") for file in result.files: ...     print(f\"{file.path}: {file.relevance_score:.3f}\")</p> Source code in <code>tenets/__init__.py</code> Python<pre><code>def rank_files(\n    self,\n    prompt: str,\n    paths: Optional[Union[str, Path, List[Path]]] = None,\n    *,  # Force keyword-only arguments\n    mode: str = \"balanced\",\n    include_patterns: Optional[List[str]] = None,\n    exclude_patterns: Optional[List[str]] = None,\n    include_tests: Optional[bool] = None,\n    exclude_tests: bool = False,\n    explain: bool = False,\n) -&gt; RankResult:\n    \"\"\"Rank files by relevance without generating full context.\n\n    This method uses the same sophisticated ranking pipeline as distill()\n    but returns only the ranked files without aggregating content.\n    Perfect for understanding which files are relevant or for automation.\n\n    Args:\n        prompt: Your query or task description\n        paths: Paths to analyze (default: current directory)\n        mode: Analysis mode - 'fast', 'balanced', or 'thorough'\n        include_patterns: File patterns to include\n        exclude_patterns: File patterns to exclude\n        include_tests: Whether to include test files\n        exclude_tests: Whether to exclude test files\n        explain: Whether to include ranking factor explanations\n\n    Returns:\n        RankResult containing the ranked files and metadata\n\n    Example:\n        &gt;&gt;&gt; result = ten.rank_files(\"fix summarizing truncation bug\")\n        &gt;&gt;&gt; for file in result.files:\n        ...     print(f\"{file.path}: {file.relevance_score:.3f}\")\n    \"\"\"\n    # Use the same pipeline as distill but stop at ranking\n\n    # 1. Parse and understand the prompt\n    prompt_context = self.distiller._parse_prompt(prompt)\n\n    # Override test inclusion if explicitly specified\n    if include_tests is not None:\n        prompt_context.include_tests = include_tests\n    elif exclude_tests:\n        prompt_context.include_tests = False\n\n    # 2. Determine paths to analyze\n    paths = self.distiller._normalize_paths(paths)\n\n    # 3. Discover relevant files\n    files = self.distiller._discover_files(\n        paths=paths,\n        prompt_context=prompt_context,\n        include_patterns=include_patterns,\n        exclude_patterns=exclude_patterns,\n    )\n\n    # 4. Analyze files for structure and content\n    analyzed_files = self.distiller._analyze_files(\n        files=files, mode=mode, prompt_context=prompt_context\n    )\n\n    # 5. Rank files by relevance (this is what we want!)\n    ranked_files = self.distiller._rank_files(\n        files=analyzed_files, prompt_context=prompt_context, mode=mode\n    )\n\n    # Create result object\n    from collections import namedtuple\n\n    RankResult = namedtuple(\"RankResult\", [\"files\", \"prompt_context\", \"mode\", \"total_scanned\"])\n\n    return RankResult(\n        files=ranked_files, prompt_context=prompt_context, mode=mode, total_scanned=len(files)\n    )\n</code></pre>"},{"location":"api/#tenets.Tenets.add_tenet","title":"add_tenet","text":"Python<pre><code>add_tenet(content: str, priority: Union[str, Priority] = 'medium', category: Optional[Union[str, TenetCategory]] = None, session: Optional[str] = None, author: Optional[str] = None) -&gt; Tenet\n</code></pre> <p>Add a new guiding principle (tenet).</p> <p>Tenets are persistent instructions that get strategically injected into generated context to maintain consistency across AI interactions. They help combat context drift and ensure important principles are followed.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>The guiding principle text</p> required <code>priority</code> <code>Union[str, Priority]</code> <p>Priority level - 'low', 'medium', 'high', or 'critical'</p> <code>'medium'</code> <code>category</code> <code>Optional[Union[str, TenetCategory]]</code> <p>Optional category - 'architecture', 'security', 'style',      'performance', 'testing', 'documentation', etc.</p> <code>None</code> <code>session</code> <code>Optional[str]</code> <p>Optional session to bind this tenet to</p> <code>None</code> <code>author</code> <code>Optional[str]</code> <p>Optional author identifier</p> <code>None</code> <p>Returns:</p> Type Description <code>Tenet</code> <p>The created Tenet object</p> Example Source code in <code>tenets/__init__.py</code> Python<pre><code>def add_tenet(\n    self,\n    content: str,\n    priority: Union[str, Priority] = \"medium\",\n    category: Optional[Union[str, TenetCategory]] = None,\n    session: Optional[str] = None,\n    author: Optional[str] = None,\n) -&gt; Tenet:\n    \"\"\"Add a new guiding principle (tenet).\n\n    Tenets are persistent instructions that get strategically injected into\n    generated context to maintain consistency across AI interactions. They\n    help combat context drift and ensure important principles are followed.\n\n    Args:\n        content: The guiding principle text\n        priority: Priority level - 'low', 'medium', 'high', or 'critical'\n        category: Optional category - 'architecture', 'security', 'style',\n                 'performance', 'testing', 'documentation', etc.\n        session: Optional session to bind this tenet to\n        author: Optional author identifier\n\n    Returns:\n        The created Tenet object\n\n    Example:\n        &gt;&gt;&gt; # Add a high-priority security tenet\n        &gt;&gt;&gt; tenet = ten.add_tenet(\n        ...     \"Always validate and sanitize user input\",\n        ...     priority=\"high\",\n        ...     category=\"security\"\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Add a session-specific tenet\n        &gt;&gt;&gt; ten.add_tenet(\n        ...     \"Use async/await for all I/O operations\",\n        ...     session=\"async-refactor\"\n        ... )\n    \"\"\"\n    return self.tenet_manager.add_tenet(\n        content=content,\n        priority=priority,\n        category=category,\n        session=session or self._session,\n        author=author,\n    )\n</code></pre>"},{"location":"api/#tenets.Tenets.add_tenet--add-a-high-priority-security-tenet","title":"Add a high-priority security tenet","text":"<p>tenet = ten.add_tenet( ...     \"Always validate and sanitize user input\", ...     priority=\"high\", ...     category=\"security\" ... )</p>"},{"location":"api/#tenets.Tenets.add_tenet--add-a-session-specific-tenet","title":"Add a session-specific tenet","text":"<p>ten.add_tenet( ...     \"Use async/await for all I/O operations\", ...     session=\"async-refactor\" ... )</p>"},{"location":"api/#tenets.Tenets.instill_tenets","title":"instill_tenets","text":"Python<pre><code>instill_tenets(session: Optional[str] = None, force: bool = False) -&gt; Dict[str, Any]\n</code></pre> <p>Instill pending tenets.</p> <p>This marks tenets as active and ready to be injected into future contexts. By default, only pending tenets are instilled, but you can force re-instillation of all tenets.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Optional[str]</code> <p>Optional session to instill tenets for</p> <code>None</code> <code>force</code> <code>bool</code> <p>If True, re-instill even already instilled tenets</p> <code>False</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with instillation results including count and tenets</p> Example Source code in <code>tenets/__init__.py</code> Python<pre><code>def instill_tenets(self, session: Optional[str] = None, force: bool = False) -&gt; Dict[str, Any]:\n    \"\"\"Instill pending tenets.\n\n    This marks tenets as active and ready to be injected into future contexts.\n    By default, only pending tenets are instilled, but you can force\n    re-instillation of all tenets.\n\n    Args:\n        session: Optional session to instill tenets for\n        force: If True, re-instill even already instilled tenets\n\n    Returns:\n        Dictionary with instillation results including count and tenets\n\n    Example:\n        &gt;&gt;&gt; # Instill all pending tenets\n        &gt;&gt;&gt; result = ten.instill_tenets()\n        &gt;&gt;&gt; print(f\"Instilled {result['count']} tenets\")\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Force re-instillation\n        &gt;&gt;&gt; ten.instill_tenets(force=True)\n    \"\"\"\n    return self.tenet_manager.instill_tenets(session=session or self._session, force=force)\n</code></pre>"},{"location":"api/#tenets.Tenets.instill_tenets--instill-all-pending-tenets","title":"Instill all pending tenets","text":"<p>result = ten.instill_tenets() print(f\"Instilled {result['count']} tenets\")</p>"},{"location":"api/#tenets.Tenets.instill_tenets--force-re-instillation","title":"Force re-instillation","text":"<p>ten.instill_tenets(force=True)</p>"},{"location":"api/#tenets.Tenets.add_file_to_session","title":"add_file_to_session","text":"Python<pre><code>add_file_to_session(file_path: Union[str, Path], session: Optional[str] = None) -&gt; bool\n</code></pre> <p>Pin a single file into a session so it is prioritized in future distill calls.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Union[str, Path]</code> <p>Path to file</p> required <code>session</code> <code>Optional[str]</code> <p>Optional session name</p> <code>None</code> Source code in <code>tenets/__init__.py</code> Python<pre><code>def add_file_to_session(\n    self, file_path: Union[str, Path], session: Optional[str] = None\n) -&gt; bool:\n    \"\"\"Pin a single file into a session so it is prioritized in future distill calls.\n\n    Args:\n        file_path: Path to file\n        session: Optional session name\n    Returns:\n        True if file pinned, False otherwise\n    \"\"\"\n    path = Path(file_path)\n    if not path.exists() or not path.is_file():\n        return False\n    sess_name = session or self._session or \"default\"\n    # Attach to in-memory session context held by session manager if available\n    try:\n        from tenets.core.session.session import SessionManager  # local import\n\n        # There may or may not be a global session manager; instantiate lightweight if needed\n    except Exception:  # pragma: no cover\n        pass\n    # For now store pinned files in config.custom for persistence stub\n    if \"pinned_files\" not in self.config.custom:\n        self.config.custom[\"pinned_files\"] = {}\n    self.config.custom[\"pinned_files\"].setdefault(sess_name, set())\n    resolved = str(path.resolve())\n    self.config.custom[\"pinned_files\"][sess_name].add(resolved)\n    # Persist in session DB metadata if available\n    try:\n        from tenets.storage.session_db import SessionDB  # local import\n\n        sdb = SessionDB(self.config)\n        # Read current metadata and merge\n        rec = sdb.get_session(sess_name)\n        existing = rec.metadata.get(\"pinned_files\") if rec else []\n        if isinstance(existing, list):\n            if resolved not in existing:\n                existing.append(resolved)\n        else:\n            existing = [resolved]\n        sdb.update_session_metadata(sess_name, {\"pinned_files\": existing})\n    except Exception:  # pragma: no cover - best effort\n        pass\n    return True\n</code></pre>"},{"location":"api/#tenets.Tenets.add_folder_to_session","title":"add_folder_to_session","text":"Python<pre><code>add_folder_to_session(folder_path: Union[str, Path], session: Optional[str] = None, include_patterns: Optional[list[str]] = None, exclude_patterns: Optional[list[str]] = None, respect_gitignore: bool = True, recursive: bool = True) -&gt; int\n</code></pre> <p>Pin all files in a folder (optionally filtered) into a session.</p> <p>Parameters:</p> Name Type Description Default <code>folder_path</code> <code>Union[str, Path]</code> <p>Directory to scan</p> required <code>session</code> <code>Optional[str]</code> <p>Session name</p> <code>None</code> <code>include_patterns</code> <code>Optional[list[str]]</code> <p>Include filter</p> <code>None</code> <code>exclude_patterns</code> <code>Optional[list[str]]</code> <p>Exclude filter</p> <code>None</code> <code>respect_gitignore</code> <code>bool</code> <p>Respect .gitignore</p> <code>True</code> <code>recursive</code> <code>bool</code> <p>Recurse into subdirectories</p> <code>True</code> Source code in <code>tenets/__init__.py</code> Python<pre><code>def add_folder_to_session(\n    self,\n    folder_path: Union[str, Path],\n    session: Optional[str] = None,\n    include_patterns: Optional[list[str]] = None,\n    exclude_patterns: Optional[list[str]] = None,\n    respect_gitignore: bool = True,\n    recursive: bool = True,\n) -&gt; int:\n    \"\"\"Pin all files in a folder (optionally filtered) into a session.\n\n    Args:\n        folder_path: Directory to scan\n        session: Session name\n        include_patterns: Include filter\n        exclude_patterns: Exclude filter\n        respect_gitignore: Respect .gitignore\n        recursive: Recurse into subdirectories\n    Returns:\n        Count of files pinned.\n    \"\"\"\n    root = Path(folder_path)\n    if not root.exists() or not root.is_dir():\n        return 0\n    from tenets.utils.scanner import FileScanner\n\n    scanner = FileScanner(self.config)\n    paths = [root]\n    files = scanner.scan(\n        paths,\n        include_patterns=include_patterns,\n        exclude_patterns=exclude_patterns,\n        follow_symlinks=False,\n        respect_gitignore=respect_gitignore,\n    )\n    count = 0\n    for f in files:\n        if self.add_file_to_session(f, session=session):\n            count += 1\n    return count\n</code></pre>"},{"location":"api/#tenets.Tenets.list_tenets","title":"list_tenets","text":"Python<pre><code>list_tenets(pending_only: bool = False, instilled_only: bool = False, session: Optional[str] = None, category: Optional[Union[str, TenetCategory]] = None) -&gt; list[dict[str, Any]]\n</code></pre> <p>List tenets with optional filtering.</p> <p>Parameters:</p> Name Type Description Default <code>pending_only</code> <code>bool</code> <p>Only show pending (not yet instilled) tenets</p> <code>False</code> <code>instilled_only</code> <code>bool</code> <p>Only show instilled tenets</p> <code>False</code> <code>session</code> <code>Optional[str]</code> <p>Filter by session binding</p> <code>None</code> <code>category</code> <code>Optional[Union[str, TenetCategory]]</code> <p>Filter by category</p> <code>None</code> <p>Returns:</p> Type Description <code>list[dict[str, Any]]</code> <p>List of tenet dictionaries</p> Example Source code in <code>tenets/__init__.py</code> Python<pre><code>def list_tenets(\n    self,\n    pending_only: bool = False,\n    instilled_only: bool = False,\n    session: Optional[str] = None,\n    category: Optional[Union[str, TenetCategory]] = None,\n) -&gt; list[dict[str, Any]]:\n    \"\"\"List tenets with optional filtering.\n\n    Args:\n        pending_only: Only show pending (not yet instilled) tenets\n        instilled_only: Only show instilled tenets\n        session: Filter by session binding\n        category: Filter by category\n\n    Returns:\n        List of tenet dictionaries\n\n    Example:\n        &gt;&gt;&gt; # List all tenets\n        &gt;&gt;&gt; all_tenets = ten.list_tenets()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # List only pending security tenets\n        &gt;&gt;&gt; pending_security = ten.list_tenets(\n        ...     pending_only=True,\n        ...     category=\"security\"\n        ... )\n    \"\"\"\n    return self.tenet_manager.list_tenets(\n        pending_only=pending_only,\n        instilled_only=instilled_only,\n        session=session or self._session,\n        category=category,\n    )\n</code></pre>"},{"location":"api/#tenets.Tenets.list_tenets--list-all-tenets","title":"List all tenets","text":"<p>all_tenets = ten.list_tenets()</p>"},{"location":"api/#tenets.Tenets.list_tenets--list-only-pending-security-tenets","title":"List only pending security tenets","text":"<p>pending_security = ten.list_tenets( ...     pending_only=True, ...     category=\"security\" ... )</p>"},{"location":"api/#tenets.Tenets.get_tenet","title":"get_tenet","text":"Python<pre><code>get_tenet(tenet_id: str) -&gt; Optional[Tenet]\n</code></pre> <p>Get a specific tenet by ID.</p> <p>Parameters:</p> Name Type Description Default <code>tenet_id</code> <code>str</code> <p>Tenet ID (can be partial)</p> required <p>Returns:</p> Type Description <code>Optional[Tenet]</code> <p>The Tenet object or None if not found</p> Source code in <code>tenets/__init__.py</code> Python<pre><code>def get_tenet(self, tenet_id: str) -&gt; Optional[Tenet]:\n    \"\"\"Get a specific tenet by ID.\n\n    Args:\n        tenet_id: Tenet ID (can be partial)\n\n    Returns:\n        The Tenet object or None if not found\n    \"\"\"\n    return self.tenet_manager.get_tenet(tenet_id)\n</code></pre>"},{"location":"api/#tenets.Tenets.remove_tenet","title":"remove_tenet","text":"Python<pre><code>remove_tenet(tenet_id: str) -&gt; bool\n</code></pre> <p>Remove (archive) a tenet.</p> <p>Parameters:</p> Name Type Description Default <code>tenet_id</code> <code>str</code> <p>Tenet ID (can be partial)</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if removed, False if not found</p> Source code in <code>tenets/__init__.py</code> Python<pre><code>def remove_tenet(self, tenet_id: str) -&gt; bool:\n    \"\"\"Remove (archive) a tenet.\n\n    Args:\n        tenet_id: Tenet ID (can be partial)\n\n    Returns:\n        True if removed, False if not found\n    \"\"\"\n    return self.tenet_manager.remove_tenet(tenet_id)\n</code></pre>"},{"location":"api/#tenets.Tenets.get_pending_tenets","title":"get_pending_tenets","text":"Python<pre><code>get_pending_tenets(session: Optional[str] = None) -&gt; List[Tenet]\n</code></pre> <p>Get all pending tenets.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Optional[str]</code> <p>Optional session filter</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Tenet]</code> <p>List of pending Tenet objects</p> Source code in <code>tenets/__init__.py</code> Python<pre><code>def get_pending_tenets(self, session: Optional[str] = None) -&gt; List[Tenet]:\n    \"\"\"Get all pending tenets.\n\n    Args:\n        session: Optional session filter\n\n    Returns:\n        List of pending Tenet objects\n    \"\"\"\n    return self.tenet_manager.get_pending_tenets(session or self._session)\n</code></pre>"},{"location":"api/#tenets.Tenets.export_tenets","title":"export_tenets","text":"Python<pre><code>export_tenets(format: str = 'yaml', session: Optional[str] = None) -&gt; str\n</code></pre> <p>Export tenets to YAML or JSON.</p> <p>Parameters:</p> Name Type Description Default <code>format</code> <code>str</code> <p>Export format - 'yaml' or 'json'</p> <code>'yaml'</code> <code>session</code> <code>Optional[str]</code> <p>Optional session filter</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Serialized tenets string</p> Source code in <code>tenets/__init__.py</code> Python<pre><code>def export_tenets(self, format: str = \"yaml\", session: Optional[str] = None) -&gt; str:\n    \"\"\"Export tenets to YAML or JSON.\n\n    Args:\n        format: Export format - 'yaml' or 'json'\n        session: Optional session filter\n\n    Returns:\n        Serialized tenets string\n    \"\"\"\n    return self.tenet_manager.export_tenets(format=format, session=session or self._session)\n</code></pre>"},{"location":"api/#tenets.Tenets.import_tenets","title":"import_tenets","text":"Python<pre><code>import_tenets(file_path: Union[str, Path], session: Optional[str] = None) -&gt; int\n</code></pre> <p>Import tenets from file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Union[str, Path]</code> <p>Path to import file (YAML or JSON)</p> required <code>session</code> <code>Optional[str]</code> <p>Optional session to bind imported tenets to</p> <code>None</code> <p>Returns:</p> Type Description <code>int</code> <p>Number of tenets imported</p> Source code in <code>tenets/__init__.py</code> Python<pre><code>def import_tenets(self, file_path: Union[str, Path], session: Optional[str] = None) -&gt; int:\n    \"\"\"Import tenets from file.\n\n    Args:\n        file_path: Path to import file (YAML or JSON)\n        session: Optional session to bind imported tenets to\n\n    Returns:\n        Number of tenets imported\n    \"\"\"\n    return self.tenet_manager.import_tenets(\n        file_path=file_path, session=session or self._session\n    )\n</code></pre>"},{"location":"api/#tenets.Tenets.examine","title":"examine","text":"Python<pre><code>examine(path: Optional[Union[str, Path]] = None, deep: bool = False, include_git: bool = True, output_metadata: bool = False) -&gt; Any\n</code></pre> <p>Examine codebase structure and metrics.</p> <p>Provides detailed analysis of your code including file counts, language distribution, complexity metrics, and potential issues.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Optional[Union[str, Path]]</code> <p>Path to examine (default: current directory)</p> <code>None</code> <code>deep</code> <code>bool</code> <p>Perform deep analysis with AST parsing</p> <code>False</code> <code>include_git</code> <code>bool</code> <p>Include git statistics</p> <code>True</code> <code>output_metadata</code> <code>bool</code> <p>Include detailed metadata in result</p> <code>False</code> <p>Returns:</p> Type Description <code>Any</code> <p>AnalysisResult object with comprehensive codebase analysis</p> Example Source code in <code>tenets/__init__.py</code> Python<pre><code>def examine(\n    self,\n    path: Optional[Union[str, Path]] = None,\n    deep: bool = False,\n    include_git: bool = True,\n    output_metadata: bool = False,\n) -&gt; Any:  # Returns AnalysisResult\n    \"\"\"Examine codebase structure and metrics.\n\n    Provides detailed analysis of your code including file counts, language\n    distribution, complexity metrics, and potential issues.\n\n    Args:\n        path: Path to examine (default: current directory)\n        deep: Perform deep analysis with AST parsing\n        include_git: Include git statistics\n        output_metadata: Include detailed metadata in result\n\n    Returns:\n        AnalysisResult object with comprehensive codebase analysis\n\n    Example:\n        &gt;&gt;&gt; # Basic examination\n        &gt;&gt;&gt; analysis = ten.examine()\n        &gt;&gt;&gt; print(f\"Found {analysis.total_files} files\")\n        &gt;&gt;&gt; print(f\"Languages: {', '.join(analysis.languages)}\")\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Deep analysis with git\n        &gt;&gt;&gt; analysis = ten.examine(deep=True, include_git=True)\n    \"\"\"\n    # This would call the analyzer module (not shown in detail here)\n    # Placeholder for now\n    from tenets.core.analysis import CodeAnalyzer\n\n    analyzer = CodeAnalyzer(self.config)\n\n    # Would return proper AnalysisResult\n    return {\n        \"total_files\": 0,\n        \"languages\": [],\n        \"message\": \"Examine functionality to be implemented\",\n    }\n</code></pre>"},{"location":"api/#tenets.Tenets.examine--basic-examination","title":"Basic examination","text":"<p>analysis = ten.examine() print(f\"Found {analysis.total_files} files\") print(f\"Languages: {', '.join(analysis.languages)}\")</p>"},{"location":"api/#tenets.Tenets.examine--deep-analysis-with-git","title":"Deep analysis with git","text":"<p>analysis = ten.examine(deep=True, include_git=True)</p>"},{"location":"api/#tenets.Tenets.track_changes","title":"track_changes","text":"Python<pre><code>track_changes(path: Optional[Union[str, Path]] = None, since: str = '1 week', author: Optional[str] = None, file_pattern: Optional[str] = None) -&gt; Dict[str, Any]\n</code></pre> <p>Track code changes over time.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Optional[Union[str, Path]]</code> <p>Repository path (default: current directory)</p> <code>None</code> <code>since</code> <code>str</code> <p>Time period (e.g., '1 week', '3 days', 'yesterday')</p> <code>'1 week'</code> <code>author</code> <code>Optional[str]</code> <p>Filter by author</p> <code>None</code> <code>file_pattern</code> <code>Optional[str]</code> <p>Filter by file pattern</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with change information</p> Source code in <code>tenets/__init__.py</code> Python<pre><code>def track_changes(\n    self,\n    path: Optional[Union[str, Path]] = None,\n    since: str = \"1 week\",\n    author: Optional[str] = None,\n    file_pattern: Optional[str] = None,\n) -&gt; Dict[str, Any]:\n    \"\"\"Track code changes over time.\n\n    Args:\n        path: Repository path (default: current directory)\n        since: Time period (e.g., '1 week', '3 days', 'yesterday')\n        author: Filter by author\n        file_pattern: Filter by file pattern\n\n    Returns:\n        Dictionary with change information\n    \"\"\"\n    # Placeholder - would integrate with git module\n    return {\n        \"commits\": [],\n        \"files\": [],\n        \"message\": \"Track changes functionality to be implemented\",\n    }\n</code></pre>"},{"location":"api/#tenets.Tenets.momentum","title":"momentum","text":"Python<pre><code>momentum(path: Optional[Union[str, Path]] = None, since: str = 'last-month', team: bool = False, author: Optional[str] = None) -&gt; Dict[str, Any]\n</code></pre> <p>Track development momentum and velocity.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Optional[Union[str, Path]]</code> <p>Repository path</p> <code>None</code> <code>since</code> <code>str</code> <p>Time period to analyze</p> <code>'last-month'</code> <code>team</code> <code>bool</code> <p>Show team-wide statistics</p> <code>False</code> <code>author</code> <code>Optional[str]</code> <p>Show stats for specific author</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with momentum metrics</p> Source code in <code>tenets/__init__.py</code> Python<pre><code>def momentum(\n    self,\n    path: Optional[Union[str, Path]] = None,\n    since: str = \"last-month\",\n    team: bool = False,\n    author: Optional[str] = None,\n) -&gt; Dict[str, Any]:\n    \"\"\"Track development momentum and velocity.\n\n    Args:\n        path: Repository path\n        since: Time period to analyze\n        team: Show team-wide statistics\n        author: Show stats for specific author\n\n    Returns:\n        Dictionary with momentum metrics\n    \"\"\"\n    # Placeholder - would integrate with git analyzer\n    return {\"overall\": {}, \"weekly\": [], \"message\": \"Momentum functionality to be implemented\"}\n</code></pre>"},{"location":"api/#tenets.Tenets.estimate_cost","title":"estimate_cost","text":"Python<pre><code>estimate_cost(result: ContextResult, model: str) -&gt; Dict[str, Any]\n</code></pre> <p>Estimate the cost of using generated context with an LLM.</p> <p>Parameters:</p> Name Type Description Default <code>result</code> <code>ContextResult</code> <p>ContextResult from distill()</p> required <code>model</code> <code>str</code> <p>Target model name</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with token counts and cost estimates</p> Source code in <code>tenets/__init__.py</code> Python<pre><code>def estimate_cost(self, result: ContextResult, model: str) -&gt; Dict[str, Any]:\n    \"\"\"Estimate the cost of using generated context with an LLM.\n\n    Args:\n        result: ContextResult from distill()\n        model: Target model name\n\n    Returns:\n        Dictionary with token counts and cost estimates\n    \"\"\"\n    from tenets.models.llm import estimate_cost as _estimate_cost\n    from tenets.models.llm import get_model_limits\n\n    input_tokens = result.token_count\n    # Use a conservative default for expected output if not specified elsewhere\n    default_output = get_model_limits(model).max_output\n    return _estimate_cost(input_tokens=input_tokens, output_tokens=default_output, model=model)\n</code></pre>"},{"location":"api/#tenets.Tenets.set_system_instruction","title":"set_system_instruction","text":"Python<pre><code>set_system_instruction(instruction: str, enable: bool = True, position: str = 'top', format: str = 'markdown', save: bool = False) -&gt; None\n</code></pre> <p>Set the system instruction for AI interactions.</p> <p>Parameters:</p> Name Type Description Default <code>instruction</code> <code>str</code> <p>The system instruction text</p> required <code>enable</code> <code>bool</code> <p>Whether to auto-inject</p> <code>True</code> <code>position</code> <code>str</code> <p>Where to inject ('top', 'after_header', 'before_content')</p> <code>'top'</code> <code>format</code> <code>str</code> <p>Format type ('markdown', 'xml', 'comment', 'plain')</p> <code>'markdown'</code> <code>save</code> <code>bool</code> <p>Whether to save to config file</p> <code>False</code> Source code in <code>tenets/__init__.py</code> Python<pre><code>def set_system_instruction(\n    self,\n    instruction: str,\n    enable: bool = True,\n    position: str = \"top\",\n    format: str = \"markdown\",\n    save: bool = False,\n) -&gt; None:\n    \"\"\"Set the system instruction for AI interactions.\n\n    Args:\n        instruction: The system instruction text\n        enable: Whether to auto-inject\n        position: Where to inject ('top', 'after_header', 'before_content')\n        format: Format type ('markdown', 'xml', 'comment', 'plain')\n        save: Whether to save to config file\n    \"\"\"\n    self.config.tenet.system_instruction = instruction\n    self.config.tenet.system_instruction_enabled = enable\n    self.config.tenet.system_instruction_position = position\n    self.config.tenet.system_instruction_format = format\n\n    if save and getattr(self.config, \"config_file\", None):\n        self.config.save()\n\n    self.logger.info(f\"System instruction set ({len(instruction)} chars)\")\n</code></pre>"},{"location":"api/#tenets.Tenets.get_system_instruction","title":"get_system_instruction","text":"Python<pre><code>get_system_instruction() -&gt; Optional[str]\n</code></pre> <p>Get the current system instruction.</p> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>The system instruction text or None</p> Source code in <code>tenets/__init__.py</code> Python<pre><code>def get_system_instruction(self) -&gt; Optional[str]:\n    \"\"\"Get the current system instruction.\n\n    Returns:\n        The system instruction text or None\n    \"\"\"\n    return self.config.tenet.system_instruction\n</code></pre>"},{"location":"api/#tenets.Tenets.clear_system_instruction","title":"clear_system_instruction","text":"Python<pre><code>clear_system_instruction(save: bool = False) -&gt; None\n</code></pre> <p>Clear the system instruction.</p> <p>Parameters:</p> Name Type Description Default <code>save</code> <code>bool</code> <p>Whether to save to config file</p> <code>False</code> Source code in <code>tenets/__init__.py</code> Python<pre><code>def clear_system_instruction(self, save: bool = False) -&gt; None:\n    \"\"\"Clear the system instruction.\n\n    Args:\n        save: Whether to save to config file\n    \"\"\"\n    self.config.tenet.system_instruction = None\n    self.config.tenet.system_instruction_enabled = False\n\n    if save and getattr(self.config, \"config_file\", None):\n        self.config.save()\n\n    self.logger.info(\"System instruction cleared\")\n</code></pre>"},{"location":"api/#core-modules","title":"Core modules","text":""},{"location":"api/#tenetscore","title":"tenets.core","text":""},{"location":"api/#tenets.core","title":"core","text":"<p>Core subsystem of Tenets.</p> <p>This package aggregates core functionality such as analysis, distillation, ranking, sessions, and related utilities.</p> <p>It exposes a stable import path for documentation and users: - tenets.core.analysis - tenets.core.ranking - tenets.core.session - tenets.core.instiller - tenets.core.git - tenets.core.summarizer</p>"},{"location":"api/#tenetscoreanalysis","title":"tenets.core.analysis","text":""},{"location":"api/#tenets.core.analysis","title":"analysis","text":"<p>Analysis package.</p> <p>Re-exports the main CodeAnalyzer after directory reorganization.</p> <p>This module intentionally re-exports <code>CodeAnalyzer</code> so callers can import <code>tenets.core.analysis.CodeAnalyzer</code>. The implementation lives in <code>analyzer.py</code> and does not import this package-level module, so exposing the symbol here will not create a circular import.</p>"},{"location":"api/#tenets.core.analysis-classes","title":"Classes","text":""},{"location":"api/#tenetscoreranking","title":"tenets.core.ranking","text":""},{"location":"api/#tenets.core.ranking","title":"ranking","text":"<p>Relevance ranking system for Tenets.</p> <p>This package provides sophisticated file ranking capabilities using multiple strategies from simple keyword matching to advanced ML-based semantic analysis. The ranking system is designed to efficiently identify the most relevant files for a given prompt or query.</p> <p>Main components: - RelevanceRanker: Main orchestrator for ranking operations - RankingFactors: Comprehensive factors used for scoring - RankedFile: File with ranking information - Ranking strategies: Fast, Balanced, Thorough, ML - TF-IDF and BM25 calculators for text similarity</p> Example usage <p>from tenets.core.ranking import RelevanceRanker, create_ranker from tenets.models.context import PromptContext</p>"},{"location":"api/#tenets.core.ranking--create-ranker-with-config","title":"Create ranker with config","text":"<p>ranker = create_ranker(algorithm=\"balanced\")</p>"},{"location":"api/#tenets.core.ranking--parse-prompt","title":"Parse prompt","text":"<p>prompt_context = PromptContext(text=\"implement OAuth authentication\")</p>"},{"location":"api/#tenets.core.ranking--rank-files","title":"Rank files","text":"<p>ranked_files = ranker.rank_files(files, prompt_context)</p>"},{"location":"api/#tenets.core.ranking--get-top-relevant-files","title":"Get top relevant files","text":"<p>for file in ranked_files[:10]: ...     print(f\"{file.path}: {file.relevance_score:.3f}\")</p>"},{"location":"api/#tenets.core.ranking-classes","title":"Classes","text":""},{"location":"api/#tenets.core.ranking.BM25Calculator","title":"BM25Calculator","text":"Python<pre><code>BM25Calculator(k1: float = 1.2, b: float = 0.75, epsilon: float = 0.25, use_stopwords: bool = False, stopword_set: str = 'code')\n</code></pre> <p>BM25 ranking algorithm with advanced features for code search.</p> This implementation provides <ul> <li>Configurable term saturation (k1) and length normalization (b)</li> <li>Efficient tokenization with optional stopword filtering</li> <li>IDF caching for performance</li> <li>Support for incremental corpus updates</li> <li>Query expansion capabilities</li> <li>Detailed scoring explanations for debugging</li> </ul> <p>Attributes:</p> Name Type Description <code>k1</code> <code>float</code> <p>Controls term frequency saturation. Higher values mean        less saturation (more weight to term frequency).        Typical range: 0.5-2.0, default: 1.2</p> <code>b</code> <code>float</code> <p>Controls document length normalization.       0 = no normalization, 1 = full normalization.       Typical range: 0.5-0.8, default: 0.75</p> <code>epsilon</code> <code>float</code> <p>Small constant to prevent division by zero</p> <p>Initialize BM25 calculator with configurable parameters.</p> <p>Parameters:</p> Name Type Description Default <code>k1</code> <code>float</code> <p>Term frequency saturation parameter. Lower values (0.5-1.0) work well for short queries, higher values (1.5-2.0) for longer queries. Default: 1.2 (good general purpose value)</p> <code>1.2</code> <code>b</code> <code>float</code> <p>Length normalization parameter. Set to 0 to disable length normalization, 1 for full normalization. Default: 0.75 (moderate normalization, good for mixed-length documents)</p> <code>0.75</code> <code>epsilon</code> <code>float</code> <p>Small constant for numerical stability</p> <code>0.25</code> <code>use_stopwords</code> <code>bool</code> <p>Whether to filter common words</p> <code>False</code> <code>stopword_set</code> <code>str</code> <p>Which stopword set to use ('code' for programming,          'english' for natural language)</p> <code>'code'</code>"},{"location":"api/#tenets.core.ranking.BM25Calculator-functions","title":"Functions","text":""},{"location":"api/#tenets.core.ranking.BM25Calculator.tokenize","title":"tokenize","text":"Python<pre><code>tokenize(text: str) -&gt; List[str]\n</code></pre> <p>Tokenize text using code-aware tokenizer.</p> Handles various code constructs <ul> <li>CamelCase and snake_case splitting</li> <li>Preservation of important symbols</li> <li>Number and identifier extraction</li> </ul> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Input text to tokenize</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List of tokens, lowercased and filtered</p>"},{"location":"api/#tenets.core.ranking.BM25Calculator.add_document","title":"add_document","text":"Python<pre><code>add_document(doc_id: str, text: str) -&gt; None\n</code></pre> <p>Add a document to the BM25 corpus.</p> <p>Updates all corpus statistics including document frequency, average document length, and vocabulary.</p> <p>Parameters:</p> Name Type Description Default <code>doc_id</code> <code>str</code> <p>Unique identifier for the document</p> required <code>text</code> <code>str</code> <p>Document content</p> required Note <p>Adding documents invalidates the IDF and score caches. For bulk loading, use build_corpus() instead.</p>"},{"location":"api/#tenets.core.ranking.BM25Calculator.build_corpus","title":"build_corpus","text":"Python<pre><code>build_corpus(documents: List[Tuple[str, str]]) -&gt; None\n</code></pre> <p>Build BM25 corpus from multiple documents efficiently.</p> <p>More efficient than repeated add_document() calls as it calculates statistics once at the end.</p> <p>Parameters:</p> Name Type Description Default <code>documents</code> <code>List[Tuple[str, str]]</code> <p>List of (doc_id, text) tuples</p> required Example <p>documents = [ ...     (\"file1.py\", \"import os\\nclass FileHandler\"), ...     (\"file2.py\", \"from pathlib import Path\") ... ] bm25.build_corpus(documents)</p>"},{"location":"api/#tenets.core.ranking.BM25Calculator.compute_idf","title":"compute_idf","text":"Python<pre><code>compute_idf(term: str) -&gt; float\n</code></pre> <p>Compute IDF (Inverse Document Frequency) for a term.</p> <p>Uses the standard BM25 IDF formula with smoothing to handle edge cases and prevent negative values.</p> Formula <p>IDF(term) = log[(N - df + 0.5) / (df + 0.5) + 1]</p> <p>Parameters:</p> Name Type Description Default <code>term</code> <code>str</code> <p>Term to compute IDF for</p> required <p>Returns:</p> Type Description <code>float</code> <p>IDF value (always positive due to +1 in formula)</p>"},{"location":"api/#tenets.core.ranking.BM25Calculator.score_document","title":"score_document","text":"Python<pre><code>score_document(query_tokens: List[str], doc_id: str, explain: bool = False) -&gt; float\n</code></pre> <p>Calculate BM25 score for a document given query tokens.</p> <p>Implements the full BM25 scoring formula with term saturation and length normalization.</p> <p>Parameters:</p> Name Type Description Default <code>query_tokens</code> <code>List[str]</code> <p>Tokenized query terms</p> required <code>doc_id</code> <code>str</code> <p>Document identifier to score</p> required <code>explain</code> <code>bool</code> <p>If True, return detailed scoring breakdown</p> <code>False</code> <p>Returns:</p> Type Description <code>float</code> <p>BM25 score (higher is more relevant)</p> <code>float</code> <p>If explain=True, returns tuple of (score, explanation_dict)</p>"},{"location":"api/#tenets.core.ranking.BM25Calculator.get_scores","title":"get_scores","text":"Python<pre><code>get_scores(query: str, doc_ids: Optional[List[str]] = None) -&gt; List[Tuple[str, float]]\n</code></pre> <p>Get BM25 scores for all documents or a subset.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>Search query string</p> required <code>doc_ids</code> <code>Optional[List[str]]</code> <p>Optional list of document IDs to score.     If None, scores all documents.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Tuple[str, float]]</code> <p>List of (doc_id, score) tuples sorted by score (descending)</p>"},{"location":"api/#tenets.core.ranking.BM25Calculator.get_top_k","title":"get_top_k","text":"Python<pre><code>get_top_k(query: str, k: int = 10, threshold: float = 0.0) -&gt; List[Tuple[str, float]]\n</code></pre> <p>Get top-k documents by BM25 score.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>Search query</p> required <code>k</code> <code>int</code> <p>Number of top documents to return</p> <code>10</code> <code>threshold</code> <code>float</code> <p>Minimum score threshold (documents below are filtered)</p> <code>0.0</code> <p>Returns:</p> Type Description <code>List[Tuple[str, float]]</code> <p>List of top-k (doc_id, score) tuples</p>"},{"location":"api/#tenets.core.ranking.BM25Calculator.compute_similarity","title":"compute_similarity","text":"Python<pre><code>compute_similarity(query: str, doc_id: str) -&gt; float\n</code></pre> <p>Compute normalized similarity score between query and document.</p> <p>Returns a value between 0 and 1 for consistency with other similarity measures.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>Query text</p> required <code>doc_id</code> <code>str</code> <p>Document identifier</p> required <p>Returns:</p> Type Description <code>float</code> <p>Normalized similarity score (0-1)</p>"},{"location":"api/#tenets.core.ranking.BM25Calculator.explain_score","title":"explain_score","text":"Python<pre><code>explain_score(query: str, doc_id: str) -&gt; Dict\n</code></pre> <p>Get detailed explanation of BM25 scoring for debugging.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>Query text</p> required <code>doc_id</code> <code>str</code> <p>Document to explain scoring for</p> required <p>Returns:</p> Type Description <code>Dict</code> <p>Dictionary with detailed scoring breakdown</p>"},{"location":"api/#tenets.core.ranking.BM25Calculator.get_stats","title":"get_stats","text":"Python<pre><code>get_stats() -&gt; Dict\n</code></pre> <p>Get calculator statistics for monitoring.</p> <p>Returns:</p> Type Description <code>Dict</code> <p>Dictionary with usage statistics</p>"},{"location":"api/#tenets.core.ranking.BM25Calculator.clear_cache","title":"clear_cache","text":"Python<pre><code>clear_cache() -&gt; None\n</code></pre> <p>Clear all caches to free memory.</p>"},{"location":"api/#tenets.core.ranking.TFIDFCalculator","title":"TFIDFCalculator","text":"Python<pre><code>TFIDFCalculator(use_stopwords: bool = False)\n</code></pre> <p>TF-IDF calculator for ranking.</p> <p>Simplified wrapper around NLP TFIDFCalculator to maintain existing ranking API while using centralized logic.</p> <p>Initialize TF-IDF calculator.</p> <p>Parameters:</p> Name Type Description Default <code>use_stopwords</code> <code>bool</code> <p>Whether to filter stopwords (uses 'code' set)</p> <code>False</code>"},{"location":"api/#tenets.core.ranking.TFIDFCalculator-attributes","title":"Attributes","text":""},{"location":"api/#tenets.core.ranking.TFIDFCalculator.document_vectors","title":"document_vectors  <code>property</code>","text":"Python<pre><code>document_vectors: Dict[str, Dict[str, float]]\n</code></pre> <p>Get document vectors.</p>"},{"location":"api/#tenets.core.ranking.TFIDFCalculator.document_norms","title":"document_norms  <code>property</code>","text":"Python<pre><code>document_norms: Dict[str, float]\n</code></pre> <p>Get document vector norms.</p>"},{"location":"api/#tenets.core.ranking.TFIDFCalculator.vocabulary","title":"vocabulary  <code>property</code>","text":"Python<pre><code>vocabulary: set\n</code></pre> <p>Get vocabulary.</p>"},{"location":"api/#tenets.core.ranking.TFIDFCalculator-functions","title":"Functions","text":""},{"location":"api/#tenets.core.ranking.TFIDFCalculator.tokenize","title":"tokenize","text":"Python<pre><code>tokenize(text: str) -&gt; List[str]\n</code></pre> <p>Tokenize text using NLP tokenizer.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Input text</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List of tokens</p>"},{"location":"api/#tenets.core.ranking.TFIDFCalculator.add_document","title":"add_document","text":"Python<pre><code>add_document(doc_id: str, text: str) -&gt; Dict[str, float]\n</code></pre> <p>Add document to corpus.</p> <p>Parameters:</p> Name Type Description Default <code>doc_id</code> <code>str</code> <p>Document identifier</p> required <code>text</code> <code>str</code> <p>Document content</p> required <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>TF-IDF vector for document</p>"},{"location":"api/#tenets.core.ranking.TFIDFCalculator.compute_similarity","title":"compute_similarity","text":"Python<pre><code>compute_similarity(query_text: str, doc_id: str) -&gt; float\n</code></pre> <p>Compute similarity between query and document.</p> <p>Parameters:</p> Name Type Description Default <code>query_text</code> <code>str</code> <p>Query text</p> required <code>doc_id</code> <code>str</code> <p>Document identifier</p> required <p>Returns:</p> Type Description <code>float</code> <p>Cosine similarity score (0-1)</p>"},{"location":"api/#tenets.core.ranking.TFIDFCalculator.get_top_terms","title":"get_top_terms","text":"Python<pre><code>get_top_terms(doc_id: str, n: int = 10) -&gt; List[Tuple[str, float]]\n</code></pre> <p>Return the top-n TF-IDF terms for a given document.</p> <p>Parameters:</p> Name Type Description Default <code>doc_id</code> <code>str</code> <p>Document identifier</p> required <code>n</code> <code>int</code> <p>Maximum number of terms to return</p> <code>10</code> <p>Returns:</p> Type Description <code>List[Tuple[str, float]]</code> <p>List of (term, score) sorted by score descending</p>"},{"location":"api/#tenets.core.ranking.TFIDFCalculator.build_corpus","title":"build_corpus","text":"Python<pre><code>build_corpus(documents: List[Tuple[str, str]]) -&gt; None\n</code></pre> <p>Build corpus from documents.</p> <p>Parameters:</p> Name Type Description Default <code>documents</code> <code>List[Tuple[str, str]]</code> <p>List of (doc_id, text) tuples</p> required"},{"location":"api/#tenets.core.ranking.FactorWeight","title":"FactorWeight","text":"<p>               Bases: <code>Enum</code></p> <p>Standard weight presets for ranking factors.</p> <p>These presets provide balanced weights for different use cases. Can be overridden with custom weights in configuration.</p>"},{"location":"api/#tenets.core.ranking.RankedFile","title":"RankedFile  <code>dataclass</code>","text":"Python<pre><code>RankedFile(analysis: FileAnalysis, score: float, factors: RankingFactors, explanation: str = '', confidence: float = 1.0, rank: Optional[int] = None, metadata: Dict[str, Any] = dict())\n</code></pre> <p>A file with its relevance ranking.</p> <p>Combines a FileAnalysis with ranking scores and metadata. Provides utilities for comparison, explanation generation, and result formatting.</p> <p>Attributes:</p> Name Type Description <code>analysis</code> <code>FileAnalysis</code> <p>The FileAnalysis object</p> <code>score</code> <code>float</code> <p>Overall relevance score (0-1)</p> <code>factors</code> <code>RankingFactors</code> <p>Detailed ranking factors</p> <code>explanation</code> <code>str</code> <p>Human-readable ranking explanation</p> <code>confidence</code> <code>float</code> <p>Confidence in the ranking (0-1)</p> <code>rank</code> <code>Optional[int]</code> <p>Position in ranked list (1-based)</p> <code>metadata</code> <code>Dict[str, Any]</code> <p>Additional ranking metadata</p>"},{"location":"api/#tenets.core.ranking.RankedFile-attributes","title":"Attributes","text":""},{"location":"api/#tenets.core.ranking.RankedFile.path","title":"path  <code>property</code>","text":"Python<pre><code>path: str\n</code></pre> <p>Get file path.</p>"},{"location":"api/#tenets.core.ranking.RankedFile.file_name","title":"file_name  <code>property</code>","text":"Python<pre><code>file_name: str\n</code></pre> <p>Get file name.</p>"},{"location":"api/#tenets.core.ranking.RankedFile.language","title":"language  <code>property</code>","text":"Python<pre><code>language: str\n</code></pre> <p>Get file language.</p>"},{"location":"api/#tenets.core.ranking.RankedFile-functions","title":"Functions","text":""},{"location":"api/#tenets.core.ranking.RankedFile.generate_explanation","title":"generate_explanation","text":"Python<pre><code>generate_explanation(weights: Dict[str, float], verbose: bool = False) -&gt; str\n</code></pre> <p>Generate human-readable explanation of ranking.</p> <p>Parameters:</p> Name Type Description Default <code>weights</code> <code>Dict[str, float]</code> <p>Factor weights used for ranking</p> required <code>verbose</code> <code>bool</code> <p>Include detailed factor breakdown</p> <code>False</code> <p>Returns:</p> Type Description <code>str</code> <p>Explanation string</p>"},{"location":"api/#tenets.core.ranking.RankedFile.to_dict","title":"to_dict","text":"Python<pre><code>to_dict() -&gt; Dict[str, Any]\n</code></pre> <p>Convert to dictionary representation.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with all ranking information</p>"},{"location":"api/#tenets.core.ranking.RankingExplainer","title":"RankingExplainer","text":"Python<pre><code>RankingExplainer()\n</code></pre> <p>Utility class for generating ranking explanations.</p> <p>Provides detailed explanations of why files ranked the way they did, useful for debugging and understanding ranking behavior.</p> <p>Initialize the explainer.</p>"},{"location":"api/#tenets.core.ranking.RankingExplainer-functions","title":"Functions","text":""},{"location":"api/#tenets.core.ranking.RankingExplainer.explain_ranking","title":"explain_ranking","text":"Python<pre><code>explain_ranking(ranked_files: List[RankedFile], weights: Dict[str, float], top_n: int = 10, include_factors: bool = True) -&gt; str\n</code></pre> <p>Generate comprehensive ranking explanation.</p> <p>Parameters:</p> Name Type Description Default <code>ranked_files</code> <code>List[RankedFile]</code> <p>List of ranked files</p> required <code>weights</code> <code>Dict[str, float]</code> <p>Factor weights used</p> required <code>top_n</code> <code>int</code> <p>Number of top files to explain</p> <code>10</code> <code>include_factors</code> <code>bool</code> <p>Include factor breakdown</p> <code>True</code> <p>Returns:</p> Type Description <code>str</code> <p>Formatted explanation string</p>"},{"location":"api/#tenets.core.ranking.RankingExplainer.compare_rankings","title":"compare_rankings","text":"Python<pre><code>compare_rankings(rankings1: List[RankedFile], rankings2: List[RankedFile], labels: Tuple[str, str] = ('Ranking 1', 'Ranking 2')) -&gt; str\n</code></pre> <p>Compare two different rankings.</p> <p>Useful for understanding how different algorithms or weights affect ranking results.</p> <p>Parameters:</p> Name Type Description Default <code>rankings1</code> <code>List[RankedFile]</code> <p>First ranking</p> required <code>rankings2</code> <code>List[RankedFile]</code> <p>Second ranking</p> required <code>labels</code> <code>Tuple[str, str]</code> <p>Labels for the two rankings</p> <code>('Ranking 1', 'Ranking 2')</code> <p>Returns:</p> Type Description <code>str</code> <p>Comparison report</p>"},{"location":"api/#tenets.core.ranking.RankingFactors","title":"RankingFactors  <code>dataclass</code>","text":"Python<pre><code>RankingFactors(keyword_match: float = 0.0, tfidf_similarity: float = 0.0, bm25_score: float = 0.0, path_relevance: float = 0.0, import_centrality: float = 0.0, dependency_depth: float = 0.0, git_recency: float = 0.0, git_frequency: float = 0.0, git_author_relevance: float = 0.0, complexity_relevance: float = 0.0, maintainability_score: float = 0.0, semantic_similarity: float = 0.0, type_relevance: float = 0.0, code_patterns: float = 0.0, ast_relevance: float = 0.0, test_coverage: float = 0.0, documentation_score: float = 0.0, custom_scores: Dict[str, float] = dict(), metadata: Dict[str, Any] = dict())\n</code></pre> <p>Comprehensive ranking factors for a file.</p> <p>Each factor represents a different dimension of relevance. The final relevance score is computed as a weighted sum of these factors.</p> <p>Factors are grouped into categories: - Text-based: keyword_match, tfidf_similarity, bm25_score - Structure-based: path_relevance, import_centrality, dependency_depth - Git-based: git_recency, git_frequency, git_author_relevance - Complexity-based: complexity_relevance, maintainability_score - Semantic: semantic_similarity (requires ML) - Pattern-based: code_patterns, ast_relevance - Custom: custom_scores for project-specific factors</p> <p>Attributes:</p> Name Type Description <code>keyword_match</code> <code>float</code> <p>Direct keyword matching score (0-1)</p> <code>tfidf_similarity</code> <code>float</code> <p>TF-IDF cosine similarity score (0-1)</p> <code>bm25_score</code> <code>float</code> <p>BM25 relevance score (0-1)</p> <code>path_relevance</code> <code>float</code> <p>File path relevance to query (0-1)</p> <code>import_centrality</code> <code>float</code> <p>How central file is in import graph (0-1)</p> <code>git_recency</code> <code>float</code> <p>How recently file was modified (0-1)</p> <code>git_frequency</code> <code>float</code> <p>How frequently file changes (0-1)</p> <code>git_author_relevance</code> <code>float</code> <p>Relevance based on commit authors (0-1)</p> <code>complexity_relevance</code> <code>float</code> <p>Relevance based on code complexity (0-1)</p> <code>maintainability_score</code> <code>float</code> <p>Code maintainability score (0-1)</p> <code>semantic_similarity</code> <code>float</code> <p>ML-based semantic similarity (0-1)</p> <code>type_relevance</code> <code>float</code> <p>Relevance based on file type (0-1)</p> <code>code_patterns</code> <code>float</code> <p>Pattern matching score (0-1)</p> <code>ast_relevance</code> <code>float</code> <p>AST structure relevance (0-1)</p> <code>dependency_depth</code> <code>float</code> <p>Dependency tree depth score (0-1)</p> <code>test_coverage</code> <code>float</code> <p>Test coverage relevance (0-1)</p> <code>documentation_score</code> <code>float</code> <p>Documentation quality score (0-1)</p> <code>custom_scores</code> <code>Dict[str, float]</code> <p>Dictionary of custom factor scores</p> <code>metadata</code> <code>Dict[str, Any]</code> <p>Additional metadata about factor calculation</p>"},{"location":"api/#tenets.core.ranking.RankingFactors-functions","title":"Functions","text":""},{"location":"api/#tenets.core.ranking.RankingFactors.get_weighted_score","title":"get_weighted_score","text":"Python<pre><code>get_weighted_score(weights: Dict[str, float], normalize: bool = True) -&gt; float\n</code></pre> <p>Calculate weighted relevance score.</p> <p>Parameters:</p> Name Type Description Default <code>weights</code> <code>Dict[str, float]</code> <p>Dictionary mapping factor names to weights</p> required <code>normalize</code> <code>bool</code> <p>Whether to normalize final score to [0, 1]</p> <code>True</code> <p>Returns:</p> Type Description <code>float</code> <p>Weighted relevance score</p>"},{"location":"api/#tenets.core.ranking.RankingFactors.get_top_factors","title":"get_top_factors","text":"Python<pre><code>get_top_factors(weights: Dict[str, float], n: int = 5) -&gt; List[Tuple[str, float, float]]\n</code></pre> <p>Get the top contributing factors.</p> <p>Parameters:</p> Name Type Description Default <code>weights</code> <code>Dict[str, float]</code> <p>Factor weights</p> required <code>n</code> <code>int</code> <p>Number of top factors to return</p> <code>5</code> <p>Returns:</p> Type Description <code>List[Tuple[str, float, float]]</code> <p>List of (factor_name, value, contribution) tuples</p>"},{"location":"api/#tenets.core.ranking.RankingFactors.to_dict","title":"to_dict","text":"Python<pre><code>to_dict() -&gt; Dict[str, Any]\n</code></pre> <p>Convert factors to dictionary representation.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with all factor values</p>"},{"location":"api/#tenets.core.ranking.RankingAlgorithm","title":"RankingAlgorithm","text":"<p>               Bases: <code>Enum</code></p> <p>Available ranking algorithms.</p> <p>Each algorithm provides different trade-offs between speed and accuracy.</p>"},{"location":"api/#tenets.core.ranking.RankingStats","title":"RankingStats  <code>dataclass</code>","text":"Python<pre><code>RankingStats(total_files: int = 0, files_ranked: int = 0, files_failed: int = 0, time_elapsed: float = 0.0, algorithm_used: str = '', threshold_applied: float = 0.0, files_above_threshold: int = 0, average_score: float = 0.0, max_score: float = 0.0, min_score: float = 0.0, corpus_stats: Dict[str, Any] = None)\n</code></pre> <p>Statistics from ranking operation.</p> <p>Tracks performance metrics and diagnostic information about the ranking process for monitoring and optimization.</p> <p>Attributes:</p> Name Type Description <code>total_files</code> <code>int</code> <p>Total number of files processed</p> <code>files_ranked</code> <code>int</code> <p>Number of files successfully ranked</p> <code>files_failed</code> <code>int</code> <p>Number of files that failed ranking</p> <code>time_elapsed</code> <code>float</code> <p>Total time in seconds</p> <code>algorithm_used</code> <code>str</code> <p>Which algorithm was used</p> <code>threshold_applied</code> <code>float</code> <p>Relevance threshold used</p> <code>files_above_threshold</code> <code>int</code> <p>Number of files above threshold</p> <code>average_score</code> <code>float</code> <p>Average relevance score</p> <code>max_score</code> <code>float</code> <p>Maximum relevance score</p> <code>min_score</code> <code>float</code> <p>Minimum relevance score</p> <code>corpus_stats</code> <code>Dict[str, Any]</code> <p>Dictionary of corpus statistics</p>"},{"location":"api/#tenets.core.ranking.RankingStats-functions","title":"Functions","text":""},{"location":"api/#tenets.core.ranking.RankingStats.to_dict","title":"to_dict","text":"Python<pre><code>to_dict() -&gt; Dict[str, Any]\n</code></pre> <p>Convert to dictionary representation.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with all statistics</p>"},{"location":"api/#tenets.core.ranking.RelevanceRanker","title":"RelevanceRanker","text":"Python<pre><code>RelevanceRanker(config: TenetsConfig, algorithm: Optional[str] = None, use_stopwords: Optional[bool] = None)\n</code></pre> <p>Main relevance ranking system.</p> <p>Orchestrates the ranking process by analyzing the corpus, selecting appropriate strategies, and producing ranked results. Supports multiple algorithms, parallel processing, and custom ranking extensions.</p> <p>The ranker follows a multi-stage process: 1. Corpus analysis (TF-IDF, import graph, statistics) 2. Strategy selection based on algorithm 3. Parallel factor calculation 4. Score aggregation and weighting 5. Filtering and sorting</p> <p>Attributes:</p> Name Type Description <code>config</code> <p>TenetsConfig instance</p> <code>logger</code> <p>Logger instance</p> <code>strategies</code> <p>Available ranking strategies</p> <code>custom_rankers</code> <code>List[Callable]</code> <p>Custom ranking functions</p> <code>executor</code> <p>Thread pool for parallel processing</p> <code>stats</code> <p>Latest ranking statistics</p> <code>cache</code> <p>Internal cache for optimizations</p> <p>Initialize the relevance ranker.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>TenetsConfig</code> <p>Tenets configuration</p> required <code>algorithm</code> <code>Optional[str]</code> <p>Override default algorithm</p> <code>None</code> <code>use_stopwords</code> <code>Optional[bool]</code> <p>Override stopword filtering setting</p> <code>None</code>"},{"location":"api/#tenets.core.ranking.RelevanceRanker-attributes","title":"Attributes","text":""},{"location":"api/#tenets.core.ranking.RelevanceRanker.executor","title":"executor  <code>property</code>","text":"Python<pre><code>executor\n</code></pre> <p>Lazy initialization of ThreadPoolExecutor to avoid Windows import issues.</p>"},{"location":"api/#tenets.core.ranking.RelevanceRanker-functions","title":"Functions","text":""},{"location":"api/#tenets.core.ranking.RelevanceRanker.rank_files","title":"rank_files","text":"Python<pre><code>rank_files(files: List[FileAnalysis], prompt_context: PromptContext, algorithm: Optional[str] = None, parallel: bool = True, explain: bool = False) -&gt; List[FileAnalysis]\n</code></pre> <p>Rank files by relevance to prompt.</p> <p>This is the main entry point for ranking files. It analyzes the corpus, applies the selected ranking strategy, and returns files sorted by relevance above the configured threshold.</p> <p>Parameters:</p> Name Type Description Default <code>files</code> <code>List[FileAnalysis]</code> <p>List of files to rank</p> required <code>prompt_context</code> <code>PromptContext</code> <p>Parsed prompt information</p> required <code>algorithm</code> <code>Optional[str]</code> <p>Override algorithm for this ranking</p> <code>None</code> <code>parallel</code> <code>bool</code> <p>Whether to rank files in parallel</p> <code>True</code> <code>explain</code> <code>bool</code> <p>Whether to generate ranking explanations</p> <code>False</code> <p>Returns:</p> Type Description <code>List[FileAnalysis]</code> <p>List of FileAnalysis objects sorted by relevance (highest first)</p> <code>List[FileAnalysis]</code> <p>and filtered by threshold</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If algorithm is invalid</p>"},{"location":"api/#tenets.core.ranking.RelevanceRanker.register_custom_ranker","title":"register_custom_ranker","text":"Python<pre><code>register_custom_ranker(ranker_func: Callable[[List[RankedFile], PromptContext], List[RankedFile]])\n</code></pre> <p>Register a custom ranking function.</p> <p>Custom rankers are applied after the main ranking strategy and can adjust scores based on project-specific logic.</p> <p>Parameters:</p> Name Type Description Default <code>ranker_func</code> <code>Callable[[List[RankedFile], PromptContext], List[RankedFile]]</code> <p>Function that takes ranked files and returns modified list</p> required Example <p>def boost_tests(ranked_files, prompt_context): ...     if 'test' in prompt_context.text: ...         for rf in ranked_files: ...             if 'test' in rf.path: ...                 rf.score *= 1.5 ...     return ranked_files ranker.register_custom_ranker(boost_tests)</p>"},{"location":"api/#tenets.core.ranking.RelevanceRanker.get_ranking_explanation","title":"get_ranking_explanation","text":"Python<pre><code>get_ranking_explanation(ranked_files: List[RankedFile], top_n: int = 10) -&gt; str\n</code></pre> <p>Get detailed explanation of ranking results.</p> <p>Parameters:</p> Name Type Description Default <code>ranked_files</code> <code>List[RankedFile]</code> <p>List of ranked files</p> required <code>top_n</code> <code>int</code> <p>Number of top files to explain</p> <code>10</code> <p>Returns:</p> Type Description <code>str</code> <p>Formatted explanation string</p>"},{"location":"api/#tenets.core.ranking.RelevanceRanker.get_stats","title":"get_stats","text":"Python<pre><code>get_stats() -&gt; RankingStats\n</code></pre> <p>Get latest ranking statistics.</p> <p>Returns:</p> Type Description <code>RankingStats</code> <p>RankingStats object</p>"},{"location":"api/#tenets.core.ranking.RelevanceRanker.shutdown","title":"shutdown","text":"Python<pre><code>shutdown()\n</code></pre> <p>Shutdown the ranker and clean up resources.</p>"},{"location":"api/#tenets.core.ranking.BalancedRankingStrategy","title":"BalancedRankingStrategy","text":"Python<pre><code>BalancedRankingStrategy()\n</code></pre> <p>               Bases: <code>RankingStrategy</code></p> <p>Balanced multi-factor ranking strategy.</p> <p>Initialize balanced ranking strategy.</p>"},{"location":"api/#tenets.core.ranking.BalancedRankingStrategy-functions","title":"Functions","text":""},{"location":"api/#tenets.core.ranking.BalancedRankingStrategy.rank_file","title":"rank_file","text":"Python<pre><code>rank_file(file: FileAnalysis, prompt_context: PromptContext, corpus_stats: Dict[str, Any]) -&gt; RankingFactors\n</code></pre> <p>Balanced ranking using multiple factors.</p>"},{"location":"api/#tenets.core.ranking.BalancedRankingStrategy.get_weights","title":"get_weights","text":"Python<pre><code>get_weights() -&gt; Dict[str, float]\n</code></pre> <p>Get weights for balanced ranking.</p>"},{"location":"api/#tenets.core.ranking.FastRankingStrategy","title":"FastRankingStrategy","text":"Python<pre><code>FastRankingStrategy()\n</code></pre> <p>               Bases: <code>RankingStrategy</code></p> <p>Fast keyword-based ranking strategy.</p> <p>Initialize fast ranking strategy.</p>"},{"location":"api/#tenets.core.ranking.FastRankingStrategy-functions","title":"Functions","text":""},{"location":"api/#tenets.core.ranking.FastRankingStrategy.rank_file","title":"rank_file","text":"Python<pre><code>rank_file(file: FileAnalysis, prompt_context: PromptContext, corpus_stats: Dict[str, Any]) -&gt; RankingFactors\n</code></pre> <p>Fast ranking based on keywords and paths.</p>"},{"location":"api/#tenets.core.ranking.FastRankingStrategy.get_weights","title":"get_weights","text":"Python<pre><code>get_weights() -&gt; Dict[str, float]\n</code></pre> <p>Get weights for fast ranking.</p>"},{"location":"api/#tenets.core.ranking.MLRankingStrategy","title":"MLRankingStrategy","text":"Python<pre><code>MLRankingStrategy()\n</code></pre> <p>               Bases: <code>RankingStrategy</code></p> <p>Machine Learning-based ranking strategy.</p> <p>Initialize ML ranking strategy.</p>"},{"location":"api/#tenets.core.ranking.MLRankingStrategy-functions","title":"Functions","text":""},{"location":"api/#tenets.core.ranking.MLRankingStrategy.rank_file","title":"rank_file","text":"Python<pre><code>rank_file(file: FileAnalysis, prompt_context: PromptContext, corpus_stats: Dict[str, Any]) -&gt; RankingFactors\n</code></pre> <p>ML-based ranking with semantic similarity.</p>"},{"location":"api/#tenets.core.ranking.MLRankingStrategy.get_weights","title":"get_weights","text":"Python<pre><code>get_weights() -&gt; Dict[str, float]\n</code></pre> <p>Get weights for ML ranking.</p>"},{"location":"api/#tenets.core.ranking.RankingStrategy","title":"RankingStrategy","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for ranking strategies.</p>"},{"location":"api/#tenets.core.ranking.RankingStrategy-attributes","title":"Attributes","text":""},{"location":"api/#tenets.core.ranking.RankingStrategy.name","title":"name  <code>abstractmethod</code> <code>property</code>","text":"Python<pre><code>name: str\n</code></pre> <p>Get strategy name.</p>"},{"location":"api/#tenets.core.ranking.RankingStrategy.description","title":"description  <code>abstractmethod</code> <code>property</code>","text":"Python<pre><code>description: str\n</code></pre> <p>Get strategy description.</p>"},{"location":"api/#tenets.core.ranking.RankingStrategy-functions","title":"Functions","text":""},{"location":"api/#tenets.core.ranking.RankingStrategy.rank_file","title":"rank_file  <code>abstractmethod</code>","text":"Python<pre><code>rank_file(file: FileAnalysis, prompt_context: PromptContext, corpus_stats: Dict[str, Any]) -&gt; RankingFactors\n</code></pre> <p>Calculate ranking factors for a file.</p>"},{"location":"api/#tenets.core.ranking.RankingStrategy.get_weights","title":"get_weights  <code>abstractmethod</code>","text":"Python<pre><code>get_weights() -&gt; Dict[str, float]\n</code></pre> <p>Get factor weights for this strategy.</p>"},{"location":"api/#tenets.core.ranking.ThoroughRankingStrategy","title":"ThoroughRankingStrategy","text":"Python<pre><code>ThoroughRankingStrategy()\n</code></pre> <p>               Bases: <code>RankingStrategy</code></p> <p>Thorough deep analysis ranking strategy using centralized NLP.</p> <p>Initialize thorough ranking strategy with NLP components.</p>"},{"location":"api/#tenets.core.ranking.ThoroughRankingStrategy-functions","title":"Functions","text":""},{"location":"api/#tenets.core.ranking.ThoroughRankingStrategy.rank_file","title":"rank_file","text":"Python<pre><code>rank_file(file: FileAnalysis, prompt_context: PromptContext, corpus_stats: Dict[str, Any]) -&gt; RankingFactors\n</code></pre> <p>Thorough ranking with deep analysis using centralized NLP.</p>"},{"location":"api/#tenets.core.ranking.ThoroughRankingStrategy.get_weights","title":"get_weights","text":"Python<pre><code>get_weights() -&gt; Dict[str, float]\n</code></pre> <p>Get weights for thorough ranking.</p>"},{"location":"api/#tenets.core.ranking-functions","title":"Functions","text":""},{"location":"api/#tenets.core.ranking.create_ranker","title":"create_ranker","text":"Python<pre><code>create_ranker(config: Optional[TenetsConfig] = None, algorithm: str = 'balanced', use_stopwords: bool = False) -&gt; RelevanceRanker\n</code></pre> <p>Create a configured relevance ranker.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Optional[TenetsConfig]</code> <p>Configuration (uses default if None)</p> <code>None</code> <code>algorithm</code> <code>str</code> <p>Ranking algorithm to use</p> <code>'balanced'</code> <code>use_stopwords</code> <code>bool</code> <p>Whether to filter stopwords</p> <code>False</code> <p>Returns:</p> Type Description <code>RelevanceRanker</code> <p>Configured RelevanceRanker instance</p>"},{"location":"api/#tenets.core.ranking.check_ml_dependencies","title":"check_ml_dependencies","text":"Python<pre><code>check_ml_dependencies()\n</code></pre> <p>Check ML dependencies (stub).</p>"},{"location":"api/#tenets.core.ranking.get_available_models","title":"get_available_models","text":"Python<pre><code>get_available_models()\n</code></pre> <p>Get available models (stub).</p>"},{"location":"api/#tenets.core.ranking.get_default_ranker","title":"get_default_ranker","text":"Python<pre><code>get_default_ranker(config: Optional[TenetsConfig] = None) -&gt; RelevanceRanker\n</code></pre> <p>Get a default configured ranker.</p> <p>Convenience function to quickly get a working ranker with sensible defaults.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Optional[TenetsConfig]</code> <p>Optional configuration override</p> <code>None</code> <p>Returns:</p> Type Description <code>RelevanceRanker</code> <p>Configured RelevanceRanker instance</p>"},{"location":"api/#tenets.core.ranking.rank_files_simple","title":"rank_files_simple","text":"Python<pre><code>rank_files_simple(files: List, prompt: str, algorithm: str = 'balanced', threshold: float = 0.1) -&gt; List\n</code></pre> <p>Simple interface for ranking files.</p> <p>Provides a simplified API for quick ranking without needing to manage ranker instances or configurations.</p> <p>Parameters:</p> Name Type Description Default <code>files</code> <code>List</code> <p>List of FileAnalysis objects</p> required <code>prompt</code> <code>str</code> <p>Search prompt or query</p> required <code>algorithm</code> <code>str</code> <p>Ranking algorithm to use</p> <code>'balanced'</code> <code>threshold</code> <code>float</code> <p>Minimum relevance score</p> <code>0.1</code> <p>Returns:</p> Type Description <code>List</code> <p>List of files sorted by relevance above threshold</p> Example <p>from tenets.core.ranking import rank_files_simple relevant_files = rank_files_simple( ...     files, ...     \"authentication logic\", ...     algorithm=\"thorough\" ... )</p>"},{"location":"api/#tenets.core.ranking.explain_ranking","title":"explain_ranking","text":"Python<pre><code>explain_ranking(files: List, prompt: str, algorithm: str = 'balanced', top_n: int = 10) -&gt; str\n</code></pre> <p>Get explanation of why files ranked the way they did.</p> <p>Useful for debugging and understanding ranking behavior.</p> <p>Parameters:</p> Name Type Description Default <code>files</code> <code>List</code> <p>List of FileAnalysis objects</p> required <code>prompt</code> <code>str</code> <p>Search prompt</p> required <code>algorithm</code> <code>str</code> <p>Algorithm used</p> <code>'balanced'</code> <code>top_n</code> <code>int</code> <p>Number of top files to explain</p> <code>10</code> <p>Returns:</p> Type Description <code>str</code> <p>Formatted explanation string</p> Example <p>from tenets.core.ranking import explain_ranking explanation = explain_ranking(files, \"database models\") print(explanation)</p>"},{"location":"api/#tenets.core.ranking.get_default_tfidf","title":"get_default_tfidf","text":"Python<pre><code>get_default_tfidf(use_stopwords: bool = False) -&gt; TFIDFCalculator\n</code></pre> <p>Get default TF-IDF calculator instance.</p> <p>Parameters:</p> Name Type Description Default <code>use_stopwords</code> <code>bool</code> <p>Whether to filter stopwords</p> <code>False</code> <p>Returns:</p> Type Description <code>TFIDFCalculator</code> <p>TFIDFCalculator instance</p>"},{"location":"api/#tenets.core.ranking.get_default_bm25","title":"get_default_bm25","text":"Python<pre><code>get_default_bm25(use_stopwords: bool = False) -&gt; BM25Calculator\n</code></pre> <p>Get default BM25 calculator instance.</p> <p>Parameters:</p> Name Type Description Default <code>use_stopwords</code> <code>bool</code> <p>Whether to filter stopwords</p> <code>False</code> <p>Returns:</p> Type Description <code>BM25Calculator</code> <p>BM25Calculator instance</p>"},{"location":"api/#tenetscoresession","title":"tenets.core.session","text":""},{"location":"api/#tenets.core.session","title":"session","text":"<p>Session management package.</p>"},{"location":"api/#tenetscoreinstiller","title":"tenets.core.instiller","text":""},{"location":"api/#tenets.core.instiller","title":"instiller","text":"<p>Instiller module for managing and injecting tenets.</p> <p>The instiller system handles the lifecycle of tenets (guiding principles) and their strategic injection into generated context to maintain consistency across AI interactions.</p>"},{"location":"api/#tenets.core.instiller-classes","title":"Classes","text":""},{"location":"api/#tenets.core.instiller.InjectionPosition","title":"InjectionPosition","text":"<p>               Bases: <code>Enum</code></p> <p>Where to inject tenets in the context.</p>"},{"location":"api/#tenets.core.instiller.TenetInjector","title":"TenetInjector","text":"Python<pre><code>TenetInjector(config: Optional[Dict[str, Any]] = None)\n</code></pre> <p>Handles strategic injection of tenets into context.</p> <p>Initialize the injector.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Optional[Dict[str, Any]]</code> <p>Injection configuration</p> <code>None</code>"},{"location":"api/#tenets.core.instiller.TenetInjector-functions","title":"Functions","text":""},{"location":"api/#tenets.core.instiller.TenetInjector.inject_tenets","title":"inject_tenets","text":"Python<pre><code>inject_tenets(content: str, tenets: List[Tenet], format: str = 'markdown', context_metadata: Optional[Dict[str, Any]] = None) -&gt; Tuple[str, Dict[str, Any]]\n</code></pre> <p>Inject tenets into content.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>The content to inject into</p> required <code>tenets</code> <code>List[Tenet]</code> <p>List of tenets to inject</p> required <code>format</code> <code>str</code> <p>Content format (markdown, xml, json)</p> <code>'markdown'</code> <code>context_metadata</code> <code>Optional[Dict[str, Any]]</code> <p>Metadata about the context</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[str, Dict[str, Any]]</code> <p>Tuple of (modified content, injection metadata)</p>"},{"location":"api/#tenets.core.instiller.TenetInjector.calculate_optimal_injection_count","title":"calculate_optimal_injection_count","text":"Python<pre><code>calculate_optimal_injection_count(content_length: int, available_tenets: int, max_token_increase: int = 1000) -&gt; int\n</code></pre> <p>Calculate optimal number of tenets to inject.</p> <p>Parameters:</p> Name Type Description Default <code>content_length</code> <code>int</code> <p>Current content length</p> required <code>available_tenets</code> <code>int</code> <p>Number of available tenets</p> required <code>max_token_increase</code> <code>int</code> <p>Maximum allowed token increase</p> <code>1000</code> <p>Returns:</p> Type Description <code>int</code> <p>Optimal number of tenets to inject</p>"},{"location":"api/#tenets.core.instiller.TenetInjector.inject_into_context_result","title":"inject_into_context_result","text":"Python<pre><code>inject_into_context_result(context_result: ContextResult, tenets: List[Tenet]) -&gt; ContextResult\n</code></pre> <p>Inject tenets into a ContextResult object.</p> <p>Parameters:</p> Name Type Description Default <code>context_result</code> <code>ContextResult</code> <p>The context result to modify</p> required <code>tenets</code> <code>List[Tenet]</code> <p>Tenets to inject</p> required <p>Returns:</p> Type Description <code>ContextResult</code> <p>Modified context result</p>"},{"location":"api/#tenets.core.instiller.InstillationResult","title":"InstillationResult  <code>dataclass</code>","text":"Python<pre><code>InstillationResult(tenets_instilled: List[Tenet], injection_positions: List[Dict[str, Any]], token_increase: int, strategy_used: str, session: Optional[str] = None, timestamp: datetime = datetime.now(), success: bool = True, error_message: Optional[str] = None, metrics: Optional[Dict[str, Any]] = None, complexity_score: float = 0.0, skip_reason: Optional[str] = None)\n</code></pre> <p>Result of a tenet instillation operation.</p> <p>Attributes:</p> Name Type Description <code>tenets_instilled</code> <code>List[Tenet]</code> <p>List of tenets that were instilled</p> <code>injection_positions</code> <code>List[Dict[str, Any]]</code> <p>Where tenets were injected</p> <code>token_increase</code> <code>int</code> <p>Number of tokens added</p> <code>strategy_used</code> <code>str</code> <p>Injection strategy that was used</p> <code>session</code> <code>Optional[str]</code> <p>Session identifier if any</p> <code>timestamp</code> <code>datetime</code> <p>When instillation occurred</p> <code>success</code> <code>bool</code> <p>Whether instillation succeeded</p> <code>error_message</code> <code>Optional[str]</code> <p>Error message if failed</p> <code>metrics</code> <code>Optional[Dict[str, Any]]</code> <p>Additional metrics from the operation</p> <code>complexity_score</code> <code>float</code> <p>Complexity score of the context</p> <code>skip_reason</code> <code>Optional[str]</code> <p>Reason if injection was skipped</p>"},{"location":"api/#tenets.core.instiller.InstillationResult-functions","title":"Functions","text":""},{"location":"api/#tenets.core.instiller.InstillationResult.to_dict","title":"to_dict","text":"Python<pre><code>to_dict() -&gt; Dict[str, Any]\n</code></pre> <p>Convert to dictionary for serialization.</p>"},{"location":"api/#tenets.core.instiller.Instiller","title":"Instiller","text":"Python<pre><code>Instiller(config: TenetsConfig)\n</code></pre> <p>Main orchestrator for tenet instillation with smart injection.</p> <p>The Instiller manages the entire process of injecting tenets into context, including: - Tracking injection history per session - Analyzing context complexity - Determining optimal injection frequency - Selecting appropriate tenets - Applying injection strategies - Recording metrics and effectiveness</p> <p>It supports multiple injection modes: - Always: Inject into every context - Periodic: Inject every Nth distillation - Adaptive: Smart injection based on complexity and session - Manual: Only inject when explicitly requested</p> <p>Initialize the Instiller.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>TenetsConfig</code> <p>Configuration object</p> required"},{"location":"api/#tenets.core.instiller.Instiller-functions","title":"Functions","text":""},{"location":"api/#tenets.core.instiller.Instiller.inject_system_instruction","title":"inject_system_instruction","text":"Python<pre><code>inject_system_instruction(content: str, format: str = 'markdown', session: Optional[str] = None) -&gt; Tuple[str, Dict[str, Any]]\n</code></pre> <p>Inject system instruction (system prompt) according to config.</p> <p>Behavior: - If system instruction is disabled or empty, return unchanged. - If session provided and once-per-session is enabled, inject only on first distill. - If no session, inject on every distill. - Placement controlled by system_instruction_position. - Formatting controlled by system_instruction_format.</p> <p>Returns modified content and metadata about injection.</p>"},{"location":"api/#tenets.core.instiller.Instiller.instill","title":"instill","text":"Python<pre><code>instill(context: Union[str, ContextResult], session: Optional[str] = None, force: bool = False, strategy: Optional[str] = None, max_tenets: Optional[int] = None, check_frequency: bool = True, inject_system_instruction: Optional[bool] = None) -&gt; Union[str, ContextResult]\n</code></pre> <p>Instill tenets into context with smart injection.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>Union[str, ContextResult]</code> <p>Context to inject tenets into</p> required <code>session</code> <code>Optional[str]</code> <p>Session identifier for tracking</p> <code>None</code> <code>force</code> <code>bool</code> <p>Force injection regardless of frequency settings</p> <code>False</code> <code>strategy</code> <code>Optional[str]</code> <p>Override injection strategy</p> <code>None</code> <code>max_tenets</code> <code>Optional[int]</code> <p>Override maximum tenets</p> <code>None</code> <code>check_frequency</code> <code>bool</code> <p>Whether to check injection frequency</p> <code>True</code> <p>Returns:</p> Type Description <code>Union[str, ContextResult]</code> <p>Modified context with tenets injected (if applicable)</p>"},{"location":"api/#tenets.core.instiller.Instiller.get_session_stats","title":"get_session_stats","text":"Python<pre><code>get_session_stats(session: str) -&gt; Dict[str, Any]\n</code></pre> <p>Get statistics for a specific session.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>str</code> <p>Session identifier</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary of session statistics</p>"},{"location":"api/#tenets.core.instiller.Instiller.get_all_session_stats","title":"get_all_session_stats","text":"Python<pre><code>get_all_session_stats() -&gt; Dict[str, Dict[str, Any]]\n</code></pre> <p>Get statistics for all sessions.</p> <p>Returns:</p> Type Description <code>Dict[str, Dict[str, Any]]</code> <p>Dictionary mapping session IDs to stats</p>"},{"location":"api/#tenets.core.instiller.Instiller.analyze_effectiveness","title":"analyze_effectiveness","text":"Python<pre><code>analyze_effectiveness(session: Optional[str] = None) -&gt; Dict[str, Any]\n</code></pre> <p>Analyze the effectiveness of tenet instillation.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Optional[str]</code> <p>Optional session to analyze</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with analysis results and recommendations</p>"},{"location":"api/#tenets.core.instiller.Instiller.export_instillation_history","title":"export_instillation_history","text":"Python<pre><code>export_instillation_history(output_path: Path, format: str = 'json', session: Optional[str] = None) -&gt; None\n</code></pre> <p>Export instillation history to file.</p> <p>Parameters:</p> Name Type Description Default <code>output_path</code> <code>Path</code> <p>Path to output file</p> required <code>format</code> <code>str</code> <p>Export format (json or csv)</p> <code>'json'</code> <code>session</code> <code>Optional[str]</code> <p>Optional session filter</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If format is not supported</p>"},{"location":"api/#tenets.core.instiller.Instiller.reset_session_history","title":"reset_session_history","text":"Python<pre><code>reset_session_history(session: str) -&gt; bool\n</code></pre> <p>Reset injection history for a session.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>str</code> <p>Session identifier</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if reset, False if session not found</p>"},{"location":"api/#tenets.core.instiller.Instiller.clear_cache","title":"clear_cache","text":"Python<pre><code>clear_cache() -&gt; None\n</code></pre> <p>Clear the results cache.</p>"},{"location":"api/#tenets.core.instiller.TenetManager","title":"TenetManager","text":"Python<pre><code>TenetManager(config: TenetsConfig)\n</code></pre> <p>Manages tenets throughout their lifecycle.</p> <p>Initialize the tenet manager.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>TenetsConfig</code> <p>Tenets configuration</p> required"},{"location":"api/#tenets.core.instiller.TenetManager-functions","title":"Functions","text":""},{"location":"api/#tenets.core.instiller.TenetManager.add_tenet","title":"add_tenet","text":"Python<pre><code>add_tenet(content: Union[str, Tenet], priority: Union[str, Priority] = 'medium', category: Optional[Union[str, TenetCategory]] = None, session: Optional[str] = None, author: Optional[str] = None) -&gt; Tenet\n</code></pre> <p>Add a new tenet.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>Union[str, Tenet]</code> <p>The guiding principle text or a Tenet object</p> required <code>priority</code> <code>Union[str, Priority]</code> <p>Priority level (low, medium, high, critical)</p> <code>'medium'</code> <code>category</code> <code>Optional[Union[str, TenetCategory]]</code> <p>Category for organization</p> <code>None</code> <code>session</code> <code>Optional[str]</code> <p>Bind to specific session</p> <code>None</code> <code>author</code> <code>Optional[str]</code> <p>Who created the tenet</p> <code>None</code> <p>Returns:</p> Type Description <code>Tenet</code> <p>The created Tenet</p>"},{"location":"api/#tenets.core.instiller.TenetManager.get_tenet","title":"get_tenet","text":"Python<pre><code>get_tenet(tenet_id: str) -&gt; Optional[Tenet]\n</code></pre> <p>Get a specific tenet by ID.</p> <p>Parameters:</p> Name Type Description Default <code>tenet_id</code> <code>str</code> <p>Tenet ID (can be partial)</p> required <p>Returns:</p> Type Description <code>Optional[Tenet]</code> <p>The Tenet or None if not found</p>"},{"location":"api/#tenets.core.instiller.TenetManager.list_tenets","title":"list_tenets","text":"Python<pre><code>list_tenets(pending_only: bool = False, instilled_only: bool = False, session: Optional[str] = None, category: Optional[Union[str, TenetCategory]] = None) -&gt; List[Dict[str, Any]]\n</code></pre> <p>List tenets with filtering.</p> <p>Parameters:</p> Name Type Description Default <code>pending_only</code> <code>bool</code> <p>Only show pending tenets</p> <code>False</code> <code>instilled_only</code> <code>bool</code> <p>Only show instilled tenets</p> <code>False</code> <code>session</code> <code>Optional[str]</code> <p>Filter by session binding</p> <code>None</code> <code>category</code> <code>Optional[Union[str, TenetCategory]]</code> <p>Filter by category</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List of tenet dictionaries</p>"},{"location":"api/#tenets.core.instiller.TenetManager.get_pending_tenets","title":"get_pending_tenets","text":"Python<pre><code>get_pending_tenets(session: Optional[str] = None) -&gt; List[Tenet]\n</code></pre> <p>Get all pending tenets.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Optional[str]</code> <p>Filter by session</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Tenet]</code> <p>List of pending Tenet objects</p>"},{"location":"api/#tenets.core.instiller.TenetManager.remove_tenet","title":"remove_tenet","text":"Python<pre><code>remove_tenet(tenet_id: str) -&gt; bool\n</code></pre> <p>Remove a tenet.</p> <p>Parameters:</p> Name Type Description Default <code>tenet_id</code> <code>str</code> <p>Tenet ID (can be partial)</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if removed, False if not found</p>"},{"location":"api/#tenets.core.instiller.TenetManager.instill_tenets","title":"instill_tenets","text":"Python<pre><code>instill_tenets(session: Optional[str] = None, force: bool = False) -&gt; Dict[str, Any]\n</code></pre> <p>Instill pending tenets.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Optional[str]</code> <p>Target session</p> <code>None</code> <code>force</code> <code>bool</code> <p>Re-instill even if already instilled</p> <code>False</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with results</p>"},{"location":"api/#tenets.core.instiller.TenetManager.get_tenets_for_injection","title":"get_tenets_for_injection","text":"Python<pre><code>get_tenets_for_injection(context_length: int, session: Optional[str] = None, max_tenets: int = 5) -&gt; List[Tenet]\n</code></pre> <p>Get tenets ready for injection into context.</p> <p>Parameters:</p> Name Type Description Default <code>context_length</code> <code>int</code> <p>Current context length in tokens</p> required <code>session</code> <code>Optional[str]</code> <p>Current session</p> <code>None</code> <code>max_tenets</code> <code>int</code> <p>Maximum number of tenets to return</p> <code>5</code> <p>Returns:</p> Type Description <code>List[Tenet]</code> <p>List of tenets to inject</p>"},{"location":"api/#tenets.core.instiller.TenetManager.export_tenets","title":"export_tenets","text":"Python<pre><code>export_tenets(format: str = 'yaml', session: Optional[str] = None, include_archived: bool = False) -&gt; str\n</code></pre> <p>Export tenets to YAML or JSON.</p> <p>Parameters:</p> Name Type Description Default <code>format</code> <code>str</code> <p>Export format (yaml or json)</p> <code>'yaml'</code> <code>session</code> <code>Optional[str]</code> <p>Filter by session</p> <code>None</code> <code>include_archived</code> <code>bool</code> <p>Include archived tenets</p> <code>False</code> <p>Returns:</p> Type Description <code>str</code> <p>Serialized tenets</p>"},{"location":"api/#tenets.core.instiller.TenetManager.import_tenets","title":"import_tenets","text":"Python<pre><code>import_tenets(file_path: Union[str, Path], session: Optional[str] = None, override_priority: Optional[Priority] = None) -&gt; int\n</code></pre> <p>Import tenets from file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Union[str, Path]</code> <p>Path to import file</p> required <code>session</code> <code>Optional[str]</code> <p>Bind imported tenets to session</p> <code>None</code> <code>override_priority</code> <code>Optional[Priority]</code> <p>Override priority for all imported tenets</p> <code>None</code> <p>Returns:</p> Type Description <code>int</code> <p>Number of tenets imported</p>"},{"location":"api/#tenets.core.instiller.TenetManager.create_collection","title":"create_collection","text":"Python<pre><code>create_collection(name: str, description: str = '', tenet_ids: Optional[List[str]] = None) -&gt; TenetCollection\n</code></pre> <p>Create a collection of related tenets.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Collection name</p> required <code>description</code> <code>str</code> <p>Collection description</p> <code>''</code> <code>tenet_ids</code> <code>Optional[List[str]]</code> <p>IDs of tenets to include</p> <code>None</code> <p>Returns:</p> Type Description <code>TenetCollection</code> <p>The created TenetCollection</p>"},{"location":"api/#tenets.core.instiller.TenetManager.analyze_tenet_effectiveness","title":"analyze_tenet_effectiveness","text":"Python<pre><code>analyze_tenet_effectiveness() -&gt; Dict[str, Any]\n</code></pre> <p>Analyze effectiveness of tenets.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Analysis of tenet usage and effectiveness</p>"},{"location":"api/#tenetscoregit","title":"tenets.core.git","text":""},{"location":"api/#tenets.core.git","title":"git","text":"<p>Git integration package.</p> <p>This package provides comprehensive git repository analysis capabilities including repository metrics, blame analysis, history chronicling, and statistical insights. It extracts valuable context from version control history to understand code evolution, team dynamics, and development patterns.</p> <p>The git package enables tenets to leverage version control information for better context building, all without requiring any external API calls.</p> <p>Main components: - GitAnalyzer: Core git repository analyzer - BlameAnalyzer: Line-by-line authorship tracking - Chronicle: Repository history narrative generator - GitStatsAnalyzer: Comprehensive repository statistics</p> Example usage <p>from tenets.core.git import GitAnalyzer from tenets.config import TenetsConfig</p> <p>config = TenetsConfig() analyzer = GitAnalyzer(config)</p>"},{"location":"api/#tenets.core.git--get-recent-commits","title":"Get recent commits","text":"<p>commits = analyzer.get_recent_commits(limit=10) for commit in commits:     print(f\"{commit['sha']}: {commit['message']}\")</p>"},{"location":"api/#tenets.core.git--analyze-repository-statistics","title":"Analyze repository statistics","text":"<p>from tenets.core.git import analyze_git_stats stats = analyze_git_stats(Path(\".\")) print(f\"Health score: {stats.health_score}\")</p>"},{"location":"api/#tenets.core.git-classes","title":"Classes","text":""},{"location":"api/#tenets.core.git.GitAnalyzer","title":"GitAnalyzer","text":"Python<pre><code>GitAnalyzer(root: Any)\n</code></pre>"},{"location":"api/#tenets.core.git.GitAnalyzer-functions","title":"Functions","text":""},{"location":"api/#tenets.core.git.GitAnalyzer.is_git_repo","title":"is_git_repo","text":"Python<pre><code>is_git_repo(path: Optional[Path] = None) -&gt; bool\n</code></pre> <p>Return True if the given path (or current root) is inside a git repo.</p> <p>If a path is provided, update internal root and repo accordingly.</p>"},{"location":"api/#tenets.core.git.GitAnalyzer.get_recent_commits","title":"get_recent_commits","text":"Python<pre><code>get_recent_commits(path: Optional[Path] = None, limit: int = 10, files: Optional[List[str]] = None) -&gt; List[Dict[str, Any]]\n</code></pre> <p>Return recent commits as dictionaries suitable for formatting.</p> <p>Each item contains: sha, author, email, message, date (ISO date string).</p>"},{"location":"api/#tenets.core.git.GitAnalyzer.get_contributors","title":"get_contributors","text":"Python<pre><code>get_contributors(path: Optional[Path] = None, files: Optional[List[str]] = None, limit: int = 20) -&gt; List[Dict[str, Any]]\n</code></pre> <p>Return contributors with commit counts.</p> <p>Returns a list of dicts: { name, email, commits } sorted by commits desc.</p>"},{"location":"api/#tenets.core.git.GitAnalyzer.get_current_branch","title":"get_current_branch","text":"Python<pre><code>get_current_branch(path: Optional[Path] = None) -&gt; str\n</code></pre> <p>Return current branch name, or 'HEAD' when detached/unknown.</p>"},{"location":"api/#tenets.core.git.GitAnalyzer.current_branch","title":"current_branch","text":"Python<pre><code>current_branch() -&gt; str\n</code></pre> <p>Alias for get_current_branch() for backward compatibility.</p>"},{"location":"api/#tenets.core.git.GitAnalyzer.get_tracked_files","title":"get_tracked_files","text":"Python<pre><code>get_tracked_files() -&gt; List[str]\n</code></pre> <p>Return list of tracked files in the repository.</p>"},{"location":"api/#tenets.core.git.GitAnalyzer.get_file_history","title":"get_file_history","text":"Python<pre><code>get_file_history(file_path: str) -&gt; List[Any]\n</code></pre> <p>Return commit history for a specific file.</p>"},{"location":"api/#tenets.core.git.GitAnalyzer.commit_count","title":"commit_count","text":"Python<pre><code>commit_count() -&gt; int\n</code></pre> <p>Return total number of commits in the repository.</p>"},{"location":"api/#tenets.core.git.GitAnalyzer.list_authors","title":"list_authors","text":"Python<pre><code>list_authors() -&gt; List[str]\n</code></pre> <p>Return list of unique authors in the repository.</p>"},{"location":"api/#tenets.core.git.GitAnalyzer.author_stats","title":"author_stats","text":"Python<pre><code>author_stats() -&gt; Dict[str, Dict[str, Any]]\n</code></pre> <p>Return statistics by author.</p>"},{"location":"api/#tenets.core.git.GitAnalyzer.get_changes_since","title":"get_changes_since","text":"Python<pre><code>get_changes_since(path: Optional[Path] = None, since: str = '1 week ago', files: Optional[List[str]] = None) -&gt; List[Dict[str, Any]]\n</code></pre> <p>Return a lightweight list of changes since a given time.</p> <p>Each item contains: sha, message, date.</p>"},{"location":"api/#tenets.core.git.GitAnalyzer.get_commits_since","title":"get_commits_since","text":"Python<pre><code>get_commits_since(since: datetime, max_count: int = 1000, author: Optional[str] = None, branch: Optional[str] = None, include_merges: bool = True) -&gt; List[Any]\n</code></pre> <p>Return raw commit objects since a given datetime.</p> <p>Parameters:</p> Name Type Description Default <code>since</code> <code>datetime</code> <p>Start datetime (inclusive)</p> required <code>max_count</code> <code>int</code> <p>Maximum number of commits</p> <code>1000</code> <code>author</code> <code>Optional[str]</code> <p>Optional author filter</p> <code>None</code> <code>branch</code> <code>Optional[str]</code> <p>Optional branch name</p> <code>None</code> <code>include_merges</code> <code>bool</code> <p>Whether to include merge commits</p> <code>True</code> <p>Returns:</p> Type Description <code>List[Any]</code> <p>List of GitPython commit objects</p>"},{"location":"api/#tenets.core.git.GitAnalyzer.get_commits","title":"get_commits","text":"Python<pre><code>get_commits(since: Optional[datetime] = None, until: Optional[datetime] = None, max_count: int = 1000, author: Optional[str] = None, branch: Optional[str] = None) -&gt; List[Dict[str, Any]]\n</code></pre> <p>Return commits between two dates.</p> <p>This method was missing and called by momentum.py.</p> <p>Parameters:</p> Name Type Description Default <code>since</code> <code>Optional[datetime]</code> <p>Start datetime (inclusive)</p> <code>None</code> <code>until</code> <code>Optional[datetime]</code> <p>End datetime (exclusive)</p> <code>None</code> <code>max_count</code> <code>int</code> <p>Maximum number of commits</p> <code>1000</code> <code>author</code> <code>Optional[str]</code> <p>Optional author filter</p> <code>None</code> <code>branch</code> <code>Optional[str]</code> <p>Optional branch name</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List of commit dictionaries with standard fields</p>"},{"location":"api/#tenets.core.git.GitAnalyzer.blame","title":"blame","text":"Python<pre><code>blame(file_path: Path) -&gt; List[Tuple[str, str]]\n</code></pre> <p>Return list of (author, line) for a file using git blame.</p>"},{"location":"api/#tenets.core.git.BlameAnalyzer","title":"BlameAnalyzer","text":"Python<pre><code>BlameAnalyzer(config: TenetsConfig)\n</code></pre> <p>Analyzer for git blame operations.</p> <p>Provides line-by-line authorship analysis using git blame, helping understand code ownership and evolution patterns.</p> <p>Attributes:</p> Name Type Description <code>config</code> <p>Configuration object</p> <code>logger</code> <p>Logger instance</p> <code>_blame_cache</code> <code>Dict[str, FileBlame]</code> <p>Cache for blame results</p> <p>Initialize blame analyzer.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>TenetsConfig</code> <p>TenetsConfig instance</p> required"},{"location":"api/#tenets.core.git.BlameAnalyzer-functions","title":"Functions","text":""},{"location":"api/#tenets.core.git.BlameAnalyzer.analyze_file","title":"analyze_file","text":"Python<pre><code>analyze_file(repo_path: Path, file_path: str, ignore_whitespace: bool = True, follow_renames: bool = True) -&gt; FileBlame\n</code></pre> <p>Analyze blame for a single file.</p> <p>Performs git blame analysis on a file to understand line-by-line authorship.</p> <p>Parameters:</p> Name Type Description Default <code>repo_path</code> <code>Path</code> <p>Path to git repository</p> required <code>file_path</code> <code>str</code> <p>Path to file relative to repo root</p> required <code>ignore_whitespace</code> <code>bool</code> <p>Ignore whitespace changes</p> <code>True</code> <code>follow_renames</code> <code>bool</code> <p>Follow file renames</p> <code>True</code> <p>Returns:</p> Name Type Description <code>FileBlame</code> <code>FileBlame</code> <p>Blame analysis for the file</p> Example <p>analyzer = BlameAnalyzer(config) blame = analyzer.analyze_file(Path(\".\"), \"src/main.py\") print(f\"Primary author: {blame.primary_author}\")</p>"},{"location":"api/#tenets.core.git.BlameAnalyzer.analyze_directory","title":"analyze_directory","text":"Python<pre><code>analyze_directory(repo_path: Path, directory: str = '.', file_pattern: str = '*', recursive: bool = True, max_files: int = 100) -&gt; BlameReport\n</code></pre> <p>Analyze blame for all files in a directory.</p> <p>Performs comprehensive blame analysis across multiple files to understand ownership patterns.</p> <p>Parameters:</p> Name Type Description Default <code>repo_path</code> <code>Path</code> <p>Path to git repository</p> required <code>directory</code> <code>str</code> <p>Directory to analyze</p> <code>'.'</code> <code>file_pattern</code> <code>str</code> <p>File pattern to match</p> <code>'*'</code> <code>recursive</code> <code>bool</code> <p>Whether to recurse into subdirectories</p> <code>True</code> <code>max_files</code> <code>int</code> <p>Maximum files to analyze</p> <code>100</code> <p>Returns:</p> Name Type Description <code>BlameReport</code> <code>BlameReport</code> <p>Comprehensive blame analysis</p> Example <p>analyzer = BlameAnalyzer(config) report = analyzer.analyze_directory( ...     Path(\".\"), ...     directory=\"src\", ...     file_pattern=\"*.py\" ... ) print(f\"Bus factor: {report.bus_factor}\")</p>"},{"location":"api/#tenets.core.git.BlameAnalyzer.get_line_history","title":"get_line_history","text":"Python<pre><code>get_line_history(repo_path: Path, file_path: str, line_number: int, max_depth: int = 10) -&gt; List[Dict[str, Any]]\n</code></pre> <p>Get history of changes for a specific line.</p> <p>Traces the evolution of a specific line through git history.</p> <p>Parameters:</p> Name Type Description Default <code>repo_path</code> <code>Path</code> <p>Path to git repository</p> required <code>file_path</code> <code>str</code> <p>Path to file</p> required <code>line_number</code> <code>int</code> <p>Line number to trace</p> required <code>max_depth</code> <code>int</code> <p>Maximum history depth to retrieve</p> <code>10</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List[Dict[str, Any]]: History of line changes</p> Example <p>analyzer = BlameAnalyzer(config) history = analyzer.get_line_history( ...     Path(\".\"), ...     \"src/main.py\", ...     42 ... ) for change in history: ...     print(f\"{change['date']}: {change['author']}\")</p>"},{"location":"api/#tenets.core.git.BlameLine","title":"BlameLine  <code>dataclass</code>","text":"Python<pre><code>BlameLine(line_number: int, content: str, author: str, author_email: str, commit_sha: str, commit_date: datetime, commit_message: str, is_original: bool = False, age_days: int = 0, previous_authors: List[str] = list())\n</code></pre> <p>Information for a single line from git blame.</p> <p>Represents authorship information for a specific line of code, including who wrote it, when, and in which commit.</p> <p>Attributes:</p> Name Type Description <code>line_number</code> <code>int</code> <p>Line number in file</p> <code>content</code> <code>str</code> <p>Content of the line</p> <code>author</code> <code>str</code> <p>Author name</p> <code>author_email</code> <code>str</code> <p>Author email</p> <code>commit_sha</code> <code>str</code> <p>Commit SHA that introduced this line</p> <code>commit_date</code> <code>datetime</code> <p>Date when line was introduced</p> <code>commit_message</code> <code>str</code> <p>Commit message (first line)</p> <code>is_original</code> <code>bool</code> <p>Whether this is from the original commit</p> <code>age_days</code> <code>int</code> <p>Age of the line in days</p> <code>previous_authors</code> <code>List[str]</code> <p>List of previous authors if line was modified</p>"},{"location":"api/#tenets.core.git.BlameLine-attributes","title":"Attributes","text":""},{"location":"api/#tenets.core.git.BlameLine.is_recent","title":"is_recent  <code>property</code>","text":"Python<pre><code>is_recent: bool\n</code></pre> <p>Check if line was recently modified.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if modified within last 30 days</p>"},{"location":"api/#tenets.core.git.BlameLine.is_old","title":"is_old  <code>property</code>","text":"Python<pre><code>is_old: bool\n</code></pre> <p>Check if line is old.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if older than 180 days</p>"},{"location":"api/#tenets.core.git.BlameLine.is_documentation","title":"is_documentation  <code>property</code>","text":"Python<pre><code>is_documentation: bool\n</code></pre> <p>Check if line appears to be documentation.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if line looks like documentation</p>"},{"location":"api/#tenets.core.git.BlameLine.is_empty","title":"is_empty  <code>property</code>","text":"Python<pre><code>is_empty: bool\n</code></pre> <p>Check if line is empty or whitespace only.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if empty or whitespace</p>"},{"location":"api/#tenets.core.git.BlameReport","title":"BlameReport  <code>dataclass</code>","text":"Python<pre><code>BlameReport(files_analyzed: int = 0, total_lines: int = 0, total_authors: int = 0, file_blames: Dict[str, FileBlame] = dict(), author_summary: Dict[str, Dict[str, Any]] = dict(), ownership_distribution: Dict[str, float] = dict(), collaboration_matrix: Dict[Tuple[str, str], int] = dict(), knowledge_map: Dict[str, Set[str]] = dict(), recommendations: List[str] = list(), hot_files: List[Dict[str, Any]] = list(), single_author_files: List[str] = list(), abandoned_code: Dict[str, int] = dict(), _bus_factor_override: Optional[int] = None, _collab_score_override: Optional[float] = None)\n</code></pre> <p>Comprehensive blame analysis report.</p> <p>Provides detailed authorship analysis across multiple files, identifying ownership patterns, knowledge distribution, and collaboration insights.</p> <p>Attributes:</p> Name Type Description <code>files_analyzed</code> <code>int</code> <p>Number of files analyzed</p> <code>total_lines</code> <code>int</code> <p>Total lines analyzed</p> <code>total_authors</code> <code>int</code> <p>Total unique authors</p> <code>file_blames</code> <code>Dict[str, FileBlame]</code> <p>Blame data for each file</p> <code>author_summary</code> <code>Dict[str, Dict[str, Any]]</code> <p>Summary statistics per author</p> <code>ownership_distribution</code> <code>Dict[str, float]</code> <p>How ownership is distributed</p> <code>collaboration_matrix</code> <code>Dict[Tuple[str, str], int]</code> <p>Who modified whose code</p> <code>knowledge_map</code> <code>Dict[str, Set[str]]</code> <p>Knowledge areas per author</p> <code>recommendations</code> <code>List[str]</code> <p>Actionable recommendations</p> <code>hot_files</code> <code>List[Dict[str, Any]]</code> <p>Files with most contributors</p> <code>single_author_files</code> <code>List[str]</code> <p>Files with only one author</p> <code>abandoned_code</code> <code>Dict[str, int]</code> <p>Code from inactive authors</p>"},{"location":"api/#tenets.core.git.BlameReport-attributes","title":"Attributes","text":""},{"location":"api/#tenets.core.git.BlameReport.bus_factor","title":"bus_factor  <code>property</code> <code>writable</code>","text":"Python<pre><code>bus_factor: int\n</code></pre> <p>Calculate bus factor based on blame data.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Bus factor (number of critical authors)</p>"},{"location":"api/#tenets.core.git.BlameReport.collaboration_score","title":"collaboration_score  <code>property</code> <code>writable</code>","text":"Python<pre><code>collaboration_score: float\n</code></pre> <p>Calculate collaboration score.</p> <p>Higher scores indicate more collaborative development.</p> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>Collaboration score (0-100)</p>"},{"location":"api/#tenets.core.git.BlameReport-functions","title":"Functions","text":""},{"location":"api/#tenets.core.git.BlameReport.to_dict","title":"to_dict","text":"Python<pre><code>to_dict() -&gt; Dict[str, Any]\n</code></pre> <p>Convert report to dictionary.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: Dictionary representation</p>"},{"location":"api/#tenets.core.git.FileBlame","title":"FileBlame  <code>dataclass</code>","text":"Python<pre><code>FileBlame(file_path: str, total_lines: int = 0, blame_lines: List[BlameLine] = list(), authors: Set[str] = set(), author_stats: Dict[str, Dict[str, Any]] = dict(), commit_shas: Set[str] = set(), oldest_line: Optional[BlameLine] = None, newest_line: Optional[BlameLine] = None, age_distribution: Dict[str, int] = dict(), ownership_map: Dict[str, List[Tuple[int, int]]] = dict(), hot_spots: List[Tuple[int, int]] = list())\n</code></pre> <p>Blame information for an entire file.</p> <p>Aggregates line-by-line blame information to provide file-level authorship insights and ownership patterns.</p> <p>Attributes:</p> Name Type Description <code>file_path</code> <code>str</code> <p>Path to the file</p> <code>total_lines</code> <code>int</code> <p>Total number of lines</p> <code>blame_lines</code> <code>List[BlameLine]</code> <p>List of blame information per line</p> <code>authors</code> <code>Set[str]</code> <p>Set of unique authors</p> <code>author_stats</code> <code>Dict[str, Dict[str, Any]]</code> <p>Statistics per author</p> <code>commit_shas</code> <code>Set[str]</code> <p>Set of unique commits</p> <code>oldest_line</code> <code>Optional[BlameLine]</code> <p>Oldest line in file</p> <code>newest_line</code> <code>Optional[BlameLine]</code> <p>Newest line in file</p> <code>age_distribution</code> <code>Dict[str, int]</code> <p>Distribution of line ages</p> <code>ownership_map</code> <code>Dict[str, List[Tuple[int, int]]]</code> <p>Line ranges owned by each author</p> <code>hot_spots</code> <code>List[Tuple[int, int]]</code> <p>Lines that changed frequently</p>"},{"location":"api/#tenets.core.git.FileBlame-attributes","title":"Attributes","text":""},{"location":"api/#tenets.core.git.FileBlame.primary_author","title":"primary_author  <code>property</code>","text":"Python<pre><code>primary_author: Optional[str]\n</code></pre> <p>Get the primary author of the file.</p> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>Optional[str]: Author with most lines or None</p>"},{"location":"api/#tenets.core.git.FileBlame.author_diversity","title":"author_diversity  <code>property</code>","text":"Python<pre><code>author_diversity: float\n</code></pre> <p>Calculate author diversity score.</p> <p>Higher scores indicate more distributed authorship.</p> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>Diversity score (0-1)</p>"},{"location":"api/#tenets.core.git.FileBlame.average_age_days","title":"average_age_days  <code>property</code>","text":"Python<pre><code>average_age_days: float\n</code></pre> <p>Calculate average age of lines in days.</p> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>Average age in days</p>"},{"location":"api/#tenets.core.git.FileBlame.freshness_score","title":"freshness_score  <code>property</code>","text":"Python<pre><code>freshness_score: float\n</code></pre> <p>Calculate code freshness score.</p> <p>Higher scores indicate more recently modified code.</p> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>Freshness score (0-100)</p>"},{"location":"api/#tenets.core.git.Chronicle","title":"Chronicle","text":"Python<pre><code>Chronicle(config: TenetsConfig)\n</code></pre> <p>Main chronicle analyzer for git repositories.</p> <p>Analyzes git history to create a narrative view of repository evolution, identifying patterns, trends, and significant events.</p> <p>Attributes:</p> Name Type Description <code>config</code> <p>Configuration object</p> <code>logger</code> <p>Logger instance</p> <code>git_analyzer</code> <code>Optional[GitAnalyzer]</code> <p>Git analyzer instance</p> <p>Initialize chronicle analyzer.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>TenetsConfig</code> <p>TenetsConfig instance</p> required"},{"location":"api/#tenets.core.git.Chronicle-functions","title":"Functions","text":""},{"location":"api/#tenets.core.git.Chronicle.analyze","title":"analyze","text":"Python<pre><code>analyze(repo_path: Path, since: Optional[str] = None, until: Optional[str] = None, author: Optional[str] = None, branch: Optional[str] = None, include_merges: bool = True, include_stats: bool = True, max_commits: int = 1000) -&gt; ChronicleReport\n</code></pre> <p>Analyze repository history and create chronicle report.</p> <p>Creates a comprehensive narrative of repository evolution including commits, contributors, trends, and significant events.</p> <p>Parameters:</p> Name Type Description Default <code>repo_path</code> <code>Path</code> <p>Path to git repository</p> required <code>since</code> <code>Optional[str]</code> <p>Start date or relative time (e.g., \"2 weeks ago\")</p> <code>None</code> <code>until</code> <code>Optional[str]</code> <p>End date or relative time</p> <code>None</code> <code>author</code> <code>Optional[str]</code> <p>Filter by specific author</p> <code>None</code> <code>branch</code> <code>Optional[str]</code> <p>Specific branch to analyze</p> <code>None</code> <code>include_merges</code> <code>bool</code> <p>Whether to include merge commits</p> <code>True</code> <code>include_stats</code> <code>bool</code> <p>Whether to include detailed statistics</p> <code>True</code> <code>max_commits</code> <code>int</code> <p>Maximum commits to analyze</p> <code>1000</code> <p>Returns:</p> Name Type Description <code>ChronicleReport</code> <code>ChronicleReport</code> <p>Comprehensive chronicle analysis</p> Example <p>chronicle = Chronicle(config) report = chronicle.analyze( ...     Path(\".\"), ...     since=\"1 month ago\", ...     include_stats=True ... ) print(report.summary)</p>"},{"location":"api/#tenets.core.git.ChronicleBuilder","title":"ChronicleBuilder","text":"Python<pre><code>ChronicleBuilder(config: Optional[TenetsConfig] = None)\n</code></pre> <p>High-level builder that assembles a simple chronicle dict for CLI.</p> <p>This composes the existing Chronicle and GitAnalyzer without duplicating analysis logic. It converts inputs to what Chronicle expects and returns a compact, CLI-friendly dictionary.</p> <p>The CLI tests patch this class, but we provide a functional default for real usage.</p>"},{"location":"api/#tenets.core.git.ChronicleBuilder-functions","title":"Functions","text":""},{"location":"api/#tenets.core.git.ChronicleBuilder.build_chronicle","title":"build_chronicle","text":"Python<pre><code>build_chronicle(repo_path: Path, *, since: Optional[object] = None, until: Optional[object] = None, branch: Optional[str] = None, authors: Optional[List[str]] = None, include_merges: bool = True, limit: Optional[int] = None) -&gt; Dict[str, Any]\n</code></pre> <p>Build a chronicle summary for the given repository.</p> <p>Parameters:</p> Name Type Description Default <code>repo_path</code> <code>Path</code> <p>Path to a git repository</p> required <code>since</code> <code>Optional[object]</code> <p>Start time (datetime or relative/ISO string)</p> <code>None</code> <code>until</code> <code>Optional[object]</code> <p>End time (datetime or relative/ISO string)</p> <code>None</code> <code>branch</code> <code>Optional[str]</code> <p>Branch name to analyze</p> <code>None</code> <code>authors</code> <code>Optional[List[str]]</code> <p>Optional author filters (currently advisory)</p> <code>None</code> <code>include_merges</code> <code>bool</code> <p>Include merge commits</p> <code>True</code> <code>limit</code> <code>Optional[int]</code> <p>Max commits to analyze (advisory to Chronicle)</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>A dictionary with keys expected by the CLI views.</p>"},{"location":"api/#tenets.core.git.ChronicleReport","title":"ChronicleReport  <code>dataclass</code>","text":"Python<pre><code>ChronicleReport(period_start: datetime, period_end: datetime, total_commits: int = 0, total_contributors: int = 0, commits: List[CommitSummary] = list(), daily_activity: List[DayActivity] = list(), contributor_stats: Dict[str, Dict[str, Any]] = dict(), commit_type_distribution: Dict[str, int] = dict(), file_change_frequency: List[Tuple[str, int]] = list(), hot_periods: List[Dict[str, Any]] = list(), quiet_periods: List[Dict[str, Any]] = list(), significant_events: List[Dict[str, Any]] = list(), trends: List[str] = list(), summary: str = '')\n</code></pre> <p>Comprehensive chronicle report of repository history.</p> <p>Provides a complete narrative view of repository evolution including commits, contributors, trends, and significant events.</p> <p>Attributes:</p> Name Type Description <code>period_start</code> <code>datetime</code> <p>Start of chronicle period</p> <code>period_end</code> <code>datetime</code> <p>End of chronicle period</p> <code>total_commits</code> <code>int</code> <p>Total commits in period</p> <code>total_contributors</code> <code>int</code> <p>Total unique contributors</p> <code>commits</code> <code>List[CommitSummary]</code> <p>List of commit summaries</p> <code>daily_activity</code> <code>List[DayActivity]</code> <p>Daily activity breakdown</p> <code>contributor_stats</code> <code>Dict[str, Dict[str, Any]]</code> <p>Statistics by contributor</p> <code>commit_type_distribution</code> <code>Dict[str, int]</code> <p>Distribution of commit types</p> <code>file_change_frequency</code> <code>List[Tuple[str, int]]</code> <p>Most frequently changed files</p> <code>hot_periods</code> <code>List[Dict[str, Any]]</code> <p>Periods of high activity</p> <code>quiet_periods</code> <code>List[Dict[str, Any]]</code> <p>Periods of low activity</p> <code>significant_events</code> <code>List[Dict[str, Any]]</code> <p>Notable events (releases, major changes)</p> <code>trends</code> <code>List[str]</code> <p>Identified trends in development</p> <code>summary</code> <code>str</code> <p>Executive summary of the period</p>"},{"location":"api/#tenets.core.git.ChronicleReport-attributes","title":"Attributes","text":""},{"location":"api/#tenets.core.git.ChronicleReport.most_active_day","title":"most_active_day  <code>property</code>","text":"Python<pre><code>most_active_day: Optional[DayActivity]\n</code></pre> <p>Get the most active day.</p> <p>Returns:</p> Type Description <code>Optional[DayActivity]</code> <p>Optional[DayActivity]: Most active day or None</p>"},{"location":"api/#tenets.core.git.ChronicleReport.activity_level","title":"activity_level  <code>property</code>","text":"Python<pre><code>activity_level: str\n</code></pre> <p>Determine overall activity level.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Activity level (high, moderate, low)</p>"},{"location":"api/#tenets.core.git.ChronicleReport-functions","title":"Functions","text":""},{"location":"api/#tenets.core.git.ChronicleReport.to_dict","title":"to_dict","text":"Python<pre><code>to_dict() -&gt; Dict[str, Any]\n</code></pre> <p>Convert to dictionary representation.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: Dictionary representation</p>"},{"location":"api/#tenets.core.git.CommitSummary","title":"CommitSummary  <code>dataclass</code>","text":"Python<pre><code>CommitSummary(sha: str, author: str, email: str, date: datetime, message: str, files_changed: int = 0, lines_added: int = 0, lines_removed: int = 0, is_merge: bool = False, is_revert: bool = False, tags: List[str] = list(), branch: Optional[str] = None, issue_refs: List[str] = list(), pr_refs: List[str] = list())\n</code></pre> <p>Summary information for a single commit.</p> <p>Provides a concise representation of a commit with key information for historical analysis and reporting.</p> <p>Attributes:</p> Name Type Description <code>sha</code> <code>str</code> <p>Commit SHA (short form)</p> <code>author</code> <code>str</code> <p>Commit author name</p> <code>email</code> <code>str</code> <p>Author email</p> <code>date</code> <code>datetime</code> <p>Commit date</p> <code>message</code> <code>str</code> <p>Commit message (first line)</p> <code>files_changed</code> <code>int</code> <p>Number of files changed</p> <code>lines_added</code> <code>int</code> <p>Lines added</p> <code>lines_removed</code> <code>int</code> <p>Lines removed</p> <code>is_merge</code> <code>bool</code> <p>Whether this is a merge commit</p> <code>is_revert</code> <code>bool</code> <p>Whether this is a revert commit</p> <code>tags</code> <code>List[str]</code> <p>Associated tags</p> <code>branch</code> <code>Optional[str]</code> <p>Branch name if available</p> <code>issue_refs</code> <code>List[str]</code> <p>Referenced issue numbers</p> <code>pr_refs</code> <code>List[str]</code> <p>Referenced PR numbers</p>"},{"location":"api/#tenets.core.git.CommitSummary-attributes","title":"Attributes","text":""},{"location":"api/#tenets.core.git.CommitSummary.net_lines","title":"net_lines  <code>property</code>","text":"Python<pre><code>net_lines: int\n</code></pre> <p>Calculate net lines changed.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Lines added minus lines removed</p>"},{"location":"api/#tenets.core.git.CommitSummary.commit_type","title":"commit_type  <code>property</code>","text":"Python<pre><code>commit_type: str\n</code></pre> <p>Determine commit type from message.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Commit type (feat, fix, docs, etc.)</p>"},{"location":"api/#tenets.core.git.CommitSummary-functions","title":"Functions","text":""},{"location":"api/#tenets.core.git.CommitSummary.to_dict","title":"to_dict","text":"Python<pre><code>to_dict() -&gt; Dict[str, Any]\n</code></pre> <p>Convert to dictionary representation.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: Dictionary representation</p>"},{"location":"api/#tenets.core.git.DayActivity","title":"DayActivity  <code>dataclass</code>","text":"Python<pre><code>DayActivity(date: datetime, commits: List[CommitSummary] = list(), total_commits: int = 0, unique_authors: Set[str] = set(), lines_added: int = 0, lines_removed: int = 0, files_touched: Set[str] = set(), commit_types: Dict[str, int] = dict(), peak_hour: Optional[int] = None, first_commit_time: Optional[datetime] = None, last_commit_time: Optional[datetime] = None)\n</code></pre> <p>Activity summary for a single day.</p> <p>Aggregates all repository activity for a specific day to provide daily development rhythm insights.</p> <p>Attributes:</p> Name Type Description <code>date</code> <code>datetime</code> <p>Date of activity</p> <code>commits</code> <code>List[CommitSummary]</code> <p>List of commits on this day</p> <code>total_commits</code> <code>int</code> <p>Total commit count</p> <code>unique_authors</code> <code>Set[str]</code> <p>Set of unique authors</p> <code>lines_added</code> <code>int</code> <p>Total lines added</p> <code>lines_removed</code> <code>int</code> <p>Total lines removed</p> <code>files_touched</code> <code>Set[str]</code> <p>Set of files modified</p> <code>commit_types</code> <code>Dict[str, int]</code> <p>Distribution of commit types</p> <code>peak_hour</code> <code>Optional[int]</code> <p>Hour with most commits</p> <code>first_commit_time</code> <code>Optional[datetime]</code> <p>Time of first commit</p> <code>last_commit_time</code> <code>Optional[datetime]</code> <p>Time of last commit</p>"},{"location":"api/#tenets.core.git.DayActivity-attributes","title":"Attributes","text":""},{"location":"api/#tenets.core.git.DayActivity.net_lines","title":"net_lines  <code>property</code>","text":"Python<pre><code>net_lines: int\n</code></pre> <p>Calculate net lines changed.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Net lines changed for the day</p>"},{"location":"api/#tenets.core.git.DayActivity.productivity_score","title":"productivity_score  <code>property</code>","text":"Python<pre><code>productivity_score: float\n</code></pre> <p>Calculate daily productivity score.</p> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>Productivity score (0-100)</p>"},{"location":"api/#tenets.core.git.CommitStats","title":"CommitStats  <code>dataclass</code>","text":"Python<pre><code>CommitStats(total_commits: int = 0, commits_per_day: float = 0.0, commits_per_week: float = 0.0, commits_per_month: float = 0.0, commit_size_avg: float = 0.0, commit_size_median: float = 0.0, commit_size_std: float = 0.0, largest_commit: Dict[str, Any] = dict(), smallest_commit: Dict[str, Any] = dict(), merge_commits: int = 0, revert_commits: int = 0, fix_commits: int = 0, feature_commits: int = 0, hourly_distribution: List[int] = (lambda: [0] * 24)(), daily_distribution: List[int] = (lambda: [0] * 7)(), monthly_distribution: List[int] = (lambda: [0] * 12)())\n</code></pre> <p>Statistics for commits.</p> <p>Provides detailed statistical analysis of commit patterns including frequency, size, timing, and distribution metrics.</p> <p>Attributes:</p> Name Type Description <code>total_commits</code> <code>int</code> <p>Total number of commits</p> <code>commits_per_day</code> <code>float</code> <p>Average commits per day</p> <code>commits_per_week</code> <code>float</code> <p>Average commits per week</p> <code>commits_per_month</code> <code>float</code> <p>Average commits per month</p> <code>commit_size_avg</code> <code>float</code> <p>Average commit size (lines changed)</p> <code>commit_size_median</code> <code>float</code> <p>Median commit size</p> <code>commit_size_std</code> <code>float</code> <p>Standard deviation of commit size</p> <code>largest_commit</code> <code>Dict[str, Any]</code> <p>Largest single commit</p> <code>smallest_commit</code> <code>Dict[str, Any]</code> <p>Smallest single commit</p> <code>merge_commits</code> <code>int</code> <p>Number of merge commits</p> <code>revert_commits</code> <code>int</code> <p>Number of revert commits</p> <code>fix_commits</code> <code>int</code> <p>Number of fix commits</p> <code>feature_commits</code> <code>int</code> <p>Number of feature commits</p> <code>hourly_distribution</code> <code>List[int]</code> <p>Commits by hour of day</p> <code>daily_distribution</code> <code>List[int]</code> <p>Commits by day of week</p> <code>monthly_distribution</code> <code>List[int]</code> <p>Commits by month</p>"},{"location":"api/#tenets.core.git.CommitStats-attributes","title":"Attributes","text":""},{"location":"api/#tenets.core.git.CommitStats.merge_ratio","title":"merge_ratio  <code>property</code>","text":"Python<pre><code>merge_ratio: float\n</code></pre> <p>Calculate merge commit ratio.</p> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>Ratio of merge commits to total</p>"},{"location":"api/#tenets.core.git.CommitStats.fix_ratio","title":"fix_ratio  <code>property</code>","text":"Python<pre><code>fix_ratio: float\n</code></pre> <p>Calculate fix commit ratio.</p> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>Ratio of fix commits to total</p>"},{"location":"api/#tenets.core.git.CommitStats.peak_hour","title":"peak_hour  <code>property</code>","text":"Python<pre><code>peak_hour: int\n</code></pre> <p>Find peak commit hour.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Hour with most commits (0-23)</p>"},{"location":"api/#tenets.core.git.CommitStats.peak_day","title":"peak_day  <code>property</code>","text":"Python<pre><code>peak_day: str\n</code></pre> <p>Find peak commit day.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Day with most commits</p>"},{"location":"api/#tenets.core.git.ContributorStats","title":"ContributorStats  <code>dataclass</code>","text":"Python<pre><code>ContributorStats(total_contributors: int = 0, active_contributors: int = 0, new_contributors: int = 0, contributor_commits: Dict[str, int] = dict(), contributor_lines: Dict[str, int] = dict(), contributor_files: Dict[str, Set[str]] = dict(), top_contributors: List[Tuple[str, int]] = list(), contribution_inequality: float = 0.0, collaboration_graph: Dict[Tuple[str, str], int] = dict(), timezone_distribution: Dict[str, int] = dict(), retention_rate: float = 0.0, churn_rate: float = 0.0)\n</code></pre> <p>Statistics for contributors.</p> <p>Provides analysis of contributor patterns, productivity metrics, and team dynamics based on git history.</p> <p>Attributes:</p> Name Type Description <code>total_contributors</code> <code>int</code> <p>Total unique contributors</p> <code>active_contributors</code> <code>int</code> <p>Contributors active in last 30 days</p> <code>new_contributors</code> <code>int</code> <p>New contributors in period</p> <code>contributor_commits</code> <code>Dict[str, int]</code> <p>Commits per contributor</p> <code>contributor_lines</code> <code>Dict[str, int]</code> <p>Lines changed per contributor</p> <code>contributor_files</code> <code>Dict[str, Set[str]]</code> <p>Files touched per contributor</p> <code>top_contributors</code> <code>List[Tuple[str, int]]</code> <p>Most active contributors</p> <code>contribution_inequality</code> <code>float</code> <p>Gini coefficient of contributions</p> <code>collaboration_graph</code> <code>Dict[Tuple[str, str], int]</code> <p>Who works with whom</p> <code>timezone_distribution</code> <code>Dict[str, int]</code> <p>Contributors by timezone</p> <code>retention_rate</code> <code>float</code> <p>Contributor retention rate</p> <code>churn_rate</code> <code>float</code> <p>Contributor churn rate</p>"},{"location":"api/#tenets.core.git.ContributorStats-attributes","title":"Attributes","text":""},{"location":"api/#tenets.core.git.ContributorStats.avg_commits_per_contributor","title":"avg_commits_per_contributor  <code>property</code>","text":"Python<pre><code>avg_commits_per_contributor: float\n</code></pre> <p>Calculate average commits per contributor.</p> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>Average commits</p>"},{"location":"api/#tenets.core.git.ContributorStats.bus_factor","title":"bus_factor  <code>property</code>","text":"Python<pre><code>bus_factor: int\n</code></pre> <p>Calculate bus factor.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Number of key contributors</p>"},{"location":"api/#tenets.core.git.ContributorStats.collaboration_score","title":"collaboration_score  <code>property</code>","text":"Python<pre><code>collaboration_score: float\n</code></pre> <p>Calculate collaboration score.</p> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>Collaboration score (0-100)</p>"},{"location":"api/#tenets.core.git.FileStats","title":"FileStats  <code>dataclass</code>","text":"Python<pre><code>FileStats(total_files: int = 0, active_files: int = 0, new_files: int = 0, deleted_files: int = 0, file_changes: Dict[str, int] = dict(), file_sizes: Dict[str, int] = dict(), largest_files: List[Tuple[str, int]] = list(), most_changed: List[Tuple[str, int]] = list(), file_age: Dict[str, int] = dict(), file_churn: Dict[str, float] = dict(), hot_files: List[str] = list(), stable_files: List[str] = list(), file_types: Dict[str, int] = dict())\n</code></pre> <p>Statistics for files.</p> <p>Provides analysis of file-level metrics including change frequency, size distribution, and file lifecycle patterns.</p> <p>Attributes:</p> Name Type Description <code>total_files</code> <code>int</code> <p>Total files in repository</p> <code>active_files</code> <code>int</code> <p>Files changed in period</p> <code>new_files</code> <code>int</code> <p>Files added in period</p> <code>deleted_files</code> <code>int</code> <p>Files deleted in period</p> <code>file_changes</code> <code>Dict[str, int]</code> <p>Number of changes per file</p> <code>file_sizes</code> <code>Dict[str, int]</code> <p>Size distribution of files</p> <code>largest_files</code> <code>List[Tuple[str, int]]</code> <p>Largest files by line count</p> <code>most_changed</code> <code>List[Tuple[str, int]]</code> <p>Most frequently changed files</p> <code>file_age</code> <code>Dict[str, int]</code> <p>Age distribution of files</p> <code>file_churn</code> <code>Dict[str, float]</code> <p>Churn rate per file</p> <code>hot_files</code> <code>List[str]</code> <p>Files with high activity</p> <code>stable_files</code> <code>List[str]</code> <p>Files with low activity</p> <code>file_types</code> <code>Dict[str, int]</code> <p>Distribution by file type</p>"},{"location":"api/#tenets.core.git.FileStats-attributes","title":"Attributes","text":""},{"location":"api/#tenets.core.git.FileStats.avg_file_size","title":"avg_file_size  <code>property</code>","text":"Python<pre><code>avg_file_size: float\n</code></pre> <p>Calculate average file size.</p> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>Average size in lines</p>"},{"location":"api/#tenets.core.git.FileStats.file_stability","title":"file_stability  <code>property</code>","text":"Python<pre><code>file_stability: float\n</code></pre> <p>Calculate overall file stability.</p> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>Stability score (0-100)</p>"},{"location":"api/#tenets.core.git.FileStats.churn_rate","title":"churn_rate  <code>property</code>","text":"Python<pre><code>churn_rate: float\n</code></pre> <p>Calculate overall churn rate.</p> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>Average churn rate</p>"},{"location":"api/#tenets.core.git.GitStatsAnalyzer","title":"GitStatsAnalyzer","text":"Python<pre><code>GitStatsAnalyzer(config: TenetsConfig)\n</code></pre> <p>Analyzer for git repository statistics.</p> <p>Provides comprehensive statistical analysis of git repositories to understand development patterns, team dynamics, and code health.</p> <p>Attributes:</p> Name Type Description <code>config</code> <p>Configuration object</p> <code>logger</code> <p>Logger instance</p> <code>git_analyzer</code> <code>Optional[GitAnalyzer]</code> <p>Git analyzer instance</p> <p>Initialize statistics analyzer.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>TenetsConfig</code> <p>TenetsConfig instance</p> required"},{"location":"api/#tenets.core.git.GitStatsAnalyzer-functions","title":"Functions","text":""},{"location":"api/#tenets.core.git.GitStatsAnalyzer.analyze","title":"analyze","text":"Python<pre><code>analyze(repo_path: Path, since: Optional[str] = None, until: Optional[str] = None, branch: Optional[str] = None, include_files: bool = True, include_languages: bool = True, max_commits: int = 10000) -&gt; RepositoryStats\n</code></pre> <p>Analyze repository statistics.</p> <p>Performs comprehensive statistical analysis of a git repository to provide insights into development patterns and health.</p> <p>Parameters:</p> Name Type Description Default <code>repo_path</code> <code>Path</code> <p>Path to git repository</p> required <code>since</code> <code>Optional[str]</code> <p>Start date or relative time</p> <code>None</code> <code>until</code> <code>Optional[str]</code> <p>End date or relative time</p> <code>None</code> <code>branch</code> <code>Optional[str]</code> <p>Specific branch to analyze</p> <code>None</code> <code>include_files</code> <code>bool</code> <p>Whether to include file statistics</p> <code>True</code> <code>include_languages</code> <code>bool</code> <p>Whether to analyze languages</p> <code>True</code> <code>max_commits</code> <code>int</code> <p>Maximum commits to analyze</p> <code>10000</code> <p>Returns:</p> Name Type Description <code>RepositoryStats</code> <code>RepositoryStats</code> <p>Comprehensive statistics</p> Example <p>analyzer = GitStatsAnalyzer(config) stats = analyzer.analyze(Path(\".\")) print(f\"Health score: {stats.health_score}\")</p>"},{"location":"api/#tenets.core.git.RepositoryStats","title":"RepositoryStats  <code>dataclass</code>","text":"Python<pre><code>RepositoryStats(repo_age_days: int = 0, total_commits: int = 0, total_contributors: int = 0, total_files: int = 0, total_lines: int = 0, languages: Dict[str, int] = dict(), commit_stats: CommitStats = CommitStats(), contributor_stats: ContributorStats = ContributorStats(), file_stats: FileStats = FileStats(), growth_rate: float = 0.0, activity_trend: str = 'stable', health_score: float = 0.0, risk_factors: List[str] = list(), strengths: List[str] = list())\n</code></pre> <p>Overall repository statistics.</p> <p>Aggregates various statistical analyses to provide comprehensive insights into repository health and development patterns.</p> <p>Attributes:</p> Name Type Description <code>repo_age_days</code> <code>int</code> <p>Age of repository in days</p> <code>total_commits</code> <code>int</code> <p>Total commits</p> <code>total_contributors</code> <code>int</code> <p>Total contributors</p> <code>total_files</code> <code>int</code> <p>Total files</p> <code>total_lines</code> <code>int</code> <p>Total lines of code</p> <code>languages</code> <code>Dict[str, int]</code> <p>Programming languages used</p> <code>commit_stats</code> <code>CommitStats</code> <p>Commit statistics</p> <code>contributor_stats</code> <code>ContributorStats</code> <p>Contributor statistics</p> <code>file_stats</code> <code>FileStats</code> <p>File statistics</p> <code>growth_rate</code> <code>float</code> <p>Repository growth rate</p> <code>activity_trend</code> <code>str</code> <p>Recent activity trend</p> <code>health_score</code> <code>float</code> <p>Overall health score</p> <code>risk_factors</code> <code>List[str]</code> <p>Identified risk factors</p> <code>strengths</code> <code>List[str]</code> <p>Identified strengths</p>"},{"location":"api/#tenets.core.git.RepositoryStats-functions","title":"Functions","text":""},{"location":"api/#tenets.core.git.RepositoryStats.to_dict","title":"to_dict","text":"Python<pre><code>to_dict() -&gt; Dict[str, Any]\n</code></pre> <p>Convert to dictionary representation.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: Dictionary representation</p>"},{"location":"api/#tenets.core.git-functions","title":"Functions","text":""},{"location":"api/#tenets.core.git.analyze_repository","title":"analyze_repository","text":"Python<pre><code>analyze_repository(path: Optional[Path] = None, config: Optional[Any] = None) -&gt; Dict[str, Any]\n</code></pre> <p>Analyze a git repository comprehensively.</p> <p>This is a convenience function that creates a GitAnalyzer instance and performs basic repository analysis.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Optional[Path]</code> <p>Path to repository (defaults to current directory)</p> <code>None</code> <code>config</code> <code>Optional[Any]</code> <p>Optional TenetsConfig instance</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with repository information including branch,</p> <code>Dict[str, Any]</code> <p>recent commits, and contributors</p> Example <p>from tenets.core.git import analyze_repository</p> <p>repo_info = analyze_repository(Path(\"./my_project\")) print(f\"Current branch: {repo_info['branch']}\") print(f\"Recent commits: {len(repo_info['recent_commits'])}\") print(f\"Contributors: {len(repo_info['contributors'])}\")</p>"},{"location":"api/#tenets.core.git.get_git_context","title":"get_git_context","text":"Python<pre><code>get_git_context(path: Optional[Path] = None, files: Optional[List[str]] = None, since: str = '1 week ago', config: Optional[Any] = None) -&gt; Dict[str, Any]\n</code></pre> <p>Get git context for specific files or time period.</p> <p>Retrieves relevant git information to provide context about recent changes, contributors, and activity patterns.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Optional[Path]</code> <p>Repository path (defaults to current directory)</p> <code>None</code> <code>files</code> <code>Optional[List[str]]</code> <p>Specific files to get context for</p> <code>None</code> <code>since</code> <code>str</code> <p>Time period to analyze (e.g., \"2 weeks ago\")</p> <code>'1 week ago'</code> <code>config</code> <code>Optional[Any]</code> <p>Optional TenetsConfig instance</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with git context including commits, contributors,</p> <code>Dict[str, Any]</code> <p>and activity summary</p> Example <p>from tenets.core.git import get_git_context</p> <p>context = get_git_context( ...     files=[\"src/main.py\", \"src/utils.py\"], ...     since=\"1 month ago\" ... ) print(f\"Changes: {len(context['commits'])}\") print(f\"Active contributors: {len(context['contributors'])}\")</p>"},{"location":"api/#tenets.core.git.analyze_blame","title":"analyze_blame","text":"Python<pre><code>analyze_blame(repo_path: Path, target: str = '.', ignore_whitespace: bool = True, follow_renames: bool = True, max_files: int = 100, config: Optional[Any] = None) -&gt; BlameReport\n</code></pre> <p>Analyze code ownership using git blame.</p> <p>Performs line-by-line authorship analysis to understand code ownership patterns and identify knowledge holders.</p> <p>Parameters:</p> Name Type Description Default <code>repo_path</code> <code>Path</code> <p>Path to git repository</p> required <code>target</code> <code>str</code> <p>File or directory to analyze</p> <code>'.'</code> <code>ignore_whitespace</code> <code>bool</code> <p>Ignore whitespace changes in blame</p> <code>True</code> <code>follow_renames</code> <code>bool</code> <p>Track file renames</p> <code>True</code> <code>max_files</code> <code>int</code> <p>Maximum files to analyze (for directories)</p> <code>100</code> <code>config</code> <code>Optional[Any]</code> <p>Optional TenetsConfig instance</p> <code>None</code> <p>Returns:</p> Type Description <code>BlameReport</code> <p>BlameReport with comprehensive ownership analysis</p> Example <p>from tenets.core.git import analyze_blame</p> <p>blame = analyze_blame(Path(\".\"), target=\"src/\") print(f\"Bus factor: {blame.bus_factor}\") print(f\"Primary authors: {blame.author_summary}\")</p>"},{"location":"api/#tenets.core.git.analyze_blame--analyze-single-file","title":"Analyze single file","text":"<p>file_blame = analyze_blame(Path(\".\"), target=\"main.py\") print(f\"Primary author: {file_blame.primary_author}\")</p>"},{"location":"api/#tenets.core.git.get_file_ownership","title":"get_file_ownership","text":"Python<pre><code>get_file_ownership(repo_path: Path, file_path: str, config: Optional[Any] = None) -&gt; Dict[str, Any]\n</code></pre> <p>Get ownership information for a specific file.</p> <p>Quick function to get the primary author and ownership distribution for a single file.</p> <p>Parameters:</p> Name Type Description Default <code>repo_path</code> <code>Path</code> <p>Repository path</p> required <code>file_path</code> <code>str</code> <p>Path to file relative to repo</p> required <code>config</code> <code>Optional[Any]</code> <p>Optional TenetsConfig instance</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with ownership information</p> Example <p>from tenets.core.git import get_file_ownership</p> <p>ownership = get_file_ownership(Path(\".\"), \"src/main.py\") print(f\"Primary author: {ownership['primary_author']}\") print(f\"Contributors: {ownership['contributors']}\")</p>"},{"location":"api/#tenets.core.git.create_chronicle","title":"create_chronicle","text":"Python<pre><code>create_chronicle(repo_path: Path, since: Optional[str] = None, until: Optional[str] = None, author: Optional[str] = None, include_stats: bool = True, max_commits: int = 1000, config: Optional[Any] = None) -&gt; ChronicleReport\n</code></pre> <p>Create a narrative chronicle of repository history.</p> <p>Generates a comprehensive narrative view of repository evolution including commits, contributors, trends, and significant events.</p> <p>Parameters:</p> Name Type Description Default <code>repo_path</code> <code>Path</code> <p>Path to repository</p> required <code>since</code> <code>Optional[str]</code> <p>Start date or relative time (e.g., \"1 month ago\")</p> <code>None</code> <code>until</code> <code>Optional[str]</code> <p>End date or relative time</p> <code>None</code> <code>author</code> <code>Optional[str]</code> <p>Filter by specific author</p> <code>None</code> <code>include_stats</code> <code>bool</code> <p>Include detailed statistics</p> <code>True</code> <code>max_commits</code> <code>int</code> <p>Maximum commits to analyze</p> <code>1000</code> <code>config</code> <code>Optional[Any]</code> <p>Optional TenetsConfig instance</p> <code>None</code> <p>Returns:</p> Type Description <code>ChronicleReport</code> <p>ChronicleReport with repository narrative</p> Example <p>from tenets.core.git import create_chronicle</p> <p>chronicle = create_chronicle( ...     Path(\".\"), ...     since=\"3 months ago\", ...     include_stats=True ... ) print(chronicle.summary) print(f\"Activity level: {chronicle.activity_level}\") for event in chronicle.significant_events:     print(f\"{event['date']}: {event['description']}\")</p>"},{"location":"api/#tenets.core.git.get_recent_history","title":"get_recent_history","text":"Python<pre><code>get_recent_history(repo_path: Path, days: int = 7, config: Optional[Any] = None) -&gt; Dict[str, Any]\n</code></pre> <p>Get recent repository history summary.</p> <p>Quick function to get a summary of recent repository activity.</p> <p>Parameters:</p> Name Type Description Default <code>repo_path</code> <code>Path</code> <p>Repository path</p> required <code>days</code> <code>int</code> <p>Number of days to look back</p> <code>7</code> <code>config</code> <code>Optional[Any]</code> <p>Optional TenetsConfig instance</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with recent history summary</p> Example <p>from tenets.core.git import get_recent_history</p> <p>history = get_recent_history(Path(\".\"), days=14) print(f\"Commits: {history['total_commits']}\") print(f\"Active contributors: {history['contributors']}\") print(f\"Most active day: {history['most_active_day']}\")</p>"},{"location":"api/#tenets.core.git.analyze_git_stats","title":"analyze_git_stats","text":"Python<pre><code>analyze_git_stats(repo_path: Path, since: Optional[str] = None, until: Optional[str] = None, include_files: bool = True, include_languages: bool = True, max_commits: int = 10000, config: Optional[Any] = None) -&gt; RepositoryStats\n</code></pre> <p>Analyze comprehensive repository statistics.</p> <p>Performs statistical analysis of repository to understand development patterns, team dynamics, and code health.</p> <p>Parameters:</p> Name Type Description Default <code>repo_path</code> <code>Path</code> <p>Path to repository</p> required <code>since</code> <code>Optional[str]</code> <p>Start date or relative time</p> <code>None</code> <code>until</code> <code>Optional[str]</code> <p>End date or relative time</p> <code>None</code> <code>include_files</code> <code>bool</code> <p>Whether to include file statistics</p> <code>True</code> <code>include_languages</code> <code>bool</code> <p>Whether to analyze languages</p> <code>True</code> <code>max_commits</code> <code>int</code> <p>Maximum commits to analyze</p> <code>10000</code> <code>config</code> <code>Optional[Any]</code> <p>Optional TenetsConfig instance</p> <code>None</code> <p>Returns:</p> Type Description <code>RepositoryStats</code> <p>RepositoryStats with comprehensive metrics</p> Example <p>from tenets.core.git import analyze_git_stats</p> <p>stats = analyze_git_stats( ...     Path(\".\"), ...     since=\"6 months ago\", ...     include_languages=True ... ) print(f\"Health score: {stats.health_score}\") print(f\"Bus factor: {stats.contributor_stats.bus_factor}\") print(f\"Top languages: {stats.languages}\")</p>"},{"location":"api/#tenets.core.git.analyze_git_stats--view-risk-factors","title":"View risk factors","text":"<p>for risk in stats.risk_factors:     print(f\"Risk: {risk}\")</p>"},{"location":"api/#tenets.core.git.get_repository_health","title":"get_repository_health","text":"Python<pre><code>get_repository_health(repo_path: Path, config: Optional[Any] = None) -&gt; Dict[str, Any]\n</code></pre> <p>Get a quick repository health assessment.</p> <p>Provides a simplified health check with key metrics and actionable recommendations.</p> <p>Parameters:</p> Name Type Description Default <code>repo_path</code> <code>Path</code> <p>Repository path</p> required <code>config</code> <code>Optional[Any]</code> <p>Optional TenetsConfig instance</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with health assessment</p> Example <p>from tenets.core.git import get_repository_health</p> <p>health = get_repository_health(Path(\".\")) print(f\"Score: {health['score']}/100\") print(f\"Status: {health['status']}\") for issue in health['issues']:     print(f\"Issue: {issue}\")</p>"},{"location":"api/#cli","title":"CLI","text":""},{"location":"api/#tenetscli","title":"tenets.cli","text":""},{"location":"api/#tenets.cli","title":"cli","text":"<p>Tenets CLI package.</p> <p>This package contains the Typer application and command groupings used by the <code>tenets</code> command-line interface.</p>"},{"location":"api/#tenets.cli--modules","title":"Modules","text":"<ul> <li>:mod:<code>tenets.cli.app</code> exposes the top-level Typer <code>app</code> and <code>run()</code>.</li> <li>:mod:<code>tenets.cli.commands</code> contains individual subcommands and groups.</li> </ul>"},{"location":"api/#tenets.cli--typical-usage","title":"Typical usage","text":"<p>from tenets.cli.app import app  # noqa: F401</p>"},{"location":"api/#tenets.cli--or-programmatically-invoke","title":"or programmatically invoke","text":""},{"location":"api/#tenets.cli--from-tenetscliapp-import-run-run","title":"from tenets.cli.app import run; run()","text":""},{"location":"api/#tenets.cli-functions","title":"Functions","text":""},{"location":"api/#tenets.cli.run","title":"run","text":"Python<pre><code>run()\n</code></pre> <p>Run the CLI application.</p>"}]}