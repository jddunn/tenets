{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Tenets - Context that feeds your prompts","text":"<p>Context that feeds your prompts</p> <p>Illuminate your codebase. Surface relevant files. Build optimal context.</p> <p>All without leaving your machine. 20+ languages including Python, Go, Rust, Java, C#, Kotlin, Swift, Dart, GDScript &amp; more.</p>        Quick Start             View on GitHub      Terminal <pre><code>$ pip install tenets\n$ tenets distill \"implement OAuth2 authentication\"\n\u2728 Finding relevant files...\n\ud83d\udcca Ranking by importance...\n\ud83d\udce6 Aggregating context (45,231 tokens)\n\u2705 Context ready for your LLM!</code></pre> Illuminating Features Why\u00a0Tenets? Context on Demand <p>Stop hunting for files. Tenets discovers, ranks and assembles your code for you\u2014so you can focus on solving the problem.</p> Deeper Insight <p>Visualize dependencies, uncover complexity hotspots and track velocity trends. Know your codebase like never before.</p> Local &amp; Private <p>Your source never leaves your machine. With zero external API calls, Tenets keeps your intellectual property safe.</p> Flexible &amp; Extensible <p>Dial the ranking algorithm, expand the token budget and add plugins when you need more. Tenets grows with you.</p> Architecture at a Glance Input Scanner Analyzer Ranker Aggregator <p>Tenets flows your query through a pipeline of scanners, analyzers, rankers and aggregators, delivering context precisely tailored to your task.</p> Intelligent Context <p>Multi-factor ranking finds exactly what you need. No more manual file hunting.</p> 100% Local <p>Your code never leaves your machine. Complete privacy, zero API calls.</p> Lightning Fast <p>Analyzes thousands of files in seconds with intelligent caching.</p> Guiding Principles <p>Add persistent instructions that maintain consistency across AI sessions.</p> Code Intelligence <p>Visualize dependencies, track velocity, identify hotspots at a glance.</p> Zero Config <p>Works instantly with smart defaults. Just install and start distilling.</p> How It Works 1 Scan <p>Discovers files respecting .gitignore</p> \u2192 2 Analyze <p>Extracts structure and dependencies</p> \u2192 3 Rank <p>Scores by relevance to your prompt</p> \u2192 4 Aggregate <p>Optimizes within token limits</p> See it in action CLI Bash<pre><code>$ tenets distill \"implement OAuth2 authentication\"\n\u2728 Finding relevant files...\n\ud83d\udcca Ranking by importance...\n\ud83d\udce6 Aggregating context (45,231 tokens)\n\u2705 Context ready for your LLM!\n</code></pre> Output Rank Files Bash<pre><code>$ tenets rank \"fix authentication bug\" --top 10 --factors\n\ud83d\udd0d Scanning codebase...\n\ud83d\udcca Ranking files by relevance...\n\n1. src/auth/service.py - Score: 0.892\n   - semantic_similarity: 85%\n   - keyword_match: 92%\n   - import_centrality: 78%\n\n2. src/auth/middleware.py - Score: 0.834\n   - semantic_similarity: 79%\n   - keyword_match: 88%\n   - import_centrality: 65%\n</code></pre> Tree View Bash<pre><code>$ tenets rank \"add caching\" --tree --scores\n\ud83d\udcc1 Ranked Files\n\u251c\u2500\u2500 \ud83d\udcc2 src/\n\u2502   \u251c\u2500\u2500 \ud83d\udcc4 cache_manager.py [0.892]\n\u2502   \u251c\u2500\u2500 \ud83d\udcc4 redis_client.py [0.834]\n\u2502   \u2514\u2500\u2500 \ud83d\udcc4 config.py [0.756]\n\u251c\u2500\u2500 \ud83d\udcc2 src/api/\n\u2502   \u251c\u2500\u2500 \ud83d\udcc4 endpoints.py [0.723]\n\u2502   \u2514\u2500\u2500 \ud83d\udcc4 middleware.py [0.689]\n\u2514\u2500\u2500 \ud83d\udcc2 tests/\n    \u2514\u2500\u2500 \ud83d\udcc4 test_cache.py [0.534]\n</code></pre> Python Python<pre><code>from tenets import Tenets\nt = Tenets()\nresult = t.distill(\n    prompt=\"map request lifecycle\"\n)\nprint(result.context[:500])  # First 500 chars\n</code></pre> Output Sessions Python<pre><code># Sessions are managed through distill parameters\nctx = t.distill(\"design payment flow\", session_name=\"checkout-flow\")\n# Pin files through pin_file method\nt.pin_file(\"payment.py\")\nt.pin_file(\"stripe.py\")\nctx = t.distill(\"add refund support\", session_name=\"checkout-flow\")\n</code></pre> Output Ready to illuminate your codebase? <p>Join thousands of developers building better with Tenets.</p>          Get Started Now        <code>pip install tenets</code>"},{"location":"BRANDING/","title":"BRANDING","text":""},{"location":"BRANDING/#primary-colors","title":"Primary Colors","text":"<p>$navy-900: #1a2332;  // Logo dark blue $navy-800: #263244;  // Slightly lighter $navy-700: #364152;  // Card backgrounds (dark mode)</p>"},{"location":"BRANDING/#accent-colors","title":"Accent Colors","text":"<p>$amber-500: #f59e0b;  // Lantern flame/glow effect $amber-400: #fbbf24;  // Hover states $amber-300: #fcd34d;  // Highlights</p>"},{"location":"BRANDING/#neutral-palette","title":"Neutral Palette","text":"<p>$cream-50:  #fdfdf9;  // Light mode background (Victorian paper) $cream-100: #f7f5f0;  // Card backgrounds (light mode) $sepia-200: #e8e2d5;  // Borders light mode $sepia-600: #6b5d4f;  // Muted text $sepia-800: #3e342a;  // Body text light mode</p>"},{"location":"BRANDING/#semantic-colors","title":"Semantic Colors","text":"<p>$success: #059669;    // Victorian green $warning: #d97706;    // Brass/copper $error:   #dc2626;    // Deep red $info:    #0891b2;    // Teal</p>"},{"location":"CLI/","title":"Tenets CLI Reference","text":"<p>tenets - Context that feeds your prompts. A command-line tool for intelligent code aggregation, analysis, and visualization.</p>"},{"location":"CLI/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Installation</li> <li>Quick Start</li> <li>Core Commands</li> <li>distill</li> <li>instill</li> <li>rank</li> <li>examine</li> <li>chronicle</li> <li>momentum</li> <li>tenet</li> <li>Visualization Commands</li> <li>viz deps</li> <li>viz complexity</li> <li>viz coupling</li> <li>viz contributors</li> <li>Session Commands</li> <li>Tenet Commands</li> <li>Instill Command</li> <li>System Instruction Commands</li> <li>Configuration</li> <li>Common Use Cases</li> <li>Examples</li> </ul>"},{"location":"CLI/#installation","title":"Installation","text":"Bash<pre><code># Basic install (core features only)\npip install tenets\n\n# With visualization support\npip install tenets[viz]\n\n# With ML-powered ranking\npip install tenets[ml]\n\n# Everything\npip install tenets[all]\n</code></pre>"},{"location":"CLI/#quick-start","title":"Quick Start","text":"Bash<pre><code># Generate context for AI pair programming\ntenets distill \"implement OAuth2\" ./src\n\n# Analyze your codebase\ntenets examine\n\n# Track recent changes\ntenets chronicle --since yesterday\n\n# Visualize dependencies (ASCII by default)\ntenets viz deps\n</code></pre>"},{"location":"CLI/#core-commands","title":"Core Commands","text":""},{"location":"CLI/#distill","title":"distill","text":"<p>Generate optimized context for LLMs from your codebase.</p> Bash<pre><code>tenets distill &lt;prompt&gt; [path] [options]\n</code></pre> <p>Arguments:</p> <ul> <li>prompt: Your query or task description (can be text or URL)</li> <li>path: Directory or files to analyze (default: current directory)</li> </ul> <p>Options:</p> <ul> <li><code>--format</code>, <code>-f</code>: Output format: markdown (default), xml, json</li> <li><code>--model</code>, <code>-m</code>: Target LLM model (e.g., gpt-4o, claude-3-opus)</li> <li><code>--output</code>, <code>-o</code>: Save to file instead of stdout</li> <li><code>--max-tokens</code>: Maximum tokens for context</li> <li><code>--mode</code>: Analysis mode: fast, balanced (default), thorough</li> <li><code>--no-git</code>: Disable git context inclusion</li> <li><code>--use-stopwords</code>: Enable stopword filtering for keyword analysis</li> <li><code>--include</code>, <code>-i</code>: Include file patterns (e.g., \".py,.js\")</li> <li><code>--exclude</code>, <code>-e</code>: Exclude file patterns (e.g., \"test_,.backup\")</li> <li><code>--session</code>, <code>-s</code>: Use a named session for stateful context</li> <li><code>--estimate-cost</code>: Show token usage and cost estimate</li> <li><code>--verbose</code>, <code>-v</code>: Show detailed analysis info</li> <li><code>--full</code>: Include full content for all ranked files (no summarization) until token budget reached</li> <li><code>--condense</code>: Condense whitespace (collapse large blank runs, trim trailing spaces) before token counting</li> <li><code>--remove-comments</code>: Strip comments (heuristic, language-aware) before token counting</li> <li><code>--copy</code>: Copy distilled context directly to clipboard (or set output.copy_on_distill: true in config)</li> </ul> <p>Examples:</p> Bash<pre><code># Basic usage - finds all relevant files for implementing OAuth2\ntenets distill \"implement OAuth2 authentication\"\n\n# From a GitHub issue\ntenets distill https://github.com/org/repo/issues/123\n\n# Target specific model with cost estimation\ntenets distill \"add caching layer\" --model gpt-4o --estimate-cost\n\n# Filter by file types\ntenets distill \"review API endpoints\" --include \"*.py,*.yaml\" --exclude \"test_*\"\n\n# Save context to file\ntenets distill \"debug login issue\" --output context.md\n\n# Use thorough analysis for complex tasks\ntenets distill \"refactor authentication system\" --mode thorough\n\n# Session-based context (maintains state)\ntenets distill \"build payment system\" --session payment-feature\n\n# Full mode (force raw content inclusion)\ntenets distill \"inspect performance code\" --full --max-tokens 60000\n\n# Reduce token usage by stripping comments &amp; whitespace\ntenets distill \"understand API surface\" --remove-comments --condense --stats\n</code></pre>"},{"location":"CLI/#content-transformations","title":"Content Transformations","text":"<p>You can optionally transform file content prior to aggregation/token counting:</p> Flag Effect Safety <code>--full</code> Disables summarization; includes raw file content until budget is hit Budget only <code>--remove-comments</code> Removes line &amp; block comments (language-aware heuristics) Aborts if &gt;60% of non-empty lines would vanish <code>--condense</code> Collapses 3+ blank lines to 1, trims trailing spaces, ensures final newline Lossless for code logic <p>Transformations are applied in this order: comment stripping -&gt; whitespace condensation. Statistics (e.g. removed comment lines) are tracked internally and may be surfaced in future <code>--stats</code> expansions.</p>"},{"location":"CLI/#pinned-files","title":"Pinned Files","text":"<p>Pin critical files so they're always considered first in subsequent distill runs for the same session:</p> Bash<pre><code># Pin individual files\ntenets instill --session refactor-auth --add-file src/auth/service.py --add-file src/auth/models.py\n\n# Pin all files in a folder (respects .gitignore)\ntenets instill --session refactor-auth --add-folder src/auth\n\n# List pinned files\ntenets instill --session refactor-auth --list-pinned\n\n# Generate context (pinned files prioritized)\ntenets distill \"add JWT refresh tokens\" --session refactor-auth --remove-comments\n</code></pre> <p>Pinned files are stored in the session metadata (SQLite) and reloaded automatically\u2014no extra flags needed when distilling.</p>"},{"location":"CLI/#ranking-presets-and-thresholds","title":"Ranking presets and thresholds","text":"<ul> <li>Presets (selected via <code>--mode</code> or config <code>ranking.algorithm</code>):</li> <li><code>fast</code> \u2013 keyword + path signals (broad, quick)</li> <li><code>balanced</code> (default) \u2013 multi-factor (keywords, path, imports, git, complexity)</li> <li> <p><code>thorough</code> \u2013 deeper analysis (heavier)</p> </li> <li> <p>Threshold (config <code>ranking.threshold</code>) controls inclusion. Lower = include more files.</p> </li> <li>Typical ranges:<ul> <li>fast: 0.05\u20130.10</li> <li>balanced: 0.10\u20130.20</li> <li>thorough: 0.10\u20130.20</li> </ul> </li> </ul> <p>Configure in <code>.tenets.yml</code> (repo root):</p> YAML<pre><code>ranking:\n  algorithm: fast      # fast | balanced | thorough\n  threshold: 0.05      # 0.0\u20131.0\n</code></pre> <p>One-off overrides (environment, Git Bash):</p> Bash<pre><code>TENETS_RANKING_THRESHOLD=0.05 TENETS_RANKING_ALGORITHM=fast \\\n  tenets distill \"implement OAuth2\" . --include \"*.py,*.md\" --max-tokens 50000\n\n# Copy output to clipboard directly\ntenets distill \"implement OAuth2\" --copy\n\n# Enable automatic copying in config\noutput:\n  copy_on_distill: true\n</code></pre> <p>Inspect current config:</p> Bash<pre><code>tenets config show --key ranking\n</code></pre> <p>See also: docs/CONFIG.md for full configuration details.</p>"},{"location":"CLI/#rank","title":"rank","text":"<p>Show ranked files by relevance without their content.</p> Bash<pre><code>tenets rank &lt;prompt&gt; [path] [options]\n</code></pre> <p>Arguments:</p> <ul> <li>prompt: Your query or task to rank files against</li> <li>path: Directory or files to analyze (default: current directory)</li> </ul> <p>Options:</p> <ul> <li><code>--format</code>, <code>-f</code>: Output format: markdown (default), json, xml, html, tree</li> <li><code>--output</code>, <code>-o</code>: Save to file instead of stdout</li> <li><code>--mode</code>, <code>-m</code>: Ranking mode: fast, balanced (default), thorough</li> <li><code>--top</code>, <code>-t</code>: Show only top N files</li> <li><code>--min-score</code>: Minimum relevance score (0.0-1.0)</li> <li><code>--max-files</code>: Maximum number of files to show</li> <li><code>--tree</code>: Show results as directory tree</li> <li><code>--scores/--no-scores</code>: Show/hide relevance scores (default: show)</li> <li><code>--factors</code>: Show ranking factor breakdown</li> <li><code>--path-style</code>: Path display: relative (default), absolute, name</li> <li><code>--include</code>, <code>-i</code>: Include file patterns (e.g., \".py,.js\")</li> <li><code>--exclude</code>, <code>-e</code>: Exclude file patterns (e.g., \"test_,.backup\")</li> <li><code>--include-tests</code>: Include test files</li> <li><code>--exclude-tests</code>: Explicitly exclude test files</li> <li><code>--no-git</code>: Disable git signals in ranking</li> <li><code>--session</code>, <code>-s</code>: Use session for stateful ranking</li> <li><code>--stats</code>: Show ranking statistics</li> <li><code>--verbose</code>, <code>-v</code>: Show detailed debug information</li> <li><code>--copy</code>: Copy file list to clipboard (also enabled automatically if config.output.copy_on_rank is true)</li> </ul> <p>Examples:</p> Bash<pre><code># Show top 10 most relevant files for OAuth implementation\ntenets rank \"implement OAuth2\" --top 10\n\n# Show files above a relevance threshold\ntenets rank \"fix authentication bug\" --min-score 0.3\n\n# Tree view with ranking factors breakdown\ntenets rank \"add caching layer\" --tree --factors\n\n# Export ranking as JSON for automation\ntenets rank \"review API endpoints\" --format json -o ranked_files.json\n\n# Quick file list to clipboard (no scores)\ntenets rank \"database queries\" --top 20 --copy --no-scores\n\n# Show only Python files with detailed factors\ntenets rank \"refactor models\" --include \"*.py\" --factors --stats\n\n# HTML report with interactive tree view\ntenets rank \"security audit\" --format html -o security_files.html --tree\n</code></pre> <p>Use Cases:</p> <ol> <li>Understanding Context: See which files would be included in a <code>distill</code> command without generating the full context</li> <li>File Discovery: Find relevant files for manual inspection</li> <li>Automation: Export ranked file lists for feeding into other tools or scripts</li> <li>Code Review: Identify files most relevant to a particular feature or bug</li> <li>Impact Analysis: See which files are most connected to a specific query</li> </ol> <p>Output Formats:</p> <ul> <li>Markdown: Numbered list sorted by relevance with scores and optional factors</li> <li>Tree: Directory tree structure sorted by relevance (directories ordered by their highest-scoring file)</li> <li>JSON: Structured data with paths, scores, ranks, and factors (preserves relevance order)</li> <li>XML: Structured XML for integration with other tools</li> <li>HTML: Interactive web page with relevance-sorted display</li> </ul> <p>The ranking uses the same intelligent multi-factor analysis as <code>distill</code>: - Semantic similarity (ML-based when available) - Keyword matching - TF-IDF statistical relevance - Import/dependency centrality - Path relevance - Git signals (recent changes, frequency)</p>"},{"location":"CLI/#examine","title":"examine","text":"<p>Analyze codebase structure, complexity, and patterns.</p> Bash<pre><code>tenets examine [path] [options]\n</code></pre> <p>Options: - <code>--deep, -d</code>: Perform deep analysis with AST parsing - <code>--output, -o</code>: Save results to file - <code>--metrics</code>: Show detailed code metrics - <code>--complexity</code>: Show complexity analysis - <code>--ownership</code>: Show code ownership (requires git) - <code>--hotspots</code>: Show frequently changed files - <code>--format, -f</code>: Output format: <code>table</code> (default), <code>json</code>, <code>yaml</code> - <code>--no-git</code>: Disable git analysis</p> <p>Examples:</p> Bash<pre><code># Basic analysis with summary table\ntenets examine\n\n# Deep analysis with metrics\ntenets examine --deep --metrics\n\n# Show complexity hotspots\ntenets examine --complexity --hotspots\n\n# Export full analysis as JSON\ntenets examine --output analysis.json --format json\n\n# Generate HTML examination report\ntenets examine --format html --output examination_report.html\n\n# Generate detailed HTML report with all analyses\ntenets examine --ownership --hotspots --show-details --format html -o report.html\n\n# Analyze specific directory with ownership tracking\ntenets examine ./src --ownership\n\n# Generate multiple format reports\ntenets examine --format json -o analysis.json\ntenets examine --format html -o analysis.html\ntenets examine --format markdown -o analysis.md\n</code></pre> <p>Coverage Reports:</p> Bash<pre><code># Run tests with coverage and generate HTML report\npytest --cov=tenets --cov-report=html\n\n# View HTML coverage report (opens htmlcov/index.html)\npython -m webbrowser htmlcov/index.html\n\n# Run tests with multiple coverage formats\npytest --cov=tenets --cov-report=html --cov-report=xml --cov-report=term\n\n# Run specific test module with coverage\npytest tests/cli/commands/test_examine.py --cov=tenets.cli.commands.examine --cov-report=html\n</code></pre> <p>Output Example (Table Format): Text Only<pre><code>Codebase Analysis\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Metric          \u2503 Value     \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 Total Files     \u2502 156       \u2502\n\u2502 Total Lines     \u2502 24,531    \u2502\n\u2502 Languages       \u2502 Python,   \u2502\n\u2502                 \u2502 JavaScript\u2502\n\u2502 Avg Complexity  \u2502 4.32      \u2502\n\u2502 Git Branch      \u2502 main      \u2502\n\u2502 Contributors    \u2502 8         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p>"},{"location":"CLI/#chronicle","title":"chronicle","text":"<p>Track code changes over time using git history.</p> Bash<pre><code>tenets chronicle [options]\n</code></pre> <p>Options: - <code>--since, -s</code>: Time period (e.g., \"yesterday\", \"last-month\", \"2024-01-01\") - <code>--path, -p</code>: Repository path (default: current directory) - <code>--author, -a</code>: Filter by author - <code>--limit, -n</code>: Maximum commits to display</p> <p>Examples:</p> Bash<pre><code># Changes in the last week\ntenets chronicle --since \"last-week\"\n\n# Changes since yesterday\ntenets chronicle --since yesterday\n\n# Filter by author\ntenets chronicle --author \"alice@example.com\"\n</code></pre>"},{"location":"CLI/#momentum","title":"momentum","text":"<p>Track development velocity and team productivity metrics.</p> Bash<pre><code>tenets momentum [options]\n</code></pre> <p>Options: - <code>--path, -p</code>: Repository path (default: current directory) - <code>--since, -s</code>: Time period (default: \"last-month\") - <code>--team</code>: Show team-wide statistics - <code>--author, -a</code>: Show stats for specific author</p> <p>Examples:</p> Bash<pre><code># Personal velocity for last month\ntenets momentum\n\n# Team velocity for the quarter\ntenets momentum --team --since \"3 months\"\n\n# Individual contributor stats\ntenets momentum --author \"alice@example.com\"\n</code></pre>"},{"location":"CLI/#instill","title":"instill","text":"<p>Apply tenets to your current context by injecting them into prompts and outputs.</p> Bash<pre><code>tenets instill [context] [options]\n</code></pre> <p>Options: - <code>--session, -s</code>: Session name for tracking - <code>--frequency</code>: Injection frequency: <code>always</code>, <code>periodic</code>, <code>adaptive</code> - <code>--priority</code>: Minimum tenet priority: <code>low</code>, <code>medium</code>, <code>high</code>, <code>critical</code> - <code>--max-tokens</code>: Maximum tokens to add - <code>--format</code>: Output format</p> <p>Examples:</p> Bash<pre><code># Apply all pending tenets\ntenets instill \"Current code context\"\n\n# Apply tenets for specific session\ntenets instill --session feature-x\n\n# Adaptive injection based on complexity\ntenets instill --frequency adaptive\n</code></pre>"},{"location":"CLI/#tenet","title":"tenet","text":"<p>Manage project tenets - rules and guidelines for your codebase.</p> Bash<pre><code>tenets tenet [subcommand] [options]\n</code></pre> <p>Subcommands: - <code>add</code>: Add a new tenet - <code>list</code>: List all tenets - <code>remove</code>: Remove a tenet - <code>show</code>: Show tenet details - <code>export</code>: Export tenets - <code>import</code>: Import tenets</p> <p>Examples:</p> Bash<pre><code># Add a new tenet\ntenets tenet add \"Always use type hints\"\n\n# List all tenets\ntenets tenet list\n\n# Remove a tenet\ntenets tenet remove &lt;tenet-id&gt;\n</code></pre>"},{"location":"CLI/#visualization-commands","title":"Visualization Commands","text":"<p>All visualization commands support ASCII output for terminal display, with optional graphical formats.</p>"},{"location":"CLI/#viz-deps","title":"viz deps","text":"<p>Visualize code dependencies and architecture with intelligent project detection.</p> Bash<pre><code>tenets viz deps [path] [options]\n</code></pre> <p>Options: - <code>--output, -o</code>: Save to file (e.g., architecture.svg) - <code>--format, -f</code>: Output format: <code>ascii</code>, <code>svg</code>, <code>png</code>, <code>html</code>, <code>json</code>, <code>dot</code> - <code>--level, -l</code>: Dependency level: <code>file</code> (default), <code>module</code>, <code>package</code> - <code>--cluster-by</code>: Group nodes by: <code>directory</code>, <code>module</code>, <code>package</code> - <code>--max-nodes</code>: Maximum nodes to display - <code>--include, -i</code>: Include file patterns (e.g., \".py\") - <code>--exclude, -e</code>: Exclude file patterns (e.g., \"*test\") - <code>--layout</code>: Graph layout: <code>hierarchical</code>, <code>circular</code>, <code>shell</code>, <code>kamada</code></p> <p>Features: - Auto-detection: Automatically detects project type (Python, Node.js, Java, Go, etc.) - Smart aggregation: Three levels of dependency views (file, module, package) - Interactive HTML: D3.js or Plotly-based interactive visualizations - Pure Python: All visualization libraries installable via <code>pip install tenets[viz]</code></p> <p>Examples:</p> Bash<pre><code># Auto-detect project type and show dependencies\ntenets viz deps\n\n# Generate interactive HTML visualization\ntenets viz deps --format html --output deps.html\n\n# Module-level dependencies as SVG\ntenets viz deps --level module --format svg --output modules.svg\n\n# Package architecture with clustering\ntenets viz deps --level package --cluster-by package --output packages.png\n\n# Circular layout for better visibility\ntenets viz deps --layout circular --format svg --output circular.svg\n\n# Limit to top 50 nodes for large projects\ntenets viz deps --max-nodes 50 --format png --output top50.png\n\n# Export to Graphviz DOT format\ntenets viz deps --format dot --output graph.dot\n\n# Filter specific files\ntenets viz deps src/ --include \"*.py\" --exclude \"*test*\"\n</code></pre> <p>ASCII Output Example: Text Only<pre><code>Dependency Tree\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\n\u251c\u2500\u2500 main.py\n\u2502   \u251c\u2500\u2500 auth/handler.py\n\u2502   \u2502   \u251c\u2500\u2500 auth/oauth.py\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 utils/crypto.py\n\u2502   \u2502   \u2514\u2500\u2500 models/user.py\n\u2502   \u2502       \u2514\u2500\u2500 db/base.py\n\u2502   \u2514\u2500\u2500 api/routes.py\n\u2502       \u251c\u2500\u2500 api/endpoints.py\n\u2502       \u2514\u2500\u2500 middleware/cors.py\n\u2514\u2500\u2500 config.py\n</code></pre></p>"},{"location":"CLI/#viz-complexity","title":"viz complexity","text":"<p>Visualize code complexity metrics.</p> Bash<pre><code>tenets viz complexity [path] [options]\n</code></pre> <p>Options: - <code>--output, -o</code>: Save to file - <code>--format, -f</code>: Format: <code>ascii</code>, <code>png</code>, <code>html</code> - <code>--metric, -m</code>: Metric type: <code>cyclomatic</code> (default), <code>cognitive</code> - <code>--threshold</code>: Highlight files above threshold - <code>--hotspots</code>: Focus on complexity hotspots</p> <p>Examples:</p> Bash<pre><code># ASCII bar chart of complexity\ntenets viz complexity\n\n# Show only high-complexity files\ntenets viz complexity --threshold 10 --hotspots\n\n# Save as image\ntenets viz complexity --output complexity.png\n</code></pre> <p>ASCII Output Example: Text Only<pre><code>Complexity Analysis (cyclomatic)\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\nauth/oauth.py                 \u25cf \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 28\nmodels/user.py               \u25d0 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 15\napi/endpoints.py             \u25d0 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 12\nutils/validators.py          \u25cf \u2588\u2588\u2588\u2588\u2588\u2588 8\nconfig/settings.py           \u25cf \u2588\u2588\u2588\u2588 5\n\nLegend: \u25cf Low  \u25d0 Medium  \u25d1 High  \u25cb Very High\n</code></pre></p>"},{"location":"CLI/#viz-coupling","title":"viz coupling","text":"<p>Visualize files that frequently change together.</p> Bash<pre><code>tenets viz coupling [path] [options]\n</code></pre> <p>Options: - <code>--output, -o</code>: Save to file - <code>--format, -f</code>: Format: <code>ascii</code>, <code>html</code> - <code>--min-coupling</code>: Minimum coupling count (default: 2)</p> <p>Examples:</p> Bash<pre><code># Show file coupling matrix\ntenets viz coupling\n\n# Only strong couplings\ntenets viz coupling --min-coupling 5\n\n# Interactive HTML matrix\ntenets viz coupling --output coupling.html\n</code></pre> <p>ASCII Output Example: Text Only<pre><code>File Coupling Matrix\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\n                    auth.py  user.py  api.py  test.py\nauth.py               -        8       3       12\nuser.py               8        -       5       10\napi.py                3        5       -       7\ntest_auth.py         12       10      7        -\n</code></pre></p>"},{"location":"CLI/#viz-contributors","title":"viz contributors","text":"<p>Visualize contributor activity and code ownership.</p> Bash<pre><code>tenets viz contributors [path] [options]\n</code></pre> <p>Options: - <code>--output, -o</code>: Save to file - <code>--format, -f</code>: Format: <code>ascii</code>, <code>png</code> - <code>--active</code>: Show only currently active contributors</p> <p>Examples:</p> Bash<pre><code># Contributor stats\ntenets viz contributors\n\n# Active contributors only\ntenets viz contributors --active\n</code></pre>"},{"location":"CLI/#session-commands","title":"Session Commands","text":"<p>Tenets can persist session state across distill runs. When a configuration is loaded, sessions are stored in a local SQLite database under the cache directory (see Storage below). Use <code>--session &lt;name&gt;</code> with commands like <code>distill</code> to build iterative context.</p> <ul> <li>Only one session is considered active at a time. Resuming a session will mark all others inactive.</li> <li>If a session NAME is omitted for <code>resume</code> or <code>exit</code>, Tenets operates on the currently active session.</li> </ul>"},{"location":"CLI/#session-create","title":"session create","text":"<p>Create a new analysis session.</p> Bash<pre><code>tenets session create &lt;name&gt;\n</code></pre> <p>Example: Bash<pre><code>tenets session create payment-integration\n</code></pre></p>"},{"location":"CLI/#session-start","title":"session start","text":"<p>Alias of <code>session create</code>.</p> Bash<pre><code>tenets session start &lt;name&gt;\n</code></pre>"},{"location":"CLI/#session-resume","title":"session resume","text":"<p>Mark an existing session as active.</p> Bash<pre><code># Resume the active session (if one exists)\ntenets session resume\n\n# Or specify by name\ntenets session resume &lt;name&gt;\n</code></pre>"},{"location":"CLI/#session-exit","title":"session exit","text":"<p>Mark a session as inactive.</p> Bash<pre><code># Exit the current active session\ntenets session exit\n\n# Or exit a specific session by name\ntenets session exit &lt;name&gt;\n</code></pre>"},{"location":"CLI/#session-list","title":"session list","text":"<p>List all sessions.</p> Bash<pre><code>tenets session list\n</code></pre> <p>The output includes an Active column (\"yes\" indicates the current session).</p>"},{"location":"CLI/#session-delete","title":"session delete","text":"<p>Delete a specific session.</p> Bash<pre><code>tenets session delete &lt;name&gt; [--keep-context]\n</code></pre> <p>Options: - <code>--keep-context</code>: Keep stored context artifacts (default: false)</p>"},{"location":"CLI/#session-reset","title":"session reset","text":"<p>Reset (delete and recreate) a session, purging its context.</p> Bash<pre><code>tenets session reset &lt;name&gt;\n</code></pre>"},{"location":"CLI/#session-clear","title":"session clear","text":"<p>Delete ALL sessions at once. Useful for clearing cache and starting fresh.</p> Bash<pre><code>tenets session clear [--keep-context]\n</code></pre> <p>Options: - <code>--keep-context</code>: Keep stored artifacts (default: false, deletes everything)</p> <p>Example: Bash<pre><code># Clear all sessions and their data\ntenets session clear\n\n# Clear sessions but preserve context files\ntenets session clear --keep-context\n</code></pre></p>"},{"location":"CLI/#session-show","title":"session show","text":"<p>Show details for a specific session.</p> Bash<pre><code>tenets session show &lt;name&gt;\n</code></pre>"},{"location":"CLI/#session-add","title":"session add","text":"<p>Attach arbitrary content to a session.</p> Bash<pre><code>tenets session add &lt;name&gt; &lt;kind&gt; &lt;file&gt;\n</code></pre> <p>Arguments: - <code>name</code>: Session name - <code>kind</code>: Content type tag (e.g., note, context_result) - <code>file</code>: File to attach</p> <p>Notes: - Creating or resetting a session marks it active. - Only one session is active at a time (resuming one deactivates others). - Session data is stored in SQLite under <code>~/.tenets/cache/sessions.db</code></p>"},{"location":"CLI/#tenet-commands","title":"Tenet Commands","text":"<p>Create and manage guiding principles (\u201ctenets\u201d) that can be injected into context.</p>"},{"location":"CLI/#tenet-add","title":"tenet add","text":"<p>Add a new tenet.</p> Bash<pre><code>tenets tenet add \"Always use type hints\" --priority high --category style\ntenets tenet add \"Validate all user inputs\" --priority critical --category security\ntenets tenet add \"Use async/await for I/O\" --session feature-x\n</code></pre> <p>Options: - <code>--priority, -p</code>: low | medium | high | critical (default: medium) - <code>--category, -c</code>: Freeform tag (e.g., architecture, security, style, performance, testing) - <code>--session, -s</code>: Bind tenet to a session</p>"},{"location":"CLI/#tenet-list","title":"tenet list","text":"<p>List tenets with filters.</p> Bash<pre><code>tenets tenet list\ntenets tenet list --pending\ntenets tenet list --session oauth --category security --verbose\n</code></pre> <p>Options: - <code>--pending</code>: Only pending - <code>--instilled</code>: Only instilled - <code>--session, -s</code>: Filter by session - <code>--category, -c</code>: Filter by category - <code>--verbose, -v</code>: Show full content and metadata</p>"},{"location":"CLI/#tenet-remove","title":"tenet remove","text":"<p>Remove a tenet by ID (partial ID accepted).</p> Bash<pre><code>tenets tenet remove abc123\ntenets tenet remove abc123 --force\n</code></pre>"},{"location":"CLI/#tenet-show","title":"tenet show","text":"<p>Show details for a tenet.</p> Bash<pre><code>tenets tenet show abc123\n</code></pre>"},{"location":"CLI/#tenet-export--import","title":"tenet export / import","text":"<p>Export/import tenets.</p> Bash<pre><code># Export to stdout or file\ntenets tenet export\ntenets tenet export --format json --session oauth -o team-tenets.json\n\n# Import from file (optionally into a session)\ntenets tenet import team-tenets.yml\ntenets tenet import standards.json --session feature-x\n</code></pre>"},{"location":"CLI/#instill-command","title":"Instill Command","text":"<p>Apply tenets to the current context with smart strategies (periodic/adaptive/manual).</p> Bash<pre><code>tenets instill [options]\n</code></pre> <p>Common options: - <code>--session, -s</code>: Use a named session for history and pinned files - <code>--force</code>: Force instillation regardless of frequency - <code>--max-tenets</code>: Cap number of tenets applied</p> <p>Examples:</p> Bash<pre><code># Apply pending tenets for a session\ntenets instill --session refactor-auth\n\n# Force all tenets once\ntenets instill --force\n</code></pre>"},{"location":"CLI/#system-instruction-commands","title":"System Instruction Commands","text":"<p>Manage the system instruction (system prompt) that can be auto-injected at the start of a session\u2019s first distill (or every output if no session is used).</p>"},{"location":"CLI/#system-instruction-set","title":"system-instruction set","text":"<p>Set/update the system instruction and options.</p> Bash<pre><code>tenets system-instruction set \"You are a helpful coding assistant\" \\\n  --enable \\\n  --position top \\\n  --format markdown\n\n# From file\ntenets system-instruction set --file prompts/system.md --enable\n</code></pre> <p>Options: - <code>--file, -f</code>: Read instruction from file - <code>--enable/--disable</code>: Enable or disable auto-injection - <code>--position</code>: Placement: <code>top</code>, <code>after_header</code>, <code>before_content</code> - <code>--format</code>: Format of injected block: <code>markdown</code>, <code>xml</code>, <code>comment</code>, <code>plain</code> - <code>--save/--no-save</code>: Persist to config</p>"},{"location":"CLI/#system-instruction-show","title":"system-instruction show","text":"<p>Display current configuration and instruction.</p> Bash<pre><code>tenets system-instruction show\ntenets system-instruction show --raw\n</code></pre> <p>Options: - <code>--raw</code>: Print raw instruction only</p>"},{"location":"CLI/#system-instruction-clear","title":"system-instruction clear","text":"<p>Clear and disable the system instruction.</p> Bash<pre><code>tenets system-instruction clear\ntenets system-instruction clear --yes\n</code></pre> <p>Options: - <code>--yes, -y</code>: Skip confirmation</p>"},{"location":"CLI/#system-instruction-test","title":"system-instruction test","text":"<p>Preview how injection would modify content.</p> Bash<pre><code>tenets system-instruction test\ntenets system-instruction test --session my-session\n</code></pre> <p>Options: - <code>--session</code>: Test with a session to respect once-per-session behavior</p>"},{"location":"CLI/#system-instruction-export","title":"system-instruction export","text":"<p>Export the instruction to a file.</p> Bash<pre><code>tenets system-instruction export prompts/system.md\n</code></pre>"},{"location":"CLI/#system-instruction-validate","title":"system-instruction validate","text":"<p>Validate the instruction for basic issues and optional token estimates.</p> Bash<pre><code>tenets system-instruction validate\ntenets system-instruction validate --tokens --max-tokens 800\n</code></pre> <p>Options: - <code>--tokens</code>: Show a rough token estimate - <code>--max-tokens</code>: Threshold for warnings/errors</p>"},{"location":"CLI/#system-instruction-edit","title":"system-instruction edit","text":"<p>Edit the instruction in your editor and save changes back to config.</p> Bash<pre><code>tenets system-instruction edit\ntenets system-instruction edit --editor code\n</code></pre>"},{"location":"CLI/#session-show_1","title":"session show","text":"<p>Show session details.</p> Bash<pre><code>tenets session show &lt;name&gt;\n</code></pre>"},{"location":"CLI/#session-add_1","title":"session add","text":"<p>Attach an artifact (stored as text) to a session.</p> Bash<pre><code>tenets session add &lt;name&gt; &lt;kind&gt; &lt;file&gt;\n</code></pre> <p>Examples of <code>kind</code>: <code>note</code>, <code>context_result</code>, <code>summary</code></p>"},{"location":"CLI/#session-reset_1","title":"session reset","text":"<p>Reset (delete and recreate) a session and purge its context.</p> Bash<pre><code>tenets session reset &lt;name&gt;\n</code></pre>"},{"location":"CLI/#session-delete_1","title":"session delete","text":"<p>Delete a session. Optionally keep stored artifacts.</p> Bash<pre><code>tenets session delete &lt;name&gt; [--keep-context]\n</code></pre>"},{"location":"CLI/#cache-management","title":"Cache Management","text":"Text Only<pre><code># Show cache stats (path, file count, size)\ntenets config cache-stats\n\n# Cleanup old/oversized entries respecting TTL\ntenets config cleanup-cache\n\n# Clear ALL caches (analysis + general) \u2013 destructive\ntenets config clear-cache --yes\n</code></pre> <p>Git data is used strictly for ranking relevance unless explicitly requested via commands like <code>chronicle</code> or <code>viz contributors</code>; it is not embedded in <code>distill</code> output.</p>"},{"location":"CLI/#configuration","title":"Configuration","text":""},{"location":"CLI/#config-set","title":"config set","text":"<p>Set configuration values.</p> Bash<pre><code>tenets config set &lt;key&gt; &lt;value&gt;\n</code></pre> <p>Examples: Bash<pre><code># Set default ranking algorithm\ntenets config set ranking.algorithm balanced\n\n# Set maximum file size\ntenets config set scanner.max_file_size 10000000\n\n# Enable ML features\ntenets config set nlp.use_embeddings true\n</code></pre></p>"},{"location":"CLI/#config-show","title":"config show","text":"<p>Show configuration.</p> Bash<pre><code>tenets config show [options]\n</code></pre> <p>Options: - <code>--key, -k</code>: Show specific key</p> <p>Examples: Bash<pre><code># Show all config\ntenets config show\n\n# Show model costs\ntenets config show --key costs\n\n# Show specific setting\ntenets config show --key ranking.algorithm\n</code></pre></p>"},{"location":"CLI/#storage","title":"Storage","text":"<p>Writable data is stored in a user/project cache directory:</p> <ul> <li>Default: <code>${HOME}/.tenets/cache</code> (Windows: <code>%USERPROFILE%\\\\.tenets\\\\cache</code>)</li> <li>Main DB: <code>${CACHE_DIR}/tenets.db</code> (sessions and future state)</li> <li>Analysis cache: <code>${CACHE_DIR}/analysis/analysis.db</code></li> </ul> <p>Override via <code>.tenets.yml</code>:</p> YAML<pre><code>cache:\n  directory: /path/to/custom/cache\n</code></pre> <p>Or environment:</p> Bash<pre><code>TENETS_CACHE_DIRECTORY=/path/to/custom/cache\n</code></pre> <p>Note on cost estimation: When <code>--estimate-cost</code> is used with <code>distill</code>, Tenets estimates costs using model limits and the built-in pricing table from <code>SUPPORTED_MODELS</code>.</p>"},{"location":"CLI/#common-use-cases","title":"Common Use Cases","text":""},{"location":"CLI/#1-ai-pair-programming","title":"1. AI Pair Programming","text":"<p>Generate context for ChatGPT/Claude when working on features:</p> Bash<pre><code># Initial context for new feature\ntenets distill \"implement user authentication with JWT\" &gt; auth_context.md\n\n# Paste auth_context.md into ChatGPT, then iterate:\ntenets distill \"add password reset functionality\" --session auth-feature\n\n# AI needs to see session info?\ntenets session show auth-feature\n</code></pre>"},{"location":"CLI/#2-code-review-preparation","title":"2. Code Review Preparation","text":"<p>Understand what changed and why:</p> Bash<pre><code># See what changed in the sprint\ntenets chronicle --since \"2 weeks\" --summary\n\n# Get context for reviewing a PR\ntenets distill \"review payment processing changes\"\n\n# Check complexity of changed files\ntenets examine --complexity --hotspots\n</code></pre>"},{"location":"CLI/#3-onboarding-to-new-codebase","title":"3. Onboarding to New Codebase","text":"<p>Quickly understand project structure:</p> Bash<pre><code># Get project overview\ntenets examine --metrics\n\n# Visualize architecture\ntenets viz deps --format ascii\n\n# Find the most complex areas\ntenets viz complexity --hotspots\n\n# See who knows what\ntenets viz contributors\n</code></pre>"},{"location":"CLI/#4-debugging-production-issues","title":"4. Debugging Production Issues","text":"<p>Find relevant code for debugging:</p> Bash<pre><code># Get all context related to the error\ntenets distill \"users getting 500 error on checkout\" --mode thorough\n\n# Include recent changes summary\ntenets chronicle --since \"last-deploy\"\n\n# Search for patterns within a session by iterating with prompts\ntenets distill \"find error handlers\" --session debug-session\n</code></pre>"},{"location":"CLI/#5-technical-debt-assessment","title":"5. Technical Debt Assessment","text":"<p>Identify areas needing refactoring:</p> Bash<pre><code># Find complex files\ntenets examine --complexity --threshold 15\n\n# Find tightly coupled code\ntenets viz coupling --min-coupling 5\n\n# Track velocity trends\ntenets momentum --team --since \"6 months\"\n</code></pre>"},{"location":"CLI/#6-architecture-documentation","title":"6. Architecture Documentation","text":"<p>Generate architecture insights:</p> Bash<pre><code># Export dependency graph\ntenets viz deps --output architecture.svg --cluster-by directory\n\n# Generate comprehensive analysis\ntenets examine --deep --output analysis.json --format json\n\n# Create context for documentation\ntenets distill \"document API architecture\" ./src/api\n</code></pre>"},{"location":"CLI/#examples","title":"Examples","text":""},{"location":"CLI/#complete-workflow-example","title":"Complete Workflow Example","text":"Bash<pre><code># 1. Start a new feature\ntenets session create oauth-integration\n\n# 2. Get initial context\ntenets distill \"implement OAuth2 with Google and GitHub\" \\\n  --session oauth-integration \\\n  --include \"*.py,*.yaml\" \\\n  --exclude \"test_*\" \\\n  --model gpt-4o \\\n  --estimate-cost &gt; oauth_context.md\n\n# 3. Paste into ChatGPT, start coding...\n\n# 4. AI needs more specific context\n# (Show session details)\ntenets session show oauth-integration\n\n# 5. Check your progress\ntenets chronicle --since \"today\"\n\n# 6. Visualize what you built\ntenets viz deps src/auth --format ascii\n\n# 7. Check complexity\ntenets examine src/auth --complexity\n\n# 8. Prepare for review\ntenets distill \"OAuth implementation ready for review\" \\\n  --session oauth-integration\n</code></pre>"},{"location":"CLI/#configuration-file-example","title":"Configuration File Example","text":"<p>Create <code>.tenets.yml</code> in your project:</p> YAML<pre><code># .tenets.yml\ncontext:\n  ranking: balanced\n  max_tokens: 100000\n  include_git: true\n\nscanner:\n  respect_gitignore: true\n  max_file_size: 5000000\n\nignore:\n  - \"*.generated.*\"\n  - \"vendor/\"\n  - \"build/\"\n\noutput:\n  format: markdown\n  summarize_long_files: true\n</code></pre>"},{"location":"CLI/#tips-and-tricks","title":"Tips and Tricks","text":"<ol> <li>Start with fast mode for quick exploration, use thorough for complex tasks</li> <li>Use sessions for multi-step features to maintain context</li> <li>ASCII visualizations are great for README files and documentation</li> <li>Combine commands - examine first, then distill with insights</li> <li>Git integration works automatically - no setup needed</li> <li>Include/exclude patterns support standard glob syntax</li> <li>Cost estimation helps budget API usage before sending to LLMs</li> </ol>"},{"location":"CLI/#environment-variables","title":"Environment Variables","text":"<ul> <li><code>TENETS_CONFIG_PATH</code>: Custom config file location</li> <li><code>TENETS_LOG_LEVEL</code>: Set log level (DEBUG, INFO, WARNING, ERROR)</li> <li><code>TENETS_CACHE_DIR</code>: Custom cache directory</li> <li><code>TENETS_NO_COLOR</code>: Disable colored output</li> </ul>"},{"location":"CLI/#exit-codes","title":"Exit Codes","text":"<ul> <li><code>0</code>: Success</li> <li><code>1</code>: General error</li> <li><code>2</code>: Invalid arguments</li> <li><code>3</code>: File not found</li> <li><code>4</code>: Git repository required but not found</li> </ul> <p>For more information, visit https://github.com/jddunn/tenets</p>"},{"location":"CLI/#verbosity--output-controls","title":"Verbosity &amp; Output Controls","text":"<p>Control log verbosity globally:</p> Bash<pre><code># Default (warnings and above only)\nTENETS_LOG_LEVEL=WARNING tenets distill \"add caching layer\"\n\n# Verbose\ntenets --verbose distill \"add caching layer\"\n\n# Quiet / errors only\ntenets --quiet distill \"add caching layer\"\n# or\ntenets --silent distill \"add caching layer\"\n</code></pre> <p>The <code>distill</code> command includes a Suggestions section when no files are included, with tips to adjust relevance thresholds, token budget, and include patterns.</p>"},{"location":"CODE_OF_CONDUCT/","title":"Code of Conduct","text":"<p>We are committed to a respectful, inclusive, and collaborative community. This Code of Conduct applies to all project spaces (GitHub issues/PRs, discussions, docs, chat) and anyone interacting with the project.</p>"},{"location":"CODE_OF_CONDUCT/#our-standards","title":"Our Standards","text":"<p>Positive behaviors: - Being welcoming, empathetic, and considerate - Giving and gracefully accepting constructive feedback - Focusing on what is best for the community - Showing respect for differing viewpoints and experiences</p> <p>Unacceptable behaviors: - Harassment, intimidation, or discrimination of any kind - Personal attacks or insults - Doxxing or sharing private information - Trolling, excessive disruption, or derailing conversations - Sexualized language or imagery; unwelcome advances</p>"},{"location":"CODE_OF_CONDUCT/#scope","title":"Scope","text":"<p>Applies within all project spaces and when representing the project in public.</p>"},{"location":"CODE_OF_CONDUCT/#enforcement","title":"Enforcement","text":"<p>Report incidents to: team@tenets.dev (core maintainers). Include: - Your contact (optional) - Names, links, or references involved - Context, timeline, and any evidence (screenshots, logs)</p> <p>Reports are handled confidentially. Maintainers may take any reasonable action: - Verbal / written warning - Temporary or permanent ban from interactions - Removal of unacceptable content - Escalation to hosting platforms if required</p>"},{"location":"CODE_OF_CONDUCT/#maintainer-responsibilities","title":"Maintainer Responsibilities","text":"<p>Maintainers must model acceptable behavior and are responsible for clarifying standards and taking corrective action when misconduct occurs.</p>"},{"location":"CODE_OF_CONDUCT/#attribution","title":"Attribution","text":"<p>Adapted from the Contributor Covenant v2.1 (https://www.contributor-covenant.org) with project-specific clarifications.</p>"},{"location":"CODE_OF_CONDUCT/#contact","title":"Contact","text":"<p>Questions or concerns: team@tenets.dev</p> <p>We strive for a community where all contributors feel safe and empowered to improve Tenets.</p>"},{"location":"CONFIG/","title":"Configuration Guide","text":"<p>Comprehensive guide to configuring Tenets for optimal code context building.</p>"},{"location":"CONFIG/#overview","title":"Overview","text":"<p>Tenets uses a hierarchical configuration system with multiple override levels:</p> <p>Precedence (lowest \u2192 highest): 1. Default configuration (built-in) 2. Project file (<code>.tenets.yml</code> at repo root) 3. User file (<code>~/.config/tenets/config.yml</code> or <code>~/.tenets.yml</code>) 4. Environment variables (<code>TENETS_*</code>) 5. CLI flags (<code>--mode</code>, <code>--max-tokens</code>, etc.) 6. Programmatic overrides (<code>Tenets(config=...)</code>)</p> <p>Inspect configuration: Bash<pre><code>tenets config show                # Full config\ntenets config show --key ranking  # Specific section\ntenets config show --format json  # JSON output\n</code></pre></p>"},{"location":"CONFIG/#files-and-locations","title":"Files and locations","text":"<p>Tenets searches these locations in order and uses the first it finds: - ./.tenets.yml - ./.tenets.yaml - ./tenets.yml - ./.config/tenets.yml - ~/.config/tenets/config.yml - ~/.tenets.yml</p> <p>Create a starter file:</p> <ul> <li>tenets config init  # writes .tenets.yml in the current directory</li> </ul>"},{"location":"CONFIG/#complete-configuration-schema","title":"Complete Configuration Schema","text":"<p>All available configuration sections and their options:</p> YAML<pre><code># ============= Core Settings =============\nmax_tokens: 100000          # Maximum tokens for context (default: 100000)\ndebug: false                # Enable debug logging\nquiet: false                # Suppress non-essential output\n\n# ============= File Scanning =============\nscanner:\n  respect_gitignore: true          # Honor .gitignore patterns\n  follow_symlinks: false           # Follow symbolic links\n  max_file_size: 5000000          # Max file size in bytes (5MB)\n  max_files: 10000                # Maximum files to scan\n  binary_check: true              # Skip binary files\n  encoding: utf-8                 # File encoding\n  workers: 4                      # Parallel scanning workers\n  parallel_mode: auto             # auto | thread | process\n  timeout: 5.0                    # Timeout per file (seconds)\n  exclude_minified: true          # Skip minified files\n  exclude_tests_by_default: true  # Skip test files unless explicit\n\n  # Ignore patterns (in addition to .gitignore)\n  additional_ignore_patterns:\n    - '*.generated.*'\n    - vendor/\n    - node_modules/\n    - '*.egg-info/'\n    - __pycache__/\n    - .pytest_cache/\n\n  # Test file patterns\n  test_patterns:\n    - test_*.py\n    - '*_test.py'\n    - '*.test.js'\n    - '*.spec.ts'\n\n  # Test directories\n  test_directories:\n    - test\n    - tests\n    - __tests__\n    - spec\n\n# ============= Ranking System =============\nranking:\n  algorithm: balanced             # fast | balanced | thorough | ml | custom\n  threshold: 0.10                 # 0.0-1.0 (lower includes more files)\n  text_similarity_algorithm: bm25 # bm25 (recommended) | tfidf (experimental)\n  # Note: TF-IDF is available for experimentation but BM25 is significantly faster\n  use_stopwords: false           # Filter common tokens\n  use_embeddings: false          # Semantic similarity (requires ML)\n  use_git: true                  # Include git signals\n  use_ml: false                  # Machine learning features\n  embedding_model: all-MiniLM-L6-v2  # Embedding model name\n  workers: 2                     # Parallel ranking workers\n  parallel_mode: auto            # thread | process | auto\n  batch_size: 100               # Files per batch\n\n  # Custom factor weights (0.0-1.0)\n  custom_weights:\n    keyword_match: 0.25\n    path_relevance: 0.20\n    import_graph: 0.20\n    git_activity: 0.15\n    file_type: 0.10\n    complexity: 0.10\n\n# ============= Summarization =============\nsummarizer:\n  default_mode: auto             # auto | extractive | abstractive\n  target_ratio: 0.3              # Target compression ratio\n  enable_cache: true             # Cache summaries\n  preserve_code_structure: true  # Keep code structure intact\n  summarize_imports: true        # Condense import statements\n  import_summary_threshold: 5    # Min imports to trigger summary\n  max_cache_size: 100           # Max cached summaries\n  quality_threshold: medium      # low | medium | high\n  batch_size: 10                # Files per batch\n  docstring_weight: 0.5         # Weight for docstrings\n  include_all_signatures: true   # Include all function signatures\n\n  # LLM settings (optional)\n  llm_provider: null            # openai | anthropic | null\n  llm_model: null               # Model name\n  llm_temperature: 0.3          # Creativity (0.0-1.0)\n  llm_max_tokens: 500           # Max tokens per summary\n  enable_ml_strategies: false    # Use ML summarization\n\n# ============= Tenet System =============\ntenet:\n  auto_instill: true              # Auto-apply tenets\n  max_per_context: 5              # Max tenets per context\n  reinforcement: true             # Reinforce important tenets\n  injection_strategy: strategic   # strategic | sequential | random\n  min_distance_between: 1000      # Min chars between injections\n  prefer_natural_breaks: true     # Insert at natural boundaries\n  storage_path: ~/.tenets/tenets  # Tenet storage location\n  collections_enabled: true       # Enable tenet collections\n\n  # Injection frequency\n  injection_frequency: adaptive   # always | periodic | adaptive | manual\n  injection_interval: 3           # For periodic mode\n  session_complexity_threshold: 0.7  # Triggers adaptive injection\n  min_session_length: 5           # Min prompts before injection\n\n  # Advanced settings\n  adaptive_injection: true        # Smart injection timing\n  track_injection_history: true   # Track what was injected\n  decay_rate: 0.1                # How fast tenets decay\n  reinforcement_interval: 10      # Reinforce every N prompts\n  session_aware: true            # Use session context\n  session_memory_limit: 100      # Max session history\n  persist_session_history: true   # Save session data\n\n  # Priority settings\n  priority_boost_critical: 2.0    # Boost for critical tenets\n  priority_boost_high: 1.5       # Boost for high priority\n  skip_low_priority_on_complex: true  # Skip low priority when complex\n\n  # System instruction\n  system_instruction: null        # Global system instruction\n  system_instruction_enabled: false  # Enable system instruction\n  system_instruction_position: top   # top | bottom\n  system_instruction_format: markdown  # markdown | plain\n  system_instruction_once_per_session: true  # Inject once per session\n\n# ============= Caching =============\ncache:\n  enabled: true                  # Enable caching\n  directory: ~/.tenets/cache     # Cache directory\n  ttl_days: 7                   # Time to live (days)\n  max_size_mb: 500              # Max cache size (MB)\n  compression: false            # Compress cache data\n  memory_cache_size: 1000       # In-memory cache entries\n  max_age_hours: 24            # Max cache age (hours)\n\n  # SQLite settings\n  sqlite_pragmas:\n    journal_mode: WAL\n    synchronous: NORMAL\n    cache_size: '-64000'\n    temp_store: MEMORY\n\n  # LLM cache\n  llm_cache_enabled: true       # Cache LLM responses\n  llm_cache_ttl_hours: 24      # LLM cache TTL\n\n# ============= Output Formatting =============\noutput:\n  default_format: markdown       # markdown | xml | json | html\n  syntax_highlighting: true      # Enable syntax highlighting\n  line_numbers: false           # Show line numbers\n  max_line_length: 120          # Max line length\n  include_metadata: true        # Include metadata\n  compression_threshold: 10000  # Compress if larger (chars)\n  summary_ratio: 0.25           # Summary compression ratio\n  copy_on_distill: false        # Auto-copy to clipboard\n  show_token_usage: true        # Show token counts\n  show_cost_estimate: true      # Show LLM cost estimates\n\n# ============= Git Integration =============\ngit:\n  enabled: true                 # Use git information\n  include_history: true         # Include commit history\n  history_limit: 100           # Max commits to analyze\n  include_blame: false         # Include git blame\n  include_stats: true          # Include statistics\n\n  # Ignore these authors\n  ignore_authors:\n    - dependabot[bot]\n    - github-actions[bot]\n    - renovate[bot]\n\n  # Main branch names\n  main_branches:\n    - main\n    - master\n    - develop\n    - trunk\n\n# ============= NLP Settings =============\nnlp:\n  enabled: true                    # Enable NLP features\n  stopwords_enabled: true          # Use stopwords\n  code_stopword_set: minimal       # minimal | standard | aggressive\n  prompt_stopword_set: aggressive  # minimal | standard | aggressive\n  custom_stopword_files: []        # Custom stopword files\n\n  # Tokenization\n  tokenization_mode: auto          # auto | simple | advanced\n  preserve_original_tokens: true   # Keep original tokens\n  split_camelcase: true           # Split CamelCase\n  split_snakecase: true           # Split snake_case\n  min_token_length: 2             # Min token length\n\n  # Keyword extraction\n  keyword_extraction_method: auto  # auto | rake | yake | tfidf\n  max_keywords: 30                # Max keywords to extract\n  ngram_size: 3                  # N-gram size\n  yake_dedup_threshold: 0.7      # YAKE deduplication\n\n  # Intent keyword filtering - filters common action words from keyword matching\n  filter_intent_keywords: true    # Filter intent action words from file matching\n  custom_intent_keywords: {}      # Custom intent-specific keywords to filter\n\n  # BM25 settings\n  bm25_k1: 1.2                   # Term frequency saturation parameter\n  bm25_b: 0.75                   # Length normalization parameter\n\n  # TF-IDF settings (experimental - use only if needed)\n  tfidf_use_sublinear: true      # Sublinear TF scaling\n  tfidf_use_idf: true           # Use IDF\n  tfidf_norm: l2                # Normalization\n\n  # Embeddings\n  embeddings_enabled: false       # Enable embeddings\n  embeddings_model: all-MiniLM-L6-v2  # Model name\n  embeddings_device: auto        # cpu | cuda | auto\n  embeddings_cache: true         # Cache embeddings\n  embeddings_batch_size: 32      # Batch size\n  similarity_metric: cosine      # cosine | euclidean | manhattan\n  similarity_threshold: 0.7      # Similarity threshold\n\n  # Cache settings\n  cache_embeddings_ttl_days: 30  # Embeddings cache TTL\n  cache_tfidf_ttl_days: 7       # TF-IDF cache TTL (if using TF-IDF)\n  cache_keywords_ttl_days: 7     # Keywords cache TTL\n\n  # Performance\n  multiprocessing_enabled: true   # Use multiprocessing\n  multiprocessing_workers: null   # null = auto-detect\n  multiprocessing_chunk_size: 100 # Chunk size\n\n# ============= LLM Settings (Optional) =============\nllm:\n  enabled: false                # Enable LLM features\n  provider: openai              # openai | anthropic | ollama\n  fallback_providers:           # Fallback providers\n    - anthropic\n    - openrouter\n\n  # API keys (use environment variables)\n  api_keys:\n    openai: ${OPENAI_API_KEY}\n    anthropic: ${ANTHROPIC_API_KEY}\n    openrouter: ${OPENROUTER_API_KEY}\n\n  # API endpoints\n  api_base_urls:\n    openai: https://api.openai.com/v1\n    anthropic: https://api.anthropic.com/v1\n    openrouter: https://openrouter.ai/api/v1\n    ollama: http://localhost:11434\n\n  # Model selection\n  models:\n    default: gpt-4o-mini\n    summarization: gpt-3.5-turbo\n    analysis: gpt-4o\n    embeddings: text-embedding-3-small\n    code_generation: gpt-4o\n\n  # Rate limits and costs\n  max_cost_per_run: 0.1         # Max $ per run\n  max_cost_per_day: 10.0        # Max $ per day\n  max_tokens_per_request: 4000   # Max tokens per request\n  max_context_length: 100000     # Max context length\n\n  # Generation settings\n  temperature: 0.3              # Creativity (0.0-1.0)\n  top_p: 0.95                  # Nucleus sampling\n  frequency_penalty: 0.0        # Frequency penalty\n  presence_penalty: 0.0         # Presence penalty\n\n  # Network settings\n  requests_per_minute: 60       # Rate limit\n  retry_on_error: true         # Retry failed requests\n  max_retries: 3              # Max retry attempts\n  retry_delay: 1.0            # Initial retry delay\n  retry_backoff: 2.0          # Backoff multiplier\n  timeout: 30                 # Request timeout (seconds)\n  stream: false               # Stream responses\n\n  # Logging and caching\n  cache_responses: true        # Cache LLM responses\n  cache_ttl_hours: 24         # Cache TTL (hours)\n  log_requests: false         # Log requests\n  log_responses: false        # Log responses\n\n# ============= Custom Settings =============\ncustom: {}  # User-defined custom settings\n</code></pre>"},{"location":"CONFIG/#key-configuration-notes","title":"Key Configuration Notes","text":"<p>Ranking: - <code>threshold</code>: Lower values (0.05-0.10) include more files, higher (0.20-0.30) for stricter matching - <code>algorithm</code>:   - <code>fast</code>: Quick keyword matching (~10ms/file)   - <code>balanced</code>: Structural analysis + BM25 scoring (default)   - <code>thorough</code>: Full analysis with relationships   - <code>ml</code>: Machine learning with embeddings (requires extras) - <code>custom_weights</code>: Fine-tune ranking factors (values 0.0-1.0)</p> <p>Scanner: - <code>respect_gitignore</code>: Always honors .gitignore patterns - <code>exclude_tests_by_default</code>: Tests excluded unless <code>--include-tests</code> used - <code>additional_ignore_patterns</code>: Added to built-in patterns</p> <p>Tenet System: - <code>auto_instill</code>: Automatically applies relevant tenets to context - <code>injection_frequency</code>:   - <code>always</code>: Every distill   - <code>periodic</code>: Every N distills   - <code>adaptive</code>: Based on complexity   - <code>manual</code>: Only when explicitly called - <code>system_instruction</code>: Global instruction added to all contexts</p> <p>Output: - <code>copy_on_distill</code>: Auto-copy result to clipboard - <code>default_format</code>: Default output format (markdown recommended for LLMs)</p> <p>Performance: - <code>workers</code>: More workers = faster but more CPU/memory - <code>cache.enabled</code>: Significantly speeds up repeated operations - <code>ranking.batch_size</code>: Larger batches = more memory but faster</p>"},{"location":"CONFIG/#environment-variable-overrides","title":"Environment Variable Overrides","text":"<p>Any configuration option can be overridden via environment variables.</p> <p>Format: - Nested keys: <code>TENETS_&lt;SECTION&gt;_&lt;KEY&gt;=value</code> - Top-level keys: <code>TENETS_&lt;KEY&gt;=value</code> - Lists: Comma-separated values - Booleans: <code>true</code> or <code>false</code> (case-insensitive)</p> <p>Common Examples: Bash<pre><code># Core settings\nexport TENETS_MAX_TOKENS=150000\nexport TENETS_DEBUG=true\nexport TENETS_QUIET=false\n\n# Ranking configuration\nexport TENETS_RANKING_ALGORITHM=thorough\nexport TENETS_RANKING_THRESHOLD=0.05\nexport TENETS_RANKING_TEXT_SIMILARITY_ALGORITHM=tfidf  # Use BM25 instead (default)\nexport TENETS_RANKING_USE_EMBEDDINGS=true\nexport TENETS_RANKING_WORKERS=4\n\n# Scanner settings\nexport TENETS_SCANNER_MAX_FILE_SIZE=10000000\nexport TENETS_SCANNER_RESPECT_GITIGNORE=true\nexport TENETS_SCANNER_EXCLUDE_TESTS_BY_DEFAULT=false\n\n# Output settings\nexport TENETS_OUTPUT_DEFAULT_FORMAT=xml\nexport TENETS_OUTPUT_COPY_ON_DISTILL=true\nexport TENETS_OUTPUT_SHOW_TOKEN_USAGE=false\n\n# Cache settings\nexport TENETS_CACHE_ENABLED=false\nexport TENETS_CACHE_DIRECTORY=/tmp/tenets-cache\nexport TENETS_CACHE_TTL_DAYS=14\n\n# Git settings\nexport TENETS_GIT_ENABLED=false\nexport TENETS_GIT_HISTORY_LIMIT=50\n\n# Tenet system\nexport TENETS_TENET_AUTO_INSTILL=false\nexport TENETS_TENET_MAX_PER_CONTEXT=10\nexport TENETS_TENET_INJECTION_FREQUENCY=periodic\nexport TENETS_TENET_INJECTION_INTERVAL=5\n\n# System instruction\nexport TENETS_TENET_SYSTEM_INSTRUCTION=\"You are a senior engineer. Focus on security and performance.\"\nexport TENETS_TENET_SYSTEM_INSTRUCTION_ENABLED=true\n</code></pre></p> <p>Usage Patterns: Bash<pre><code># One-time override\nTENETS_RANKING_ALGORITHM=fast tenets distill \"fix bug\"\n\n# Session-wide settings\nexport TENETS_RANKING_THRESHOLD=0.05\nexport TENETS_OUTPUT_COPY_ON_DISTILL=true\ntenets distill \"implement feature\"  # Uses exported settings\n\n# Verify configuration\ntenets config show --key ranking\ntenets config show --format json | jq '.ranking'\n</code></pre></p>"},{"location":"CONFIG/#cli-flags-and-programmatic-control","title":"CLI Flags and Programmatic Control","text":""},{"location":"CONFIG/#cli-flags","title":"CLI Flags","text":"<p>Command-line flags override configuration for that specific run:</p> Bash<pre><code># Core overrides\ntenets distill \"query\" --max-tokens 50000\ntenets distill \"query\" --format xml\ntenets distill \"query\" --copy\n\n# Ranking mode\ntenets distill \"query\" --mode fast      # Quick analysis\ntenets distill \"query\" --mode thorough  # Deep analysis\ntenets distill \"query\" --mode ml        # With embeddings\n\n# File filtering\ntenets distill \"query\" --include \"*.py\" --exclude \"test_*.py\"\ntenets distill \"query\" --include-tests  # Include test files\n\n# Git control\ntenets distill \"query\" --no-git  # Disable git signals\n\n# Session management\ntenets distill \"query\" --session feature-x\n\n# Content optimization\ntenets distill \"query\" --condense        # Aggressive compression\ntenets distill \"query\" --remove-comments # Strip comments\ntenets distill \"query\" --full            # No summarization\n</code></pre>"},{"location":"CONFIG/#programmatic-configuration","title":"Programmatic Configuration","text":"<p>Basic usage with custom config: Python<pre><code>from tenets import Tenets\nfrom tenets.config import TenetsConfig\n\n# Create custom configuration\nconfig = TenetsConfig(\n    max_tokens=150000,\n    ranking={\n        \"algorithm\": \"thorough\",\n        \"threshold\": 0.05,\n        \"text_similarity_algorithm\": \"bm25\",  # Recommended\n        \"use_embeddings\": True,\n        \"workers\": 4,\n        \"custom_weights\": {\n            \"keyword_match\": 0.30,\n            \"path_relevance\": 0.25,\n            \"git_activity\": 0.20,\n        }\n    },\n    scanner={\n        \"respect_gitignore\": True,\n        \"max_file_size\": 10_000_000,\n        \"exclude_tests_by_default\": False,\n    },\n    output={\n        \"default_format\": \"xml\",\n        \"copy_on_distill\": True,\n    },\n    tenet={\n        \"auto_instill\": True,\n        \"max_per_context\": 10,\n        \"system_instruction\": \"Focus on security and performance\",\n        \"system_instruction_enabled\": True,\n    }\n)\n\n# Initialize with custom config\ntenets = Tenets(config=config)\n\n# Use it\nresult = tenets.distill(\n    \"implement caching layer\",\n    max_tokens=80000,  # Override config for this call\n    mode=\"balanced\",    # Override algorithm\n)\n</code></pre></p> <p>Load and modify existing config: Python<pre><code>from tenets import Tenets\nfrom tenets.config import TenetsConfig\n\n# Load from file\nconfig = TenetsConfig.from_file(\".tenets.yml\")\n\n# Modify specific settings\nconfig.ranking.algorithm = \"fast\"\nconfig.ranking.threshold = 0.08\nconfig.output.copy_on_distill = True\n\n# Use modified config\ntenets = Tenets(config=config)\n</code></pre></p> <p>Runtime overrides: Python<pre><code># Config precedence: method args &gt; instance config &gt; file config\nresult = tenets.distill(\n    prompt=\"add authentication\",\n    mode=\"thorough\",        # Overrides config.ranking.algorithm\n    max_tokens=100000,      # Overrides config.max_tokens\n    format=\"json\",          # Overrides config.output.default_format\n    session_name=\"auth\",    # Session-specific\n    include_patterns=[\"*.py\", \"*.js\"],\n    exclude_patterns=[\"*.test.js\"],\n)\n</code></pre></p>"},{"location":"CONFIG/#configuration-recipes","title":"Configuration Recipes","text":""},{"location":"CONFIG/#for-different-use-cases","title":"For Different Use Cases","text":"<p>Large Monorepo (millions of files): YAML<pre><code>max_tokens: 150000\nscanner:\n  max_files: 50000\n  workers: 8\n  parallel_mode: process\n  exclude_tests_by_default: true\nranking:\n  algorithm: fast\n  threshold: 0.15\n  workers: 4\n  batch_size: 500\ncache:\n  enabled: true\n  memory_cache_size: 5000\n</code></pre></p> <p>Small Project (high precision): YAML<pre><code>max_tokens: 80000\nranking:\n  algorithm: thorough\n  threshold: 0.08\n  text_similarity_algorithm: bm25  # Recommended over tfidf\n  use_embeddings: true\n  custom_weights:\n    keyword_match: 0.35\n    import_graph: 0.25\n</code></pre></p> <p>Documentation-Heavy Project: YAML<pre><code>summarizer:\n  docstring_weight: 0.8\n  include_all_signatures: true\n  preserve_code_structure: false\nranking:\n  custom_weights:\n    keyword_match: 0.20\n    path_relevance: 0.30  # Prioritize doc paths\n</code></pre></p> <p>Security-Focused Analysis: YAML<pre><code>tenet:\n  system_instruction: |\n    Focus on security implications.\n    Flag any potential vulnerabilities.\n    Suggest secure alternatives.\n  system_instruction_enabled: true\n  auto_instill: true\nscanner:\n  additional_ignore_patterns: []  # Don't skip anything\n  exclude_tests_by_default: false\n</code></pre></p>"},{"location":"CONFIG/#performance-tuning","title":"Performance Tuning","text":"<p>Maximum Speed (sacrifices precision): YAML<pre><code>ranking:\n  algorithm: fast\n  threshold: 0.05\n  text_similarity_algorithm: bm25  # Default\n  use_embeddings: false\n  workers: 8\nscanner:\n  workers: 8\n  timeout: 2.0\ncache:\n  enabled: true\n  compression: false\n</code></pre></p> <p>Maximum Precision (slower): YAML<pre><code>ranking:\n  algorithm: thorough\n  threshold: 0.20\n  text_similarity_algorithm: bm25  # Recommended over tfidf\n  use_embeddings: true\n  use_git: true\n  workers: 2\nsummarizer:\n  quality_threshold: high\n  enable_ml_strategies: true\n</code></pre></p> <p>Memory-Constrained Environment: YAML<pre><code>scanner:\n  max_files: 1000\n  workers: 1\nranking:\n  workers: 1\n  batch_size: 50\ncache:\n  memory_cache_size: 100\n  max_size_mb: 100\nnlp:\n  embeddings_batch_size: 8\n  multiprocessing_enabled: false\n</code></pre></p>"},{"location":"CONFIG/#common-workflows","title":"Common Workflows","text":"<p>Bug Investigation: YAML<pre><code>ranking:\n  algorithm: balanced\n  threshold: 0.10\n  custom_weights:\n    git_activity: 0.30  # Recent changes matter\n    complexity: 0.20    # Complex code = more bugs\ngit:\n  include_history: true\n  history_limit: 200\n  include_blame: true\n</code></pre></p> <p>New Feature Development: YAML<pre><code>ranking:\n  algorithm: balanced\n  threshold: 0.08\n  custom_weights:\n    import_graph: 0.30  # Dependencies matter\n    path_relevance: 0.25 # Related modules\noutput:\n  copy_on_distill: true\n  show_token_usage: true\n</code></pre></p> <p>Code Review Preparation: YAML<pre><code>summarizer:\n  target_ratio: 0.5  # More detail\n  preserve_code_structure: true\n  include_all_signatures: true\noutput:\n  syntax_highlighting: true\n  line_numbers: true\n  include_metadata: true\n</code></pre></p>"},{"location":"CONFIG/#troubleshooting","title":"Troubleshooting","text":""},{"location":"CONFIG/#common-issues-and-solutions","title":"Common Issues and Solutions","text":"<p>No files included in context: - Lower <code>ranking.threshold</code> (try 0.05) - Use <code>--mode fast</code> for broader inclusion - Increase <code>max_tokens</code> limit - Check if files match <code>--include</code> patterns - Verify files aren't in <code>.gitignore</code> - Use <code>--include-tests</code> if analyzing test files</p> <p>Configuration not taking effect: Bash<pre><code># Check which config file is loaded\ntenets config show | head -20\n\n# Verify specific setting\ntenets config show --key ranking.threshold\n\n# Check config file location\nls -la .tenets.yml\n\n# Test with explicit config\ntenets --config ./my-config.yml distill \"query\"\n</code></pre></p> <p>Environment variables not working: Bash<pre><code># Verify export (not just set)\nexport TENETS_RANKING_THRESHOLD=0.05  # Correct\nTENETS_RANKING_THRESHOLD=0.05         # Wrong (not exported)\n\n# Check if variable is set\necho $TENETS_RANKING_THRESHOLD\n\n# Debug with explicit env\nTENETS_DEBUG=true tenets config show\n</code></pre></p> <p>Performance issues: - Reduce <code>scanner.max_files</code> and <code>scanner.max_file_size</code> - Enable caching: <code>cache.enabled: true</code> - Use <code>ranking.algorithm: fast</code> - Reduce <code>ranking.workers</code> if CPU-constrained - Exclude unnecessary paths with <code>additional_ignore_patterns</code></p> <p>Token limit exceeded: - Increase <code>max_tokens</code> or use <code>--max-tokens</code> - Enable <code>--condense</code> flag - Use <code>--remove-comments</code> - Increase <code>ranking.threshold</code> for stricter filtering - Exclude test files: <code>scanner.exclude_tests_by_default: true</code></p> <p>Cache issues: Bash<pre><code># Clear cache\nrm -rf ~/.tenets/cache\n\n# Disable cache temporarily\nTENETS_CACHE_ENABLED=false tenets distill \"query\"\n\n# Use custom cache location\nexport TENETS_CACHE_DIRECTORY=/tmp/tenets-cache\n</code></pre></p>"},{"location":"CONFIG/#validation-commands","title":"Validation Commands","text":"Bash<pre><code># Validate configuration syntax\ntenets config validate\n\n# Show effective configuration\ntenets config show --format json | jq\n\n# Test configuration with dry run\ntenets distill \"test query\" --dry-run\n\n# Check what files would be scanned\ntenets examine . --dry-run\n\n# Debug ranking process\nTENETS_DEBUG=true tenets distill \"query\" 2&gt;debug.log\n</code></pre>"},{"location":"CONFIG/#advanced-topics","title":"Advanced Topics","text":""},{"location":"CONFIG/#intent-keyword-filtering","title":"Intent Keyword Filtering","text":"<p>Tenets can intelligently filter common action words (like \"fix\", \"debug\", \"implement\") from keyword matching while preserving them for intent detection. This prevents generic action words from affecting file ranking while maintaining accurate intent classification.</p> <p>How it works: - Action words are detected from intent patterns - These words are filtered from keyword matching for file ranking - Domain-specific terms are always preserved - Configurable per-intent with custom keywords</p> <p>Configuration: YAML<pre><code>nlp:\n  # Enable/disable intent keyword filtering\n  filter_intent_keywords: true  # Default: true\n\n  # Custom intent-specific keywords to filter\n  custom_intent_keywords:\n    debug:\n      - custom_debug_word\n      - another_debug_term\n    implement:\n      - custom_impl_word\n</code></pre></p> <p>Example scenario: - Query: \"debug the tokenizing issue in the parser\" - Intent detected: \"debug\" - Keywords before filtering: [\"debug\", \"tokenizing\", \"issue\", \"parser\"] - Keywords after filtering: [\"tokenizing\", \"parser\"] - Result: Files are ranked based on \"tokenizing\" and \"parser\", not generic \"debug\" or \"issue\"</p> <p>Benefits: - More accurate file ranking - Reduces noise from common action words - Preserves domain-specific terminology - Maintains intent detection accuracy</p>"},{"location":"CONFIG/#custom-ranking-strategies","title":"Custom Ranking Strategies","text":"<p>Create a custom ranking strategy by combining weights:</p> YAML<pre><code>ranking:\n  algorithm: custom\n  custom_weights:\n    keyword_match: 0.40    # Emphasize keyword relevance\n    path_relevance: 0.15   # De-emphasize path matching\n    import_graph: 0.15     # Moderate dependency weight\n    git_activity: 0.10     # Low git signal weight\n    file_type: 0.10        # File type matching\n    complexity: 0.10       # Code complexity\n</code></pre>"},{"location":"CONFIG/#multi-environment-setup","title":"Multi-Environment Setup","text":"<p>Create environment-specific configs:</p> Bash<pre><code># Development\ncp .tenets.yml .tenets.dev.yml\n# Edit for dev settings\n\n# Production analysis\ncp .tenets.yml .tenets.prod.yml\n# Edit for production settings\n\n# Use specific config\ntenets --config .tenets.dev.yml distill \"query\"\n</code></pre>"},{"location":"CONFIG/#integration-with-cicd","title":"Integration with CI/CD","text":"YAML<pre><code># .tenets.ci.yml - Optimized for CI\nmax_tokens: 50000\nquiet: true\nscanner:\n  max_files: 5000\n  workers: 2\nranking:\n  algorithm: fast\n  threshold: 0.10\ncache:\n  enabled: false  # Fresh analysis each run\noutput:\n  default_format: json  # Machine-readable\n</code></pre>"},{"location":"CONFIG/#see-also","title":"See Also","text":"<ul> <li>CLI Reference - Complete command documentation</li> <li>API Reference - Python API documentation</li> <li>Architecture - System design details</li> </ul>"},{"location":"CONTRIBUTING/","title":"Contributing to Tenets","text":"<p>Thanks for your interest in improving Tenets! Contributions of all kinds are welcome: bug reports, docs, tests, features, performance improvements, refactors, and feedback.</p>"},{"location":"CONTRIBUTING/#quick-start-tldr","title":"Quick Start (TL;DR)","text":"Bash<pre><code># Fork / clone\n git clone https://github.com/jddunn/tenets.git\n cd tenets\n\n# Create a virtual environment (or use pyenv / conda)\n python -m venv .venv &amp;&amp; source .venv/bin/activate  # Windows: .venv\\Scripts\\activate\n\n# Install core + dev extras\n pip install -e .[dev]\n # (or: make dev)\n\n# Run tests\n pytest -q\n\n# Lint &amp; type check\n ruff check .\n mypy tenets\n\n# Format\n black .\n\n# Run a sample command\n tenets distill \"hello world\" . --stats\n</code></pre>"},{"location":"CONTRIBUTING/#project-philosophy","title":"Project Philosophy","text":"<p>Tenets is: - Local-first, privacy-preserving - Fast with graceful scalability (analyze only as deep as necessary) - Extensible without forcing heavyweight ML (opt-in extras) - Transparent in ranking decisions (explanations where reasonable)</p>"},{"location":"CONTRIBUTING/#issue-tracking","title":"Issue Tracking","text":"<p>Before filing: 1. Search existing issues (open + closed) 2. For questions / ideas, consider starting a GitHub Discussion (if enabled) or Discord 3. Provide reproduction steps and environment info (OS, Python version, extras installed)</p> <p>Good bug report template: Text Only<pre><code>### Description\nClear, concise description of the problem.\n\n### Reproduction\nCommands or code snippet that reproduces the issue.\n\n### Expected vs Actual\nWhat you expected / what happened.\n\n### Environment\nOS / Python / tenets version / installed extras.\n</code></pre></p>"},{"location":"CONTRIBUTING/#branch--commit-conventions","title":"Branch &amp; Commit Conventions","text":"<ul> <li>Create feature branches off <code>dev</code> (default contribution branch)</li> <li>Keep PRs narrowly scoped when possible</li> <li>Conventional Commit prefixes (enforced via commitizen config):</li> <li>feat: new user-facing feature</li> <li>fix: bug fix</li> <li>refactor: code change without feature/bug semantics</li> <li>perf: performance improvement</li> <li>docs: docs only changes</li> <li>test: add or improve tests</li> <li>chore: tooling / infra / build</li> </ul> <p>Example: Text Only<pre><code>feat(ranking): add parallel TF-IDF corpus prepass\n</code></pre></p> <p>Use <code>cz commit</code> if you have commitizen installed.</p>"},{"location":"CONTRIBUTING/#code-style--tooling","title":"Code Style &amp; Tooling","text":"Tool Purpose Command black Formatting <code>black .</code> ruff Linting (multi-plugin) <code>ruff check .</code> mypy Static typing <code>mypy tenets</code> pytest Tests + coverage <code>pytest -q</code> coverage HTML / XML reports <code>pytest --cov</code> commitizen Conventional versioning <code>cz bump</code> <p>Pre-commit hooks (optional): Bash<pre><code>pip install pre-commit\npre-commit install\n</code></pre></p>"},{"location":"CONTRIBUTING/#tests","title":"Tests","text":"<p>Guidelines: - Place tests under <code>tests/</code> mirroring module paths - Use <code>pytest</code> fixtures; prefer explicit data over deep mocks - Mark slow tests with <code>@pytest.mark.slow</code> - Keep unit tests fast (&lt;300ms ideally) - Add at least one failing test before a bug fix</p> <p>Run selectively: Bash<pre><code>pytest tests/core/analysis -k python_analyzer\npytest -m \"not slow\"\n</code></pre></p>"},{"location":"CONTRIBUTING/#type-hints","title":"Type Hints","text":"<ul> <li>New/modified public functions must be fully typed</li> <li>Avoid <code>Any</code> unless absolutely necessary; justify in a comment</li> <li>mypy config is strict\u2014fix or silence with narrow <code># type: ignore[...]</code></li> </ul>"},{"location":"CONTRIBUTING/#documentation","title":"Documentation","text":"<p>User docs live in <code>docs/</code> (MkDocs Material). For changes affecting users: - Update <code>README.md</code> - Update or create relevant page under <code>docs/</code> - Add examples (<code>quickstart.md</code>) if CLI/API behavior changes - Link new pages in <code>mkdocs.yml</code></p> <p>Serve docs locally: Bash<pre><code>mkdocs serve\n</code></pre></p>"},{"location":"CONTRIBUTING/#adding-a-language-analyzer","title":"Adding a Language Analyzer","text":"<ol> <li>Create <code>&lt;language&gt;_analyzer.py</code> under <code>tenets/core/analysis/implementations/</code></li> <li>Subclass <code>LanguageAnalyzer</code></li> <li>Implement <code>match(path)</code> and <code>analyze(content)</code></li> <li>Add tests under <code>tests/core/analysis/implementations/</code></li> <li>Update <code>supported-languages.md</code></li> </ol>"},{"location":"CONTRIBUTING/#ranking-extensions","title":"Ranking Extensions","text":"<ul> <li>Register custom rankers via provided registration API (see <code>tenets/core/ranking/ranker.py</code>)</li> <li>Provide deterministic output; avoid network calls in ranking stage</li> <li>Document new algorithm flags in <code>config.md</code></li> </ul>"},{"location":"CONTRIBUTING/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Avoid O(n^2) scans over file lists when possible</li> <li>Cache expensive analysis (see existing caching layer)</li> <li>Add benchmarks if adding heavy operations (future / optional)</li> </ul>"},{"location":"CONTRIBUTING/#security--privacy","title":"Security / Privacy","text":"<ul> <li>Never exfiltrate code or send network requests without explicit user config</li> <li>Keep default extras minimal</li> </ul>"},{"location":"CONTRIBUTING/#release-process-maintainers","title":"Release Process (Maintainers)","text":"<ol> <li>Ensure <code>dev</code> is green (CI + coverage)</li> <li>Bump version: <code>cz bump</code> (updates <code>pyproject.toml</code>, tag, CHANGELOG)</li> <li>Build: <code>make build</code> (or <code>python -m build</code>)</li> <li>Publish: <code>twine upload dist/*</code></li> <li>Merge <code>dev</code> -&gt; <code>master</code> and push tags</li> </ol>"},{"location":"CONTRIBUTING/#code-of-conduct","title":"Code of Conduct","text":"<p>This project follows the Code of Conduct. By participating you agree to uphold it.</p>"},{"location":"CONTRIBUTING/#license","title":"License","text":"<p>By contributing you agree your contributions are licensed under the MIT License.</p> <p>Questions? Open an issue or reach out via Discord.</p>"},{"location":"DEPLOYMENT/","title":"Deployment Guide","text":"<p>This guide outlines the process for releasing new versions of Tenets to PyPI and deploying documentation.</p>"},{"location":"DEPLOYMENT/#release-process-automated","title":"Release Process (Automated)","text":"<p>Standard path: merge conventional commits into <code>main</code>; automation versions &amp; publishes.</p>"},{"location":"DEPLOYMENT/#how-it-works","title":"How It Works","text":"<ol> <li>Merge PR \u2192 <code>version-bump.yml</code> runs</li> <li>Determines bump size (major / minor / patch / skip) from commit messages</li> <li>Updates <code>pyproject.toml</code> + appends grouped section to <code>CHANGELOG.md</code></li> <li>Commits <code>chore(release): vX.Y.Z</code> and tags <code>vX.Y.Z</code></li> <li>Tag triggers <code>release.yml</code>: build, publish to PyPI, (future) Docker, docs deploy</li> <li>Release notes composed from changelog / draft config</li> </ol>"},{"location":"DEPLOYMENT/#bump-rules-summary","title":"Bump Rules (Summary)","text":"Commit Types Seen Result BREAKING CHANGE / <code>!</code> Major feat / perf Minor fix / refactor / chore Patch (unless higher trigger present) Only docs / test / style Skip"},{"location":"DEPLOYMENT/#manual-overrides-rare","title":"Manual Overrides (Rare)","text":"<p>If automation blocked (workflow infra outage): Bash<pre><code>git checkout main &amp;&amp; git pull\ncz bump --increment PATCH  # or MINOR / MAJOR\ngit push &amp;&amp; git push --tags\n</code></pre> Resume automation next merge.</p>"},{"location":"DEPLOYMENT/#first-release-bootstrap-v010","title":"First Release Bootstrap (v0.1.0)","text":"<p>For the initial v0.1.0 release, follow this manual process:</p> <ol> <li>Update CHANGELOG.md with v0.1.0 entries (on dev branch)</li> <li>Commit and push to dev: <code>git commit -m \"docs: update CHANGELOG for v0.1.0\"</code></li> <li>Merge dev \u2192 master</li> <li>From master, create and push tag: Bash<pre><code>git checkout master &amp;&amp; git pull\ngit tag -a v0.1.0 -m \"Release v0.1.0 - Initial public release\"\ngit push origin v0.1.0  # This triggers everything!\n</code></pre></li> <li>The tag push automatically triggers:</li> <li>GitHub Release creation with artifacts</li> <li>PyPI package publishing (if PYPI_API_TOKEN is set)</li> <li>Documentation deployment to GitHub Pages</li> </ol> <p>After v0.1.0: Automation takes over - commits trigger version bumps based on conventional commit messages.</p>"},{"location":"DEPLOYMENT/#verification-checklist","title":"Verification Checklist","text":"Step Command / Action Install published wheel <code>pip install --no-cache-dir tenets==X.Y.Z</code> CLI version matches <code>tenets --version</code> Release notes present Check GitHub Release page Docs updated Visit docs site / gh-pages commit"},{"location":"DEPLOYMENT/#troubleshooting","title":"Troubleshooting","text":"Issue Cause Resolution No tag created Only docs/test/style commits Land a fix/feat/perf commit Wrong bump size Mis-typed commit message Amend &amp; force push before merge; or follow-up commit PyPI publish failed Missing PyPI token / trust approval pending Add <code>PYPI_API_TOKEN</code> or approve trusted publisher Duplicate releases Manual tag + automated tag Avoid manual tagging unless emergency"},{"location":"DEPLOYMENT/#documentation-deployment","title":"Documentation Deployment","text":"<p>Docs are (a) built in CI on PR for validation; (b) deployed on release tag push by <code>release.yml</code> (or dedicated docs deploy step on main). GitHub Pages serves from <code>gh-pages</code>.</p>"},{"location":"DEPLOYMENT/#required--optional-secrets","title":"Required / Optional Secrets","text":"Secret Required Purpose Notes <code>PYPI_API_TOKEN</code> Yes* PyPI publish in <code>release.yml</code> *Omit if using Trusted Publishing (approve first build). <code>CODECOV_TOKEN</code> Public: often no / Private: yes Coverage uploads Set to be explicit. <code>GOOGLE_ANALYTICS_ID</code> Optional GA4 measurement ID for docs analytics Used by MkDocs Material via <code>!ENV</code> in <code>mkdocs.yml</code> (e.g., <code>G-XXXXXXXXXX</code>). If unset/empty, analytics are disabled. <code>DOCKER_USERNAME</code> / <code>DOCKER_TOKEN</code> Optional Future Docker image publishing Not required yet. <code>GH_PAT</code> No Cross-repo automation (not standard) Avoid storing if unused. <p>Environment (optional): <code>TENETS_DEBUG</code>, <code>TENETS_CACHE_DIRECTORY</code>.</p>"},{"location":"DEPLOYMENT/#google-analytics-optional","title":"Google Analytics (optional)","text":"<p>MkDocs Material analytics are wired to an environment variable:</p> <ul> <li>In <code>mkdocs.yml</code>: <code>extra.analytics.property: !ENV [GOOGLE_ANALYTICS_ID, \"\"]</code></li> <li>Provide a GA4 Measurement ID (format <code>G-XXXXXXXXXX</code>). If the variable is unset or empty, analytics are disabled automatically.</li> </ul> <p>Local usage</p> Bash<pre><code># bash / Git Bash / WSL\nexport GOOGLE_ANALYTICS_ID=G-XXXXXXXXXX\nmkdocs serve\n</code></pre> PowerShell<pre><code># PowerShell\n$env:GOOGLE_ANALYTICS_ID = 'G-XXXXXXXXXX'\nmkdocs serve\n</code></pre> <p>GitHub Actions (recommended)</p> YAML<pre><code>jobs:\n   docs:\n      runs-on: ubuntu-latest\n      env:\n         GOOGLE_ANALYTICS_ID: ${{ secrets.GOOGLE_ANALYTICS_ID }}\n      steps:\n         - uses: actions/checkout@v4\n         - uses: actions/setup-python@v5\n            with:\n               python-version: '3.12'\n         - run: pip install -e '.[docs]'\n         - run: mkdocs build --clean\n</code></pre> <p>Store your GA4 Measurement ID as a repository secret named <code>GOOGLE_ANALYTICS_ID</code>. The docs build will inject it at build time; if not present, analytics are off.</p>"},{"location":"DEPLOYMENT/#with-specific-features","title":"With specific features","text":"<p>pip install tenets[ml]  # ML features pip install tenets[viz]  # Visualization pip install tenets[all]  # Everything Text Only<pre><code>### 2. Development Installation\n\n```bash\n# From source\ngit clone https://github.com/jddunn/tenets.git\ncd tenets\npip install -e \".[dev]\"\n</code></pre></p>"},{"location":"DEPLOYMENT/#3-docker-container","title":"3. Docker Container","text":"Bash<pre><code># Pull from Docker Hub\ndocker pull tenets/tenets:latest\n\n# Run command\ndocker run --rm -v $(pwd):/workspace tenets/tenets make-context \"query\" .\n\n# Interactive shell\ndocker run -it --rm -v $(pwd):/workspace tenets/tenets bash\n</code></pre>"},{"location":"DEPLOYMENT/#4-standalone-binary","title":"4. Standalone Binary","text":"<p>Download from GitHub Releases:</p> Bash<pre><code># Linux/macOS\ncurl -L https://github.com/jddunn/tenets/releases/latest/download/tenets-linux -o tenets\nchmod +x tenets\n./tenets --version\n\n# Windows\n# Download tenets-windows.exe from releases page\n</code></pre>"},{"location":"DEPLOYMENT/#pypi-publishing","title":"PyPI Publishing","text":""},{"location":"DEPLOYMENT/#first-time-setup","title":"First-Time Setup","text":"<ol> <li>Create PyPI account:</li> <li>Register at pypi.org</li> <li> <p>Enable 2FA (required)</p> </li> <li> <p>Configure trusted publishing:</p> </li> <li>Go to your project settings on PyPI</li> <li>Add GitHub Actions as trusted publisher:<ul> <li>Owner: <code>jddunn</code></li> <li>Repository: <code>tenets</code></li> <li>Workflow: <code>release.yml</code></li> <li>Environment: <code>pypi</code></li> </ul> </li> </ol>"},{"location":"DEPLOYMENT/#manual-publishing-emergency-only","title":"Manual Publishing (Emergency Only)","text":"Bash<pre><code># Build distribution\npython -m build\n\n# Check package\ntwine check dist/*\n\n# Upload to TestPyPI first\ntwine upload --repository testpypi dist/*\n\n# Test installation\npip install --index-url https://test.pypi.org/simple/ tenets\n\n# Upload to PyPI\ntwine upload dist/*\n</code></pre>"},{"location":"DEPLOYMENT/#docker-deployment","title":"Docker Deployment","text":""},{"location":"DEPLOYMENT/#building-images","title":"Building Images","text":"Docker<pre><code># Dockerfile\nFROM python:3.11-slim\n\n# Install system dependencies\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    git \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\n# Create non-root user\nRUN useradd -m -s /bin/bash tenets\n\n# Set working directory\nWORKDIR /app\n\n# Install tenets\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\nCOPY . .\nRUN pip install --no-cache-dir -e .\n\n# Switch to non-root user\nUSER tenets\n\n# Set entrypoint\nENTRYPOINT [\"tenets\"]\n</code></pre>"},{"location":"DEPLOYMENT/#multi-architecture-build","title":"Multi-Architecture Build","text":"Bash<pre><code># Setup buildx\ndocker buildx create --use\n\n# Build for multiple platforms\ndocker buildx build \\\n  --platform linux/amd64,linux/arm64 \\\n  --tag tenets/tenets:latest \\\n  --tag tenets/tenets:v0.1.0 \\\n  --push .\n</code></pre>"},{"location":"DEPLOYMENT/#docker-compose","title":"Docker Compose","text":"YAML<pre><code># docker-compose.yml\nversion: '3.8'\n\nservices:\n  tenets:\n    image: tenets/tenets:latest\n    volumes:\n      - .:/workspace\n      - ~/.tenets:/home/tenets/.tenets\n    working_dir: /workspace\n    environment:\n      - TENETS_LOG_LEVEL=INFO\n    command: make-context \"implement feature\" .\n</code></pre>"},{"location":"DEPLOYMENT/#binary-distribution","title":"Binary Distribution","text":""},{"location":"DEPLOYMENT/#building-binaries","title":"Building Binaries","text":"Bash<pre><code># Install PyInstaller\npip install pyinstaller\n\n# Build for current platform\npyinstaller \\\n  --onefile \\\n  --name tenets \\\n  --add-data \"tenets:tenets\" \\\n  --hidden-import tenets.core \\\n  --hidden-import tenets.models \\\n  --hidden-import tenets.utils \\\n  tenets/__main__.py\n\n# Output in dist/tenets\n</code></pre>"},{"location":"DEPLOYMENT/#cross-platform-building","title":"Cross-Platform Building","text":"<p>Use GitHub Actions for multi-platform builds: - Linux: Ubuntu runner - macOS: macOS runner - Windows: Windows runner</p>"},{"location":"DEPLOYMENT/#code-signing-optional","title":"Code Signing (Optional)","text":"Bash<pre><code># macOS\ncodesign --deep --force --verify --verbose \\\n  --sign \"Developer ID Application: Your Name\" \\\n  dist/tenets\n\n# Windows (using signtool)\nsigntool sign /t http://timestamp.digicert.com dist/tenets.exe\n</code></pre>"},{"location":"DEPLOYMENT/#documentation-deployment_1","title":"Documentation Deployment","text":""},{"location":"DEPLOYMENT/#building-documentation","title":"Building Documentation","text":"Bash<pre><code># Install dependencies\npip install -e \".[docs]\"\n\n# Build docs\nmkdocs build\n\n# Test locally\nmkdocs serve\n</code></pre>"},{"location":"DEPLOYMENT/#versioned-documentation","title":"Versioned Documentation","text":"Bash<pre><code># Deploy new version\nmike deploy --push --update-aliases 0.1.0 latest\n\n# Deploy development docs\nmike deploy --push dev\n\n# Set default version\nmike set-default --push latest\n</code></pre>"},{"location":"DEPLOYMENT/#github-pages-setup","title":"GitHub Pages Setup","text":"<ol> <li>Enable GitHub Pages in repository settings</li> <li>Set source to <code>gh-pages</code> branch</li> <li>Documentation auto-deploys on release</li> </ol>"},{"location":"DEPLOYMENT/#security-considerations","title":"Security Considerations","text":""},{"location":"DEPLOYMENT/#release-security","title":"Release Security","text":"<ol> <li> <p>Sign commits and tags:    Bash<pre><code>git config --global commit.gpgsign true\ngit config --global tag.gpgsign true\n</code></pre></p> </li> <li> <p>Verify dependencies:    Bash<pre><code># Check for vulnerabilities\nsafety check\n\n# Audit dependencies\npip-audit\n</code></pre></p> </li> <li> <p>Scan for secrets:    Bash<pre><code># Pre-release scan\ndetect-secrets scan --all-files\n</code></pre></p> </li> </ol>"},{"location":"DEPLOYMENT/#deployment-security","title":"Deployment Security","text":"<ol> <li> <p>Use minimal base images:    Docker<pre><code>FROM python:3.11-slim  # Not full python image\n</code></pre></p> </li> <li> <p>Run as non-root:    Docker<pre><code>USER nobody\n</code></pre></p> </li> <li> <p>Scan images:    Bash<pre><code># Scan for vulnerabilities\ndocker scan tenets/tenets:latest\n</code></pre></p> </li> </ol>"},{"location":"DEPLOYMENT/#monitoring--maintenance","title":"Monitoring &amp; Maintenance","text":""},{"location":"DEPLOYMENT/#release-monitoring","title":"Release Monitoring","text":"<ol> <li>PyPI Statistics:</li> <li>Check download stats</li> <li> <p>Monitor for unusual activity</p> </li> <li> <p>GitHub Insights:</p> </li> <li>Track clone/download metrics</li> <li> <p>Monitor issue trends</p> </li> <li> <p>Error Tracking:</p> </li> <li>Set up Sentry (optional)</li> <li>Monitor GitHub issues</li> </ol>"},{"location":"DEPLOYMENT/#maintenance-tasks","title":"Maintenance Tasks","text":""},{"location":"DEPLOYMENT/#weekly","title":"Weekly","text":"<ul> <li>Review and triage issues</li> <li>Check for security advisories</li> <li>Update dependencies</li> </ul>"},{"location":"DEPLOYMENT/#monthly","title":"Monthly","text":"<ul> <li>Review performance metrics</li> <li>Update documentation</li> <li>Clean up old releases</li> </ul>"},{"location":"DEPLOYMENT/#quarterly","title":"Quarterly","text":"<ul> <li>Major dependency updates</li> <li>Security audit</li> <li>Performance benchmarking</li> </ul>"},{"location":"DEPLOYMENT/#rollback-procedure","title":"Rollback Procedure","text":"<p>If a release has critical issues:</p> <ol> <li> <p>Yank from PyPI (last resort):    Bash<pre><code># This prevents new installations\n# Existing installations continue to work\ntwine yank tenets==0.1.0\n</code></pre></p> </li> <li> <p>Create hotfix:    Bash<pre><code>git checkout -b hotfix/critical-bug\n# Fix issue\ngit commit -m \"fix: critical bug in analyzer\"\ncz bump --increment PATCH\ngit push origin hotfix/critical-bug\n</code></pre></p> </li> <li> <p>Fast-track release:</p> </li> <li>Create PR with hotfix</li> <li>Bypass normal review (emergency)</li> <li>Merge and tag immediately</li> </ol>"},{"location":"DEPLOYMENT/#deployment-environments","title":"Deployment Environments","text":""},{"location":"DEPLOYMENT/#development","title":"Development","text":"Bash<pre><code>pip install -e \".[dev]\"\nexport TENETS_ENV=development\n</code></pre>"},{"location":"DEPLOYMENT/#staging","title":"Staging","text":"Bash<pre><code>pip install tenets==0.1.0rc1  # Release candidate\nexport TENETS_ENV=staging\n</code></pre>"},{"location":"DEPLOYMENT/#production","title":"Production","text":"Bash<pre><code>pip install tenets==0.1.0\nexport TENETS_ENV=production\n</code></pre>"},{"location":"DEPLOYMENT/#troubleshooting_1","title":"Troubleshooting","text":""},{"location":"DEPLOYMENT/#common-issues","title":"Common Issues","text":"<ol> <li>PyPI upload fails:</li> <li>Check PyPI status</li> <li>Verify credentials</li> <li> <p>Ensure version doesn't exist</p> </li> <li> <p>Docker build fails:</p> </li> <li>Clear builder cache</li> <li>Check Docker Hub limits</li> <li> <p>Verify multi-arch support</p> </li> <li> <p>Documentation not updating:</p> </li> <li>Check GitHub Pages settings</li> <li>Verify mike configuration</li> <li>Clear browser cache</li> </ol>"},{"location":"DEPLOYMENT/#getting-help","title":"Getting Help","text":"<ul> <li>GitHub Issues for bugs</li> <li>Discussions for questions</li> <li>team@tenets.dev for security issues</li> </ul> <p>Remember: Every release should make developers' lives easier. \ud83d\ude80</p>"},{"location":"DEVELOPMENT/","title":"Development Guide","text":"<p>This guide provides instructions for setting up your development environment, running tests, and contributing to the Tenets project.</p>"},{"location":"DEVELOPMENT/#1-initial-setup","title":"1. Initial Setup","text":""},{"location":"DEVELOPMENT/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.9+</li> <li>Git</li> <li>An activated Python virtual environment (e.g., <code>venv</code>, <code>conda</code>).</li> </ul>"},{"location":"DEVELOPMENT/#fork-and-clone","title":"Fork and Clone","text":"<ol> <li>Fork the repository on GitHub.</li> <li>Clone your fork locally:    Bash<pre><code>git clone https://github.com/jddunn/tenets.git\ncd tenets\n</code></pre></li> </ol>"},{"location":"DEVELOPMENT/#install-dependencies","title":"Install Dependencies","text":"<p>Install the project in \"editable\" mode along with all development dependencies. This allows you to modify the source code and have the changes immediately reflected.</p> <p>Bash<pre><code>pip install -e \".[all,dev]\"\n</code></pre> This command installs everything needed for development, including core dependencies, optional features (<code>all</code>), and development tools (<code>dev</code>).</p>"},{"location":"DEVELOPMENT/#set-up-pre-commit-hooks","title":"Set up Pre-Commit Hooks","text":"<p>This project uses <code>pre-commit</code> to automatically run linters and formatters before each commit.</p> Bash<pre><code>pre-commit install\n</code></pre>"},{"location":"DEVELOPMENT/#alternative-installs","title":"Alternative Installs","text":"<p>If you only need core + dev tooling (faster): Bash<pre><code>pip install -e \".[dev]\"\n</code></pre> If you need a minimal footprint for quick iteration (no optional extras): Bash<pre><code>pip install -e .\n</code></pre></p>"},{"location":"DEVELOPMENT/#verifying-the-cli","title":"Verifying the CLI","text":"Bash<pre><code>tenets --version\ntenets --help | head\n</code></pre> <p>If the command is not found, ensure your virtualenv is activated and that the <code>scripts</code> (Windows) or <code>bin</code> (Unix) directory is on PATH.</p>"},{"location":"DEVELOPMENT/#11-building-distribution-artifacts-optional","title":"1.1 Building Distribution Artifacts (Optional)","text":"<p>You typically do NOT need to build wheels / sdists for day\u2011to\u2011day development; the editable install auto-reflects code edits. Build only when testing packaging or release steps.</p> Bash<pre><code>python -m build               # creates dist/*.whl and dist/*.tar.gz\npip install --force-reinstall dist/tenets-*.whl  # sanity check install\n</code></pre> <p>To inspect what went into the wheel: Bash<pre><code>unzip -l dist/tenets-*.whl | grep analysis/implementations | head\n</code></pre></p>"},{"location":"DEVELOPMENT/#12-clean-environment-tasks","title":"1.2 Clean Environment Tasks","text":"Bash<pre><code>pip cache purge        # optional: clear wheel cache\nfind . -name \"__pycache__\" -exec rm -rf {} +\nrm -rf .pytest_cache .ruff_cache .mypy_cache build dist *.egg-info\n</code></pre>"},{"location":"DEVELOPMENT/#13-using-poetry-instead-of-pip-optional","title":"1.3 Using Poetry Instead of pip (Optional)","text":"<p>Poetry can manage the virtual environment and extras if you prefer: Bash<pre><code>poetry install -E all -E dev   # full feature + dev toolchain\npoetry run pytest              # run tests\npoetry run tenets --help       # invoke CLI\n</code></pre> Update dependencies: Bash<pre><code>poetry update\n</code></pre> Add a new optional dependency (example): Bash<pre><code>poetry add --optional rich\n</code></pre> Text Only<pre><code>## 2. Running Tests\n\nThe test suite uses `pytest`. We have a comprehensive configuration in `pytest.ini` that handles most settings automatically.\n\n### Running All Tests\nTo run the entire test suite:\n```bash\npytest\n</code></pre></p>"},{"location":"DEVELOPMENT/#running-tests-with-coverage","title":"Running Tests with Coverage","text":"<p>To generate a test coverage report: Bash<pre><code>pytest --cov\n</code></pre> This command is configured in <code>pytest.ini</code> to: - Measure coverage for the <code>tenets</code> package. - Generate reports in the terminal, as XML (<code>coverage.xml</code>), and as a detailed HTML report (<code>htmlcov/</code>). - Fail the build if coverage drops below 70%.</p> <p>To view the interactive HTML report: Bash<pre><code># On macOS\nopen htmlcov/index.html\n\n# On Windows\nstart htmlcov/index.html\n\n# On Linux\nxdg-open htmlcov/index.html\n</code></pre></p>"},{"location":"DEVELOPMENT/#3-required--optional-secrets","title":"3. Required / Optional Secrets","text":"<p>Configure these in GitHub: Settings \u2192 Secrets and variables \u2192 Actions.</p> Secret Required? Purpose Notes <code>PYPI_API_TOKEN</code> Yes* Upload package in <code>release.yml</code> *If using PyPI Trusted Publishing you can omit and approve first publication manually. Keep token while bootstrapping. <code>CODECOV_TOKEN</code> Yes (private repo) / No (public) Coverage uploads in CI Public repos sometimes auto-detect; set to be explicit. <code>DOCKER_USERNAME</code> Optional Auth for Docker image push (if enabled) Only needed if/when container publishing is turned on. <code>DOCKER_TOKEN</code> Optional Password / token for Docker Hub Pair with username. <code>GH_PAT</code> No Only for advanced workflows (e.g. cross\u2011repo automation) Not needed for standard release pipeline. <p>Additional environment driven configs (rarely needed): | Variable | Effect | |----------|-------| | <code>TENETS_CACHE_DIRECTORY</code> | Override default cache directory | | <code>TENETS_DEBUG</code> | Enables verbose debug logging when <code>true</code> |</p> <p>Security tips: - Grant least privilege (PyPI token scoped to project if possible) - Rotate any credentials annually or on role changes - Prefer Trusted Publishing over long\u2011lived API tokens once stable</p>"},{"location":"DEVELOPMENT/#4-code-style-and-linting","title":"4. Code Style and Linting","text":"<p>We use <code>ruff</code> for linting and formatting. The pre-commit hook runs it automatically, but you can also run it manually:</p> Bash<pre><code># Check for linting errors\nruff check .\n\n# Automatically fix linting errors\nruff check . --fix\n\n# Format the code\nruff format .\n</code></pre>"},{"location":"DEVELOPMENT/#5-building-documentation","title":"5. Building Documentation","text":"<p>The documentation is built using MkDocs with the Material theme.</p>"},{"location":"DEVELOPMENT/#installing-documentation-dependencies","title":"Installing Documentation Dependencies","text":"Bash<pre><code># Install MkDocs and theme\npip install mkdocs mkdocs-material\n\n# Or if you installed with dev dependencies, it's already included:\npip install -e \".[dev]\"\n</code></pre>"},{"location":"DEVELOPMENT/#serving-documentation-locally","title":"Serving Documentation Locally","text":""},{"location":"DEVELOPMENT/#fast-development-mode-recommended-for-editing-docs","title":"FAST Development Mode (Recommended for editing docs)","text":"Bash<pre><code># Use the lightweight dev config with dirty reload for FASTEST iteration\nmkdocs serve -f mkdocs.dev.yml --dirtyreload\n\n# Without dirty reload (still faster than full build)\nmkdocs serve -f mkdocs.dev.yml\n</code></pre> <p>mkdocs.dev.yml differences: - Disables heavy plugins: No API generation, no mkdocstrings, no minification - Faster rebuilds: Skips expensive operations - Dirty reload: Only rebuilds changed pages (not entire site) - Perfect for: Writing/editing documentation content</p>"},{"location":"DEVELOPMENT/#full-production-mode-for-testing-final-output","title":"Full Production Mode (for testing final output)","text":"Bash<pre><code># Full build with all features including API docs generation\nmkdocs serve\n\n# Serve on a different port\nmkdocs serve -a localhost:8080\n\n# Serve with verbose output for debugging\nmkdocs serve --verbose\n\n# With clean rebuild\nmkdocs serve --clean\n</code></pre> <p>The development server includes: - Live reload: Changes to docs files automatically refresh the browser - API docs generation: Auto-generates from Python docstrings - Full theme features: All navigation and search features enabled</p>"},{"location":"DEVELOPMENT/#building-static-documentation","title":"Building Static Documentation","text":"Bash<pre><code># Build the static site to site/ directory\nmkdocs build\n\n# Build with strict mode (fails on warnings)\nmkdocs build --strict\n\n# Build with verbose output\nmkdocs build --verbose\n\n# Clean build (removes old files first)\nmkdocs build --clean\n</code></pre>"},{"location":"DEVELOPMENT/#documentation-structure","title":"Documentation Structure","text":"Text Only<pre><code>docs/\n\u251c\u2500\u2500 index.md           # Homepage\n\u251c\u2500\u2500 overrides/        # Custom HTML templates\n\u2502   \u2514\u2500\u2500 home.html     # Custom homepage\n\u251c\u2500\u2500 styles/           # Custom CSS\n\u2502   \u251c\u2500\u2500 main.css\n\u2502   \u251c\u2500\u2500 search.css\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 assets/           # Images and screenshots\n\u2502   \u2514\u2500\u2500 images/\n\u2514\u2500\u2500 *.md             # Documentation pages\n</code></pre>"},{"location":"DEVELOPMENT/#api-documentation-generation","title":"API Documentation Generation","text":"<p>The API documentation is auto-generated from Python docstrings using <code>mkdocstrings</code> and <code>gen-files</code> plugins.</p>"},{"location":"DEVELOPMENT/#how-it-works","title":"How it works:","text":"<ol> <li><code>docs/gen_api.py</code> script runs during build:</li> <li>Scans all Python modules in <code>tenets/</code></li> <li>Generates markdown files with <code>:::</code> mkdocstrings syntax</li> <li> <p>Creates navigation structure in <code>api/</code> directory</p> </li> <li> <p><code>mkdocstrings</code> plugin processes the generated files:</p> </li> <li>Extracts docstrings from Python code</li> <li>Renders them as formatted documentation</li> <li>Includes type hints, parameters, returns, examples</li> </ol>"},{"location":"DEVELOPMENT/#regenerating-api-docs","title":"Regenerating API docs:","text":"Bash<pre><code># Full build with API generation (automatic)\nmkdocs build\n\n# Or serve with API generation\nmkdocs serve  # Uses mkdocs.yml which has gen-files enabled\n\n# Skip API generation for faster dev\nmkdocs serve -f mkdocs.dev.yml --dirtyreload\n</code></pre>"},{"location":"DEVELOPMENT/#writing-good-docstrings-for-api-docs","title":"Writing Good Docstrings for API docs:","text":"Python<pre><code>def example_function(param1: str, param2: int = 0) -&gt; bool:\n    \"\"\"Short summary of what this function does.\n\n    Longer description with more details about the function's\n    behavior, use cases, and any important notes.\n\n    Args:\n        param1: Description of first parameter\n        param2: Description of second parameter (default: 0)\n\n    Returns:\n        Description of return value\n\n    Raises:\n        ValueError: When something goes wrong\n\n    Example:\n        &gt;&gt;&gt; example_function(\"test\", 42)\n        True\n    \"\"\"\n</code></pre>"},{"location":"DEVELOPMENT/#making-documentation-changes","title":"Making Documentation Changes","text":"<ol> <li>For content/markdown: Edit files in <code>docs/</code> directory</li> <li>For API docs: Update docstrings in Python source files</li> <li>Preview changes:</li> <li>Fast: <code>mkdocs serve -f mkdocs.dev.yml --dirtyreload</code></li> <li>Full: <code>mkdocs serve</code></li> <li>Test the build: <code>mkdocs build --strict</code></li> <li>Check for broken links in the browser console</li> </ol>"},{"location":"DEVELOPMENT/#deploying-documentation","title":"Deploying Documentation","text":"Bash<pre><code># Deploy to GitHub Pages (requires push permissions)\nmkdocs gh-deploy\n\n# Deploy with custom commit message\nmkdocs gh-deploy -m \"Update documentation\"\n\n# Deploy without pushing (dry run)\nmkdocs gh-deploy --no-push\n</code></pre> <p>The site will be available at <code>https://[username].github.io/tenets/</code>.</p>"},{"location":"DEVELOPMENT/#2-making-changes","title":"2. Making Changes","text":"<p>Follow the coding standards: - Write clean, readable code - Add comprehensive docstrings (Google style) - Include type hints for all functions - Write tests for new functionality</p>"},{"location":"DEVELOPMENT/#3-committing-changes","title":"3. Committing Changes","text":"<p>We use Conventional Commits:</p> Bash<pre><code># Interactive commit\nmake commit  # or: cz commit\n\n# Manual commit (must follow format)\ngit commit -m \"feat(analyzer): add support for Rust AST parsing\"\n</code></pre> <p>Commit types: - <code>feat</code>: New feature - <code>fix</code>: Bug fix - <code>docs</code>: Documentation changes - <code>style</code>: Code style changes (formatting) - <code>refactor</code>: Code refactoring - <code>perf</code>: Performance improvements - <code>test</code>: Test additions or changes - <code>chore</code>: Maintenance tasks</p>"},{"location":"DEVELOPMENT/#4-running-tests","title":"4. Running Tests","text":"Bash<pre><code># Run all tests\nmake test\n\n# Run fast tests only\nmake test-fast\n\n# Run specific test file\npytest tests/test_analyzer.py\n\n# Run with coverage\npytest --cov=tenets --cov-report=html\n</code></pre>"},{"location":"DEVELOPMENT/#5-code-quality-checks","title":"5. Code Quality Checks","text":"Bash<pre><code># Run all checks\nmake lint\n\n# Auto-format code\nmake format\n\n# Individual tools\nblack .\nisort .\nruff check .\nmypy tenets --strict\nbandit -r tenets\n</code></pre>"},{"location":"DEVELOPMENT/#6-pushing-changes","title":"6. Pushing Changes","text":"Bash<pre><code># Pre-commit hooks will run automatically\ngit push origin feature/your-feature-name\n</code></pre>"},{"location":"DEVELOPMENT/#7-creating-a-pull-request","title":"7. Creating a Pull Request","text":"<ol> <li>Go to GitHub and create a PR</li> <li>Fill out the PR template</li> <li>Ensure all CI checks pass</li> <li>Request review from maintainers</li> </ol>"},{"location":"DEVELOPMENT/#testing","title":"Testing","text":""},{"location":"DEVELOPMENT/#test-structure","title":"Test Structure","text":"Text Only<pre><code>tests/\n\u251c\u2500\u2500 unit/              # Unit tests\n\u2502   \u251c\u2500\u2500 test_analyzer.py\n\u2502   \u251c\u2500\u2500 test_nlp.py\n\u2502   \u2514\u2500\u2500 test_scanner.py\n\u251c\u2500\u2500 integration/       # Integration tests\n\u2502   \u251c\u2500\u2500 test_cli.py\n\u2502   \u2514\u2500\u2500 test_workflow.py\n\u251c\u2500\u2500 fixtures/         # Test data\n\u2502   \u2514\u2500\u2500 sample_repo/\n\u2514\u2500\u2500 conftest.py      # Pytest configuration\n</code></pre>"},{"location":"DEVELOPMENT/#writing-tests","title":"Writing Tests","text":"Python<pre><code>\"\"\"Test module for analyzer functionality.\"\"\"\n\nimport pytest\nfrom tenets.core.analysis import CodeAnalyzer\n\n\nclass TestCodeAnalyzer:\n    \"\"\"Test suite for CodeAnalyzer.\"\"\"\n\n    @pytest.fixture\n    def analyzer(self):\n        \"\"\"Create analyzer instance.\"\"\"\n        return CodeAnalyzer()\n\n    def test_analyze_python_file(self, analyzer, tmp_path):\n        \"\"\"Test Python file analysis.\"\"\"\n        # Create test file\n        test_file = tmp_path / \"test.py\"\n        test_file.write_text(\"def hello():\\n    return 'world'\")\n\n        # Analyze\n        result = analyzer.analyze_file(test_file)\n\n        # Assertions\n        assert result.language == \"python\"\n        assert len(result.functions) == 1\n        assert result.functions[0][\"name\"] == \"hello\"\n</code></pre>"},{"location":"DEVELOPMENT/#test-markers","title":"Test Markers","text":"Bash<pre><code># Run only unit tests\npytest -m unit\n\n# Skip slow tests\npytest -m \"not slow\"\n\n# Run tests requiring git\npytest -m requires_git\n</code></pre>"},{"location":"DEVELOPMENT/#code-quality","title":"Code Quality","text":""},{"location":"DEVELOPMENT/#style-guide","title":"Style Guide","text":"<p>We follow PEP 8 with these modifications: - Line length: 100 characters - Use Black for formatting - Use Google-style docstrings</p>"},{"location":"DEVELOPMENT/#type-hints","title":"Type Hints","text":"<p>All functions must have type hints:</p> Python<pre><code>from typing import List, Optional, Dict, Any\n\n\ndef analyze_files(\n    paths: List[Path],\n    deep: bool = False,\n    max_workers: Optional[int] = None\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Analyze multiple files in parallel.\n\n    Args:\n        paths: List of file paths to analyze\n        deep: Whether to perform deep analysis\n        max_workers: Maximum number of parallel workers\n\n    Returns:\n        Dictionary containing analysis results\n    \"\"\"\n    ...\n</code></pre>"},{"location":"DEVELOPMENT/#docstrings","title":"Docstrings","text":"<p>Use Google-style docstrings:</p> Python<pre><code>def calculate_relevance(\n    file: FileAnalysis,\n    prompt: PromptContext,\n    algorithm: str = \"balanced\"\n) -&gt; float:\n    \"\"\"\n    Calculate relevance score for a file.\n\n    Uses multi-factor scoring to determine how relevant a file is\n    to the given prompt context.\n\n    Args:\n        file: Analyzed file data\n        prompt: Parsed prompt context\n        algorithm: Ranking algorithm to use\n\n    Returns:\n        Relevance score between 0.0 and 1.0\n\n    Raises:\n        ValueError: If algorithm is not recognized\n\n    Example:\n        &gt;&gt;&gt; relevance = calculate_relevance(file, prompt, \"thorough\")\n        &gt;&gt;&gt; print(f\"Relevance: {relevance:.2f}\")\n        0.85\n    \"\"\"\n    ...\n</code></pre>"},{"location":"DEVELOPMENT/#documentation","title":"Documentation","text":""},{"location":"DEVELOPMENT/#building-documentation","title":"Building Documentation","text":"Bash<pre><code># Build docs\nmake docs\n\n# Serve locally\nmake serve-docs\n# Visit http://localhost:8000\n</code></pre>"},{"location":"DEVELOPMENT/#writing-documentation","title":"Writing Documentation","text":"<ol> <li>API Documentation: Auto-generated from docstrings</li> <li>User Guides: Written in Markdown in <code>docs/</code></li> <li>Examples: Include code examples in docstrings</li> </ol>"},{"location":"DEVELOPMENT/#documentation-standards","title":"Documentation Standards","text":"<ul> <li>Use clear, concise language</li> <li>Include code examples</li> <li>Add diagrams where helpful</li> <li>Keep it up-to-date with code changes</li> </ul>"},{"location":"DEVELOPMENT/#debugging","title":"Debugging","text":""},{"location":"DEVELOPMENT/#debug-mode","title":"Debug Mode","text":"Bash<pre><code># Enable debug logging\nexport TENETS_DEBUG=true\ntenets make-context \"test\" . --verbose\n\n# Or in code\nimport logging\nlogging.basicConfig(level=logging.DEBUG)\n</code></pre>"},{"location":"DEVELOPMENT/#using-vs-code","title":"Using VS Code","text":"<p><code>.vscode/launch.json</code>: JSON<pre><code>{\n    \"version\": \"0.2.0\",\n    \"configurations\": [\n        {\n            \"name\": \"Debug tenets CLI\",\n            \"type\": \"python\",\n            \"request\": \"launch\",\n            \"module\": \"tenets.cli.main\",\n            \"args\": [\"make-context\", \"test query\", \".\"],\n            \"console\": \"integratedTerminal\"\n        }\n    ]\n}\n</code></pre></p>"},{"location":"DEVELOPMENT/#common-issues","title":"Common Issues","text":"<ol> <li>Import errors: Ensure you've installed in development mode (<code>pip install -e .</code>)</li> <li>Type errors: Run <code>mypy</code> to catch type issues</li> <li>Test failures: Check if you need to install optional dependencies</li> </ol>"},{"location":"DEVELOPMENT/#contributing-guidelines","title":"Contributing Guidelines","text":""},{"location":"DEVELOPMENT/#before-you-start","title":"Before You Start","text":"<ol> <li>Check existing issues and PRs</li> <li>Open an issue to discuss large changes</li> <li>Read the architecture documentation</li> </ol>"},{"location":"DEVELOPMENT/#code-review-checklist","title":"Code Review Checklist","text":"<ul> <li> Tests pass locally</li> <li> Code is formatted (black, isort)</li> <li> Type hints are present</li> <li> Docstrings are complete</li> <li> Tests cover new functionality</li> <li> Documentation is updated</li> <li> Commit messages follow convention</li> <li> No security issues (bandit)</li> </ul>"},{"location":"DEVELOPMENT/#getting-help","title":"Getting Help","text":"<ul> <li>Open an issue for bugs</li> <li>Start a discussion for features</li> <li>Join our Discord (coming soon)</li> <li>Email: team@tenets.dev</li> </ul>"},{"location":"DEVELOPMENT/#release--versioning","title":"Release &amp; Versioning","text":"<p>Releases are automated. Merging conventional commits into <code>main</code> (from PRs) is all you normally do.</p>"},{"location":"DEVELOPMENT/#branch-model","title":"Branch Model","text":"Branch Purpose <code>dev</code> (or feature branches) Integration / iterative work <code>main</code> Always releasable; auto-versioned on merge"},{"location":"DEVELOPMENT/#workflows-high-level","title":"Workflows (high level)","text":"<ol> <li>PR merged into <code>main</code>.</li> <li><code>version-bump.yml</code> runs:<ul> <li>Collects commits since last tag</li> <li>Determines next version:</li> <li>Major: commit body contains <code>BREAKING CHANGE:</code> or type suffixed with <code>!</code></li> <li>Minor: at least one <code>feat:</code> or <code>perf:</code> commit (performance treated as minor to signal impact)</li> <li>Patch: any <code>fix</code>, <code>refactor</code>, <code>chore</code> (unless a higher bump already chosen)</li> <li>Skip: only docs / test / style commits (no release)</li> <li>Updates <code>pyproject.toml</code></li> <li>Appends a section to <code>CHANGELOG.md</code> grouping commits (Features / Performance / Fixes / Refactoring / Chore)</li> <li>Commits with message <code>chore(release): vX.Y.Z</code> and creates annotated tag <code>vX.Y.Z</code></li> </ul> </li> <li>Tag push triggers <code>release.yml</code>:<ul> <li>Builds wheel + sdist</li> <li>Publishes to PyPI (token or Trusted Publishing)</li> <li>(Optional) Builds &amp; publishes Docker image (future enablement)</li> <li>Deploys docs (if configured) / updates site</li> </ul> </li> <li><code>release-drafter</code> (config) ensures GitHub Release notes reflect categorized changes (either via draft or final publish depending on config state).</li> </ol> <p>You do NOT run <code>cz bump</code> manually during normal flow; the workflow handles versioning.</p>"},{"location":"DEVELOPMENT/#conventional-commit-expectations","title":"Conventional Commit Expectations","text":"<p>Use clear scopes where possible: Text Only<pre><code>feat(ranking): add semantic similarity signal\nfix(cli): prevent crash on empty directory\nperf(analyzer): cache parsed ASTs\nrefactor(config): simplify loading logic\ndocs: update quickstart for --copy flag\n</code></pre></p> <p>Edge cases: - Multiple commit types: highest precedence decides (major &gt; minor &gt; patch) - Mixed docs + fix: still releases (fix wins) - Only docs/test/style: skipped; no tag produced</p>"},{"location":"DEVELOPMENT/#first-release-bootstrap","title":"First Release (Bootstrap)","text":"<p>If no existing tag: 1. Merge initial feature set into <code>main</code> 2. Push a commit with <code>feat: initial release</code> (or similar) 3. Workflow sets version to <code>0.1.0</code> (or bump logic starting point defined in workflow)</p> <p>If you need a different starting version (e.g. <code>0.3.0</code>): create an annotated tag manually once, then subsequent merges resume automation.</p>"},{"location":"DEVELOPMENT/#manual--emergency-release","title":"Manual / Emergency Release","text":"<p>Only when automation is blocked: Bash<pre><code>git checkout main &amp;&amp; git pull\ncz bump --increment PATCH  # or MINOR / MAJOR\ngit push &amp;&amp; git push --tags\n</code></pre> Monitor <code>release.yml</code>. After resolution, revert to automated flow.</p>"},{"location":"DEVELOPMENT/#verifying-a-release","title":"Verifying a Release","text":"<p>After automation completes: Bash<pre><code>pip install --no-cache-dir tenets==&lt;new_version&gt;\ntenets --version\n</code></pre> Smoke test a core command: Bash<pre><code>tenets distill \"smoke\" . --max-tokens 2000 --mode fast --stats || true\n</code></pre></p>"},{"location":"DEVELOPMENT/#troubleshooting","title":"Troubleshooting","text":"Symptom Likely Cause Fix No new tag after merge Only docs/test/style commits Land a non-skipped commit (e.g. fix) Wrong bump size Commit type misclassified Amend / add corrective commit (e.g. feat) PyPI publish failed Missing / invalid <code>PYPI_API_TOKEN</code> or Trusted Publishing not approved yet Add token or approve in PyPI UI Changelog missing section Commit type not in allowed list Ensure conventional type used Duplicate release notes Manual tag + automated tag Avoid manual tagging except emergencies"},{"location":"DEVELOPMENT/#philosophy","title":"Philosophy","text":"<p>Keep <code>main</code> always shippable. Small, frequent releases reduce risk and keep context fresh for users.</p>"},{"location":"DEVELOPMENT/#advanced-topics","title":"Advanced Topics","text":""},{"location":"DEVELOPMENT/#adding-a-new-language-analyzer","title":"Adding a New Language Analyzer","text":"<ol> <li> <p>Create analyzer in <code>tenets/core/analysis/</code>:    Python<pre><code>class RustAnalyzer(LanguageAnalyzer):\n    language_name = \"rust\"\n\n    def extract_imports(self, content: str) -&gt; List[Import]:\n        # Implementation\n        ...\n</code></pre></p> </li> <li> <p>Register in <code>analysis/analyzer.py</code>:    Python<pre><code>analyzers['.rs'] = RustAnalyzer()\n</code></pre></p> </li> <li> <p>Add tests in <code>tests/unit/test_rust_analyzer.py</code></p> </li> </ol>"},{"location":"DEVELOPMENT/#creating-custom-ranking-algorithms","title":"Creating Custom Ranking Algorithms","text":"<ol> <li> <p>Implement algorithm:    Python<pre><code>class SecurityRanking:\n    def score_file(self, file, prompt):\n        # Custom scoring logic\n        ...\n</code></pre></p> </li> <li> <p>Register algorithm:    Python<pre><code>@register_algorithm(\"security\")\nclass SecurityRanking:\n    ...\n</code></pre></p> </li> <li> <p>Document usage in <code>docs/api.md</code></p> </li> </ol> <p>Happy coding! \ud83d\ude80 Remember: context is everything.</p>"},{"location":"SECURITY/","title":"Security Policy","text":""},{"location":"SECURITY/#supported-versions","title":"Supported Versions","text":"<p>The project is pre-1.0; security fixes are applied to the latest released version. Older versions may not receive backports.</p>"},{"location":"SECURITY/#reporting-a-vulnerability","title":"Reporting a Vulnerability","text":"<p>Email: security@tenets.dev (or team@tenets.dev if unreachable)</p> <p>Please include: - Description of the issue - Steps to reproduce / proof-of-concept - Potential impact / affected components - Your environment (OS, Python, tenets version)</p> <p>We aim to acknowledge within 3 business days and provide a remediation ETA after triage.</p>"},{"location":"SECURITY/#responsible-disclosure","title":"Responsible Disclosure","text":"<p>Do not open public issues for exploitable vulnerabilities. Use the private email above. We will coordinate disclosure and credit (if desired) after a fix is released.</p>"},{"location":"SECURITY/#scope","title":"Scope","text":"<p>Tenets runs locally. Primary concerns: - Arbitrary code execution via file parsing - Directory traversal / path injection - Insecure temporary file handling - Leakage of private repository data beyond intended output</p> <p>Out of scope: - Issues requiring malicious local user privilege escalation - Vulnerabilities in optional third-party dependencies (report upstream)</p>"},{"location":"SECURITY/#security-best-practices-users","title":"Security Best Practices (Users)","text":"<ul> <li>Pin versions in production workflows</li> <li>Run latest patch release</li> <li>Review output before sharing externally</li> <li>Avoid running against untrusted repositories without isolation (use containers)</li> </ul>"},{"location":"SECURITY/#patching-process","title":"Patching Process","text":"<ol> <li>Triage &amp; reproduce</li> <li>Develop fix in private branch</li> <li>Add regression tests</li> <li>Coordinate release (patch version bump)</li> <li>Publish advisory in CHANGELOG / release notes</li> </ol>"},{"location":"SECURITY/#contact","title":"Contact","text":"<p>security@tenets.dev</p>"},{"location":"TESTING/","title":"Testing","text":""},{"location":"TESTING/#quick-start","title":"Quick Start","text":"Bash<pre><code># One-liner (editable install + test deps + coverage helpers)\npip install -e '.[test]' pytest pytest-cov\n\n# Run all tests (quiet)\npytest -q\n\n# Run with coverage + fail if below threshold (adjust as policy evolves)\npytest --cov=tenets --cov-report=term-missing --cov-fail-under=70\n\n# Generate XML (CI) + HTML\npytest --cov=tenets --cov-report=xml --cov-report=html\n\n# Open HTML (macOS/Linux)\nopen htmlcov/index.html || xdg-open htmlcov/index.html || true\n\n# Specific test file / test\npytest tests/core/analysis/test_analyzer.py::test_basic_python_analysis -q\n\n# Pattern match\npytest -k analyzer -q\n\n# Parallel (if pytest-xdist installed)\npytest -n auto\n</code></pre> <p>Optional feature extras (install before running related tests): Bash<pre><code>pip install -e '.[light]'   # TF-IDF / YAKE ranking tests\npip install -e '.[viz]'     # Visualization tests\npip install -e '.[ml]'      # Embedding / semantic tests (heavy)\n</code></pre></p>"},{"location":"TESTING/#test-structure","title":"Test Structure","text":"Text Only<pre><code>tests/\n\u251c\u2500\u2500 conftest.py              # Shared fixtures\n\u251c\u2500\u2500 test_config.py           # Config tests\n\u251c\u2500\u2500 test_tenets.py           # Main module tests\n\u251c\u2500\u2500 core/\n\u2502   \u251c\u2500\u2500 analysis/           # Code analysis tests\n\u2502   \u251c\u2500\u2500 distiller/          # Context distillation tests\n\u2502   \u251c\u2500\u2500 git/                # Git integration tests\n\u2502   \u251c\u2500\u2500 prompt/             # Prompt parsing tests\n\u2502   \u251c\u2500\u2500 ranker/             # File ranking tests\n\u2502   \u251c\u2500\u2500 session/            # Session management tests\n\u2502   \u2514\u2500\u2500 summarizer/         # Summarization tests\n\u251c\u2500\u2500 storage/\n\u2502   \u251c\u2500\u2500 test_cache.py       # Caching system tests\n\u2502   \u251c\u2500\u2500 test_session_db.py  # Session persistence tests\n\u2502   \u2514\u2500\u2500 test_sqlite.py      # SQLite utilities tests\n\u2514\u2500\u2500 utils/\n    \u251c\u2500\u2500 test_scanner.py     # File scanning tests\n    \u251c\u2500\u2500 test_tokens.py      # Token counting tests\n    \u2514\u2500\u2500 test_logger.py      # Logging tests\n</code></pre>"},{"location":"TESTING/#running-tests","title":"Running Tests","text":""},{"location":"TESTING/#by-category","title":"By Category","text":"Bash<pre><code># Unit tests only\npytest -m unit\n\n# Integration tests\npytest -m integration\n\n# Skip slow tests\npytest -m \"not slow\"\n\n# Tests requiring git\npytest -m requires_git\n\n# Tests requiring ML dependencies\npytest -m requires_ml\n</code></pre>"},{"location":"TESTING/#coverage-reports","title":"Coverage Reports","text":"Bash<pre><code># Terminal report\npytest --cov=tenets --cov-report=term-missing\n\n# Enforce minimum (CI/local gate)\npytest --cov=tenets --cov-report=term-missing --cov-fail-under=80\n\n# HTML report\npytest --cov=tenets --cov-report=html\n\n# XML for CI services (Codecov)\npytest --cov=tenets --cov-report=xml\n</code></pre>"},{"location":"TESTING/#debug-mode","title":"Debug Mode","text":"Bash<pre><code># Show print statements\npytest -s\n\n# Stop on first failure\npytest -x\n\n# Drop into debugger on failure\npytest --pdb\n\n# Verbose output\npytest -vv\n</code></pre>"},{"location":"TESTING/#writing-tests","title":"Writing Tests","text":""},{"location":"TESTING/#basic-test","title":"Basic Test","text":"Python<pre><code>def test_feature(config, analyzer):\n    \"\"\"Test feature description.\"\"\"\n    result = analyzer.analyze_file(Path(\"test.py\"))\n    assert result.language == \"python\"\n</code></pre>"},{"location":"TESTING/#using-fixtures","title":"Using Fixtures","text":"Python<pre><code>@pytest.fixture\ndef temp_project(tmp_path):\n    \"\"\"Create temporary project structure.\"\"\"\n    (tmp_path / \"src\").mkdir()\n    (tmp_path / \"src/main.py\").write_text(\"print('hello')\")\n    return tmp_path\n\ndef test_with_project(temp_project):\n    files = list(temp_project.glob(\"**/*.py\"))\n    assert len(files) == 1\n</code></pre>"},{"location":"TESTING/#mocking","title":"Mocking","text":"Python<pre><code>from unittest.mock import Mock, patch\n\ndef test_with_mock():\n    with patch('tenets.utils.tokens.count_tokens') as mock_count:\n        mock_count.return_value = 100\n        # test code\n</code></pre>"},{"location":"TESTING/#parametrized-tests","title":"Parametrized Tests","text":"Python<pre><code>@pytest.mark.parametrize(\"input,expected\", [\n    (\"test.py\", \"python\"),\n    (\"test.js\", \"javascript\"),\n    (\"test.go\", \"go\"),\n])\ndef test_language_detection(analyzer, input, expected):\n    assert analyzer._detect_language(Path(input)) == expected\n</code></pre>"},{"location":"TESTING/#test-markers","title":"Test Markers","text":"<p>Add to test functions:</p> Python<pre><code>@pytest.mark.slow\ndef test_heavy_operation():\n    pass\n\n@pytest.mark.requires_git\ndef test_git_features():\n    pass\n\n@pytest.mark.skipif(not HAS_TIKTOKEN, reason=\"tiktoken not installed\")\ndef test_token_counting():\n    pass\n</code></pre>"},{"location":"TESTING/#ci-integration","title":"CI Integration","text":"YAML<pre><code># .github/workflows/test.yml\n- name: Run tests\n  run: |\n    pytest --cov=tenets --cov-report=xml\n\n- name: Upload coverage\n  uses: codecov/codecov-action@v3\n  with:\n    file: ./coverage.xml\n</code></pre>"},{"location":"TESTING/#pre-commit-hook","title":"Pre-commit Hook","text":"YAML<pre><code># .pre-commit-config.yaml\n- repo: local\n  hooks:\n    - id: tests\n      name: tests\n      entry: pytest\n      language: system\n      pass_filenames: false\n      always_run: true\n</code></pre>"},{"location":"TESTING/#release-test-checklist","title":"Release Test Checklist","text":"<p>Before tagging a release:</p> Bash<pre><code># 1. Clean environment\nrm -rf .venv dist build *.egg-info &amp;&amp; python -m venv .venv &amp;&amp; source .venv/bin/activate\n\n# 2. Install with all needed extras for full test surface\npip install -e '.[all,test]' pytest pytest-cov\n\n# 3. Lint / type (if tools configured)\n# ruff check .\n# mypy tenets\n\n# 4. Run tests with coverage gate\npytest --cov=tenets --cov-report=term-missing --cov-fail-under=80\n\n# 5. Spot-check critical CLI commands\nfor cmd in \\\n  \"distill 'smoke test' --stats\" \\\n  \"instill 'example tenet'\" \\\n  \"session create release-smoke\" \\\n  \"config cache-stats\"; do\n  echo \"tenets $cmd\"; tenets $cmd || exit 1; done\n\n# 6. Build sdist/wheel\npython -m build\n\n# 7. Install built artifact in fresh venv &amp; re-smoke\npython -m venv verify &amp;&amp; source verify/bin/activate &amp;&amp; pip install dist/*.whl &amp;&amp; tenets version\n</code></pre> <p>Minimal CHANGELOG update + version bump in <code>tenets/__init__.py</code> must precede tagging.</p>"},{"location":"TESTING/#performance-testing","title":"Performance Testing","text":"Bash<pre><code># Benchmark tests\npytest tests/performance/ --benchmark-only\n\n# Profile slow tests\npytest --durations=10\n</code></pre>"},{"location":"TESTING/#troubleshooting","title":"Troubleshooting","text":""},{"location":"TESTING/#common-issues","title":"Common Issues","text":"<p>Import errors: Ensure package is installed with test extras: Bash<pre><code>pip install -e \".[test]\"\n</code></pre></p> <p>Slow tests: Use parallel execution: Bash<pre><code>pytest -n auto\n</code></pre></p> <p>Flaky tests: Re-run failures: Bash<pre><code>pytest --reruns 3\n</code></pre></p> <p>Memory issues: Run tests in chunks: Bash<pre><code>pytest tests/core/\npytest tests/storage/\npytest tests/utils/\n</code></pre></p>"},{"location":"TESTING/#coverage-goals","title":"Coverage Goals","text":"<ul> <li>Overall: &gt;80%</li> <li>Core logic: &gt;90%</li> <li>Error paths: &gt;70%</li> <li>Utils: &gt;85%</li> </ul> <p>Check current coverage: Bash<pre><code>pytest --cov=tenets --cov-report=term-missing | grep TOTAL\n</code></pre></p>"},{"location":"VIZ_CHEATSHEET/","title":"Tenets Viz Deps Command Cheat Sheet","text":""},{"location":"VIZ_CHEATSHEET/#installation","title":"Installation","text":"Bash<pre><code>pip install tenets[viz]  # Install visualization dependencies\n</code></pre>"},{"location":"VIZ_CHEATSHEET/#basic-commands","title":"Basic Commands","text":""},{"location":"VIZ_CHEATSHEET/#simple-usage","title":"Simple Usage","text":"Bash<pre><code>tenets viz deps                     # Auto-detect project, show ASCII tree\ntenets viz deps .                   # Analyze current directory\ntenets viz deps src/                # Analyze specific directory\n</code></pre>"},{"location":"VIZ_CHEATSHEET/#output-formats","title":"Output Formats","text":"Bash<pre><code>tenets viz deps --format ascii      # Terminal tree (default)\ntenets viz deps --format svg --output arch.svg     # Scalable vector graphics\ntenets viz deps --format png --output arch.png     # PNG image\ntenets viz deps --format html --output deps.html   # Interactive HTML\ntenets viz deps --format dot --output graph.dot    # Graphviz DOT\ntenets viz deps --format json --output data.json   # Raw JSON data\n</code></pre>"},{"location":"VIZ_CHEATSHEET/#aggregation-levels","title":"Aggregation Levels","text":"Bash<pre><code>tenets viz deps --level file        # Individual file dependencies (detailed)\ntenets viz deps --level module      # Module-level aggregation (recommended)\ntenets viz deps --level package     # Package-level view (high-level)\n</code></pre>"},{"location":"VIZ_CHEATSHEET/#clustering-options","title":"Clustering Options","text":"Bash<pre><code>tenets viz deps --cluster-by directory   # Group by directory structure\ntenets viz deps --cluster-by module      # Group by module\ntenets viz deps --cluster-by package     # Group by package\n</code></pre>"},{"location":"VIZ_CHEATSHEET/#layout-algorithms","title":"Layout Algorithms","text":"Bash<pre><code>tenets viz deps --layout hierarchical   # Tree-like layout (default)\ntenets viz deps --layout circular       # Circular/radial layout\ntenets viz deps --layout shell          # Concentric circles\ntenets viz deps --layout kamada         # Force-directed layout\n</code></pre>"},{"location":"VIZ_CHEATSHEET/#filtering","title":"Filtering","text":"Bash<pre><code># Include specific patterns\ntenets viz deps --include \"*.py\"                    # Only Python files\ntenets viz deps --include \"*.js,*.jsx\"              # JavaScript files\ntenets viz deps --include \"src/**/*.py\"             # Python in src/\n\n# Exclude patterns\ntenets viz deps --exclude \"*test*\"                  # No test files\ntenets viz deps --exclude \"*.min.js,node_modules\"   # Skip minified and deps\n\n# Combined\ntenets viz deps --include \"*.py\" --exclude \"*test*\"\n</code></pre>"},{"location":"VIZ_CHEATSHEET/#node-limiting","title":"Node Limiting","text":"Bash<pre><code>tenets viz deps --max-nodes 50      # Show only top 50 most connected nodes\ntenets viz deps --max-nodes 100     # Useful for large projects\n</code></pre>"},{"location":"VIZ_CHEATSHEET/#real-world-examples","title":"Real-World Examples","text":""},{"location":"VIZ_CHEATSHEET/#for-documentation","title":"For Documentation","text":"Bash<pre><code># Clean architecture diagram for docs\ntenets viz deps . --level package --format svg --output docs/architecture.svg\n\n# Module overview with clustering\ntenets viz deps . --level module --cluster-by directory --format png --output modules.png\n</code></pre>"},{"location":"VIZ_CHEATSHEET/#for-code-review","title":"For Code Review","text":"Bash<pre><code># Interactive exploration\ntenets viz deps . --level module --format html --output review.html\n\n# Focused on specific subsystem\ntenets viz deps src/api --include \"*.py\" --format svg --output api_deps.svg\n</code></pre>"},{"location":"VIZ_CHEATSHEET/#for-refactoring","title":"For Refactoring","text":"Bash<pre><code># Find circular dependencies\ntenets viz deps . --layout circular --format html --output circular_deps.html\n\n# Identify tightly coupled modules\ntenets viz deps . --level module --layout circular --max-nodes 50 --output coupling.svg\n</code></pre>"},{"location":"VIZ_CHEATSHEET/#for-large-projects","title":"For Large Projects","text":"Bash<pre><code># Top-level overview\ntenets viz deps . --level package --max-nodes 20 --format svg --output overview.svg\n\n# Most connected files\ntenets viz deps . --max-nodes 100 --format html --output top100.html\n\n# Specific subsystem deep dive\ntenets viz deps backend/ --level module --cluster-by module --format html -o backend.html\n</code></pre>"},{"location":"VIZ_CHEATSHEET/#project-type-auto-detection","title":"Project Type Auto-Detection","text":"<p>The command automatically detects: - Python: Packages, Django, Flask, FastAPI - JavaScript/TypeScript: Node.js, React, Vue, Angular - Java: Maven, Gradle, Spring - Go: Go modules - Rust: Cargo projects - Ruby: Rails, Gems - PHP: Laravel, Composer - And more...</p>"},{"location":"VIZ_CHEATSHEET/#tips","title":"Tips","text":"<ol> <li>Start Simple: Use <code>tenets viz deps</code> first to see what's detected</li> <li>Use Levels: Start with <code>--level package</code> for overview, drill down to <code>module</code> or <code>file</code></li> <li>Interactive HTML: Best for exploration, use <code>--format html</code></li> <li>Filter Noise: Use <code>--exclude \"*test*,*mock*\"</code> to focus on core code</li> <li>Save Time: Use <code>--max-nodes</code> for large codebases</li> <li>Documentation: SVG format scales well for docs</li> <li>Clustering: Helps organize complex graphs visually</li> </ol>"},{"location":"VIZ_CHEATSHEET/#troubleshooting","title":"Troubleshooting","text":"Bash<pre><code># Check if dependencies are installed\npython -c \"import networkx, matplotlib, graphviz, plotly\" 2&gt;/dev/null &amp;&amp; echo \"All deps OK\" || echo \"Run: pip install tenets[viz]\"\n\n# Debug mode\nTENETS_LOG_LEVEL=DEBUG tenets viz deps . 2&gt;&amp;1 | grep -E \"(Detected|Found|Analyzing)\"\n\n# If graph is too large\ntenets viz deps . --max-nodes 50 --level module  # Reduce nodes and aggregate\n</code></pre>"},{"location":"VIZ_CHEATSHEET/#output-examples","title":"Output Examples","text":""},{"location":"VIZ_CHEATSHEET/#ascii-tree-default","title":"ASCII Tree (default)","text":"Text Only<pre><code>Dependency Graph:\n==================================================\n\nmain.py\n  \u2514\u2500&gt; utils.py\n  \u2514\u2500&gt; config.py\n  \u2514\u2500&gt; models.py\n\nutils.py\n  \u2514\u2500&gt; config.py\n\nmodels.py\n  \u2514\u2500&gt; utils.py\n</code></pre>"},{"location":"VIZ_CHEATSHEET/#what-you-get","title":"What You Get","text":"<ul> <li>Project Info: Auto-detected type, languages, frameworks</li> <li>Entry Points: Identified main files (main.py, index.js, etc.)</li> <li>Dependency Graph: Visual representation of code relationships</li> <li>Multiple Views: File, module, or package level perspectives</li> </ul>"},{"location":"best_practices/","title":"Best Practices","text":""},{"location":"best_practices/#repository-setup","title":"Repository Setup","text":""},{"location":"best_practices/#clean-working-directory","title":"Clean Working Directory","text":"<p>Always run Tenets on a clean working directory for accurate results: Bash<pre><code>git status  # Ensure no uncommitted changes\ntenets examine\n</code></pre></p>"},{"location":"best_practices/#gitignore-configuration","title":"Gitignore Configuration","text":"<p>Ensure your <code>.gitignore</code> is properly configured to exclude: - Build artifacts - Node modules - Virtual environments - Temporary files</p>"},{"location":"best_practices/#command-usage","title":"Command Usage","text":""},{"location":"best_practices/#examine-command","title":"Examine Command","text":"<ul> <li>Use <code>--format</code> for different output formats</li> <li>Filter by language with <code>--language</code></li> <li>Focus on specific paths for targeted analysis</li> </ul>"},{"location":"best_practices/#chronicle-command","title":"Chronicle Command","text":"<ul> <li>Use time ranges appropriate to your project's activity</li> <li>Filter by author for team member contributions</li> <li>Combine with <code>--pattern</code> for specific file analysis</li> </ul>"},{"location":"best_practices/#distill-command","title":"Distill Command","text":"<ul> <li>Run after significant development milestones</li> <li>Use to generate weekly or monthly insights</li> <li>Combine with chronicle for historical context</li> </ul>"},{"location":"best_practices/#performance-optimization","title":"Performance Optimization","text":""},{"location":"best_practices/#large-repositories","title":"Large Repositories","text":"<p>For repositories with many files: Bash<pre><code># Focus on specific directories\ntenets examine src/ --depth 3\n\n# Exclude certain patterns\ntenets examine --exclude \"**/test/**\"\n</code></pre></p>"},{"location":"best_practices/#memory-management","title":"Memory Management","text":"<ul> <li>Use <code>--batch-size</code> for large analyses</li> <li>Enable streaming output with <code>--stream</code></li> </ul>"},{"location":"best_practices/#integration","title":"Integration","text":""},{"location":"best_practices/#cicd-pipeline","title":"CI/CD Pipeline","text":"<p>Add Tenets to your CI pipeline: YAML<pre><code>- name: Code Analysis\n  run: |\n    pip install tenets\n    tenets examine --format json &gt; analysis.json\n</code></pre></p>"},{"location":"best_practices/#pre-commit-hooks","title":"Pre-commit Hooks","text":"<p>Use Tenets in pre-commit hooks: YAML<pre><code>repos:\n  - repo: local\n    hooks:\n      - id: tenets-check\n        name: Tenets Analysis\n        entry: tenets examine --quick\n        language: system\n        pass_filenames: false\n</code></pre></p>"},{"location":"best_practices/#team-collaboration","title":"Team Collaboration","text":""},{"location":"best_practices/#sharing-reports","title":"Sharing Reports","text":"<ul> <li>Generate HTML reports for stakeholder review</li> <li>Export JSON for further processing</li> <li>Use visualizations for architecture discussions</li> </ul>"},{"location":"best_practices/#code-reviews","title":"Code Reviews","text":"<p>Use Tenets output to: - Identify complex areas needing review - Track ownership changes - Monitor technical debt</p>"},{"location":"best_practices/#next-steps","title":"Next Steps","text":"<ul> <li>See Examples for real-world scenarios</li> <li>Review CLI Reference for all options</li> <li>Check Configuration for customization</li> </ul>"},{"location":"docs/","title":"Documentation","text":"<p>Welcome to the Tenets documentation hub. Explore guides and references below.</p> <ul> <li>Quick Start: Get started fast</li> <li>Supported Languages: List</li> <li>CLI Reference: Commands</li> <li>Configuration: Config guide</li> <li> <p>Architecture: System overview</p> </li> <li> <p>API Reference: Browse API</p> </li> </ul> <p>If you were looking for the homepage, go to /.</p>"},{"location":"examples/","title":"Examples","text":""},{"location":"examples/#basic-analysis-examples","title":"Basic Analysis Examples","text":""},{"location":"examples/#analyzing-a-python-project","title":"Analyzing a Python Project","text":"Bash<pre><code># Basic examination\ntenets examine my_python_project/\n\n# With specific focus\ntenets examine my_python_project/ --language python --depth 3\n\n# Output to JSON\ntenets examine my_python_project/ --format json &gt; analysis.json\n</code></pre>"},{"location":"examples/#analyzing-a-javascripttypescript-project","title":"Analyzing a JavaScript/TypeScript Project","text":"Bash<pre><code># Examine with TypeScript support\ntenets examine frontend/ --language typescript\n\n# Exclude node_modules\ntenets examine frontend/ --exclude \"**/node_modules/**\"\n</code></pre>"},{"location":"examples/#chronicle-examples","title":"Chronicle Examples","text":""},{"location":"examples/#team-contribution-analysis","title":"Team Contribution Analysis","text":"Bash<pre><code># Last month's team activity\ntenets chronicle --days 30 --format table\n\n# Specific developer's contributions\ntenets chronicle --author \"Jane Doe\" --days 90\n\n# Focus on feature branch\ntenets chronicle --branch feature/new-ui --days 14\n</code></pre>"},{"location":"examples/#release-analysis","title":"Release Analysis","text":"Bash<pre><code># Changes between releases\ntenets chronicle --from v1.0.0 --to v2.0.0\n\n# Recent hotfixes\ntenets chronicle --pattern \"**/hotfix/**\" --days 7\n</code></pre>"},{"location":"examples/#distill-examples","title":"Distill Examples","text":""},{"location":"examples/#project-insights","title":"Project Insights","text":"Bash<pre><code># Generate comprehensive insights\ntenets distill --comprehensive\n\n# Quick summary\ntenets distill --quick\n\n# Export for reporting\ntenets distill --format markdown &gt; insights.md\n</code></pre>"},{"location":"examples/#visualization-examples","title":"Visualization Examples","text":""},{"location":"examples/#architecture-visualization","title":"Architecture Visualization","text":"Bash<pre><code># Interactive HTML graph\ntenets viz --output architecture.html\n\n# Include all relationships\ntenets viz --include-all --output full-graph.html\n\n# Focus on core modules\ntenets viz --filter \"core/**\" --output core-modules.html\n</code></pre>"},{"location":"examples/#momentum-tracking","title":"Momentum Tracking","text":""},{"location":"examples/#development-velocity","title":"Development Velocity","text":"Bash<pre><code># Weekly momentum report\ntenets momentum --period week\n\n# Monthly trends\ntenets momentum --period month --format chart\n\n# Team momentum\ntenets momentum --team --days 30\n</code></pre>"},{"location":"examples/#advanced-combinations","title":"Advanced Combinations","text":""},{"location":"examples/#pre-release-audit","title":"Pre-Release Audit","text":"Bash<pre><code># Full pre-release analysis\ntenets examine --comprehensive &gt; examine-report.txt\ntenets chronicle --days 30 &gt; chronicle-report.txt\ntenets distill --format json &gt; insights.json\ntenets viz --output release-architecture.html\n</code></pre>"},{"location":"examples/#technical-debt-assessment","title":"Technical Debt Assessment","text":"Bash<pre><code># Identify complex areas\ntenets examine --metric complexity --threshold high\n\n# Find stale code\ntenets chronicle --stale --days 180\n\n# Ownership gaps\ntenets examine --ownership --unowned\n</code></pre>"},{"location":"examples/#team-performance-review","title":"Team Performance Review","text":"Bash<pre><code># Individual contributions\nfor author in \"Alice\" \"Bob\" \"Charlie\"; do\n  tenets chronicle --author \"$author\" --days 90 &gt; \"$author-report.txt\"\ndone\n\n# Team visualization\ntenets viz --team --output team-collaboration.html\n</code></pre>"},{"location":"examples/#integration-examples","title":"Integration Examples","text":""},{"location":"examples/#github-actions","title":"GitHub Actions","text":"YAML<pre><code>name: Code Analysis\non: [push, pull_request]\n\njobs:\n  analyze:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2\n      - name: Install Tenets\n        run: pip install tenets\n      - name: Run Analysis\n        run: |\n          tenets examine --format json &gt; examine.json\n          tenets chronicle --days 7 --format json &gt; chronicle.json\n      - name: Upload Results\n        uses: actions/upload-artifact@v2\n        with:\n          name: analysis-results\n          path: |\n            examine.json\n            chronicle.json\n</code></pre>"},{"location":"examples/#pre-commit-hook","title":"Pre-commit Hook","text":"YAML<pre><code># .pre-commit-config.yaml\nrepos:\n  - repo: local\n    hooks:\n      - id: tenets-complexity\n        name: Check Code Complexity\n        entry: tenets examine --metric complexity --fail-on high\n        language: system\n        pass_filenames: false\n</code></pre>"},{"location":"examples/#next-steps","title":"Next Steps","text":"<ul> <li>Review Best Practices for optimal usage</li> <li>See CLI Reference for all available options</li> <li>Check Configuration for customization options</li> </ul>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.9 or higher</li> <li>Git</li> </ul>"},{"location":"installation/#install-from-pypi","title":"Install from PyPI","text":"Bash<pre><code>pip install tenets\n</code></pre>"},{"location":"installation/#install-from-source","title":"Install from Source","text":"Bash<pre><code>git clone https://github.com/yourusername/tenets.git\ncd tenets\npip install -e .\n</code></pre>"},{"location":"installation/#verify-installation","title":"Verify Installation","text":"Bash<pre><code>tenets --version\n</code></pre>"},{"location":"installation/#next-steps","title":"Next Steps","text":"<p>After installation, see Quick Start to get started with your first analysis.</p>"},{"location":"quickstart/","title":"Quickstart","text":""},{"location":"quickstart/#real-world-flow-system-instruction--tenets--sessions","title":"Real-world flow: System instruction + Tenets + Sessions","text":"Bash<pre><code># Create a working session\ntenets session create auth-refresh\n\n# Add guiding principles (tenets)\ntenets tenet add \"Prefer small, safe diffs\" --priority high --category style\ntenets tenet add \"Always validate user input\" --priority critical --category security\n\n# Apply tenets for this session\ntenets instill --session auth-refresh\n\n# Set a global system instruction\ntenets system-instruction set \"You are a senior engineer. Add tests and document trade-offs.\" --enable\n\n# Build context with transformations for token efficiency\ntenets distill \"add OAuth2 refresh tokens\" --session auth-refresh --remove-comments --condense\n\n# Pin files as you learn what matters\ntenets instill --session auth-refresh --add-file src/auth/service.py --add-folder src/auth/routes\ntenets instill --session auth-refresh --list-pinned\n</code></pre> <p>See also: CLI &gt; System Instruction Commands, Tenet Commands, and Instill.</p>"},{"location":"quickstart/#quick-start","title":"Quick Start","text":"<p>Get productive with Tenets in under 60 seconds.</p>"},{"location":"quickstart/#1-install","title":"1. Install","text":"Bash<pre><code>pip install tenets\n</code></pre>"},{"location":"quickstart/#2-generate-context-cli","title":"2. Generate Context (CLI)","text":"Bash<pre><code>tenets distill \"add optimistic locking to order updates\"\n</code></pre> <p>Copy straight to your clipboard:</p> Bash<pre><code>tenets distill \"refactor payment flow\" --copy\n</code></pre> <p>Or enable auto-copy in <code>tenets.toml</code>:</p> TOML<pre><code>[output]\ncopy_on_distill = true\n</code></pre>"},{"location":"quickstart/#3-refine","title":"3. Refine","text":"<p>Pin or force-include critical files:</p> Bash<pre><code># Build context for investigation\ntenets distill \"investigate cache stampede\"\n\n# Pin files are managed through instill command for sessions\ntenets instill --add-file cache/*.py --add-file config/settings.py\n</code></pre> <p>Exclude noise:</p> Bash<pre><code>tenets distill \"debug webhook\" --exclude \"**/migrations/**,**/tests/**\"\n</code></pre>"},{"location":"quickstart/#4-python-api","title":"4. Python API","text":"Python<pre><code>from tenets import Tenets\n\ntenets = Tenets()\nresult = tenets.distill(\n    prompt=\"implement bulk import\",\n    max_tokens=80_000,\n)\nprint(result.token_count, \"tokens\")\n# Copy is done via CLI flag --copy or config setting\n</code></pre>"},{"location":"quickstart/#5-sessions-iterate","title":"5. Sessions (Iterate)","text":"Python<pre><code>tenets = Tenets()\n# Sessions are managed through distill parameters\nfirst = tenets.distill(\"trace 500 errors in checkout\", session_name=\"checkout-fixes\")\nsecond = tenets.distill(\"add instrumentation around payment retries\", session_name=\"checkout-fixes\")\n</code></pre>"},{"location":"quickstart/#6-visualization--insight","title":"6. Visualization &amp; Insight","text":"Bash<pre><code># Complexity &amp; hotspots\ntenets examine . --show-details --hotspots\n\n# Dependency graph (Interactive HTML)\ntenets viz deps --format html --output deps.html\n</code></pre>"},{"location":"quickstart/#7-next","title":"7. Next","text":"<ul> <li>See full CLI options: CLI Reference</li> <li>Tune ranking &amp; tokens: Configuration</li> <li>Dive deeper: Architecture</li> </ul>"},{"location":"supported_languages/","title":"Supported Languages","text":"<p>Tenets ships with first-class analyzers for a broad set of ecosystems. Each analyzer extracts structural signals (definitions, imports, dependencies) that feed ranking.</p> Language / Tech Analyzer Class Extensions Notes Python PythonAnalyzer .py AST parsing, imports, class/function graph JavaScript / TypeScript* JavaScriptAnalyzer .js, .jsx, .ts, .tsx Lightweight regex/heuristic (TypeScript treated as JS for now) Java JavaAnalyzer .java Package &amp; import extraction Go GoAnalyzer .go Import graph, function signatures C# CSharpAnalyzer .cs Namespace &amp; using directives (great for Unity scripts) C / C++ CppAnalyzer .c, .h, .cpp, .hpp Include graph detection Rust RustAnalyzer .rs Module/use extraction Scala ScalaAnalyzer .scala Object/class/trait discovery Kotlin KotlinAnalyzer .kt, .kts Package &amp; import extraction Swift SwiftAnalyzer .swift Import/use lines PHP PhpAnalyzer .php Namespace/use detection Ruby RubyAnalyzer .rb Class/module defs Dart DartAnalyzer .dart Import and class/function capture GDScript (Godot) GDScriptAnalyzer .gd Signals + extends parsing HTML HTMLAnalyzer .html, .htm Link/script/style references CSS CSSAnalyzer .css @import and rule summarization Generic Text GenericAnalyzer * (fallback) Used when no specific analyzer matches <p>*TypeScript currently leverages the JavaScript analyzer (roadmap: richer TS-specific parsing).</p>"},{"location":"supported_languages/#detection-rules","title":"Detection Rules","text":"<p>File extension matching selects the analyzer. Unsupported files fall back to the generic analyzer supplying minimal term frequency and path heuristics.</p>"},{"location":"supported_languages/#adding-a-new-language","title":"Adding a New Language","text":"<ol> <li>Subclass <code>LanguageAnalyzer</code> in <code>tenets/core/analysis/implementations</code></li> <li>Implement <code>match(path)</code> and <code>analyze(content)</code></li> <li>Register in the analyzer registry (if dynamic) or import to ensure discovery</li> <li>Add tests under <code>tests/core/analysis/implementations</code></li> <li>Update this page</li> </ol>"},{"location":"supported_languages/#roadmap","title":"Roadmap","text":"<p>Planned enhancements:</p> <ul> <li>Deeper TypeScript semantic model</li> <li>SQL schema/introspection analyzer</li> <li>Proto / gRPC IDL support</li> <li>Framework-aware weighting (Django, Rails, Spring) optional modules</li> </ul> <p>Got a priority? Open an issue or PR.</p>"},{"location":"architecture/","title":"Tenets Architecture Overview","text":""},{"location":"architecture/#quick-navigation","title":"Quick Navigation","text":"<p>Choose your level of detail:</p>"},{"location":"architecture/#system-overview","title":"System Overview","text":"<p>High-level architecture, core principles, and system design philosophy.</p>"},{"location":"architecture/#core-systems","title":"Core Systems","text":"<p>Detailed breakdown of analysis engines, ranking systems, and processing pipelines.</p>"},{"location":"architecture/#data--storage","title":"Data &amp; Storage","text":"<p>Session management, caching architecture, and persistence layers.</p>"},{"location":"architecture/#integration--apis","title":"Integration &amp; APIs","text":"<p>CLI architecture, Git integration, and extensibility systems.</p>"},{"location":"architecture/#performance--deployment","title":"Performance &amp; Deployment","text":"<p>Performance architecture, scalability, and deployment strategies.</p>"},{"location":"architecture/#what-is-tenets","title":"What is Tenets?","text":"<p>Tenets is a sophisticated, local-first code intelligence platform that revolutionizes how developers interact with their codebases when working with AI assistants.</p>"},{"location":"architecture/#core-architecture-principles","title":"Core Architecture Principles","text":""},{"location":"architecture/#1-local-first-processing","title":"1. Local-First Processing","text":"<p>All analysis, ranking, and context generation happens on the developer's machine. No code ever leaves the local environment.</p>"},{"location":"architecture/#2-progressive-enhancement","title":"2. Progressive Enhancement","text":"<p>The system provides value immediately with just Python installed, and scales up with optional dependencies.</p>"},{"location":"architecture/#3-intelligent-caching","title":"3. Intelligent Caching","text":"<p>Every expensive operation is cached at multiple levels - memory, SQLite, disk, and specialized embedding caches.</p>"},{"location":"architecture/#4-configurable-intelligence","title":"4. Configurable Intelligence","text":"<p>Every aspect of ranking and analysis can be configured. Users can adjust factor weights, enable/disable features, and add custom functions.</p>"},{"location":"architecture/#5-streaming-architecture","title":"5. Streaming Architecture","text":"<p>Uses streaming and incremental processing. Files are analyzed as discovered, rankings computed in parallel, results stream to users.</p>"},{"location":"architecture/#-high-level-system-flow","title":"\ud83d\uddfa\ufe0f High-Level System Flow","text":"<pre><code>graph TB\n    A[User Prompt] --&gt; B[Prompt Parser]\n    B --&gt; C[Intent Detection]\n    C --&gt; D[File Discovery]\n    D --&gt; E[Code Analysis]\n    E --&gt; F[Relevance Ranking]\n    F --&gt; G[Context Assembly]\n    G --&gt; H[Output Generation]\n\n    subgraph \"Caching Layers\"\n        I[Memory Cache]\n        J[SQLite DB]\n        K[File System Cache]\n    end\n\n    E --&gt; I\n    F --&gt; J\n    G --&gt; K\n\n    style A fill:#e1f5fe\n    style H fill:#e8f5e8\n    style E fill:#fff3e0\n    style F fill:#fce4ec</code></pre>"},{"location":"architecture/#-documentation-sections","title":"\ud83d\udcda Documentation Sections","text":"Section What You'll Learn Best For Overview System design, philosophy, key concepts Product managers, architects Core Systems Analysis engines, ML pipelines, ranking Senior developers, integrators Data &amp; Storage Database design, caching, sessions Backend developers, DBAs Integration APIs, CLI, Git integration, plugins DevOps, tool builders Performance Scalability, optimization, deployment Performance engineers, SREs"},{"location":"architecture/#-quick-architecture-facts","title":"\ud83d\udd0d Quick Architecture Facts","text":"\ud83c\udfd7\ufe0f Modular Design <p>20+ specialized modules working in harmony</p> \u26a1 Performance First <p>Multi-level caching, parallel processing, streaming</p> \ud83d\udd12 Privacy Focused <p>100% local processing, no data leaves your machine</p> \ud83d\udd27 Highly Configurable <p>Every ranking factor and feature can be tuned</p>"},{"location":"architecture/#-where-to-start","title":"\ud83c\udfaf Where to Start?","text":"<ul> <li>New to Tenets? \u2192 System Overview</li> <li>Want technical details? \u2192 Core Systems</li> <li>Building integrations? \u2192 Integration &amp; APIs</li> <li>Performance questions? \u2192 Performance &amp; Deployment</li> </ul>"},{"location":"architecture/core_systems/","title":"Core Systems Architecture","text":""},{"location":"architecture/core_systems/#overview","title":"Overview","text":"<p>Tenets is built around a sophisticated, multi-layered architecture that transforms raw code into intelligent, contextual insights. At its heart, the system uses advanced ranking algorithms, comprehensive code analysis, and intelligent caching to provide developers with exactly the right context at exactly the right time.</p>"},{"location":"architecture/core_systems/#core-design-decisions","title":"Core Design Decisions","text":""},{"location":"architecture/core_systems/#text-matching-philosophy","title":"Text Matching Philosophy","text":"<p>Tenets employs three distinct matching strategies optimized for different use cases:</p> Mode Matching Behavior Design Rationale Fast Simple substring matching (no word boundaries) Maximum speed, no corpus building Balanced Word boundaries + BM25 ranking + text processing Accurate results for development Thorough Balanced features + ML embeddings + semantic search Deep code understanding"},{"location":"architecture/core_systems/#key-matching-principles","title":"Key Matching Principles","text":"<ol> <li>Mode-Specific Matching Behavior</li> <li>Fast: Simple substring matching (e.g., \"auth\" matches \"authentication\", \"oauth\", \"authorized\")</li> <li>Balanced: Word boundary enforcement (e.g., \"auth\" only matches standalone \"auth\", not \"oauth\")</li> <li>Thorough: Semantic matching (e.g., \"auth\" matches \"login\", \"authentication\", \"security\")</li> <li> <p>Rationale: Trade-off between speed and precision based on use case</p> </li> <li> <p>No Typo Tolerance By Design</p> </li> <li>\u274c \"auht\" does NOT match \"auth\" in any mode</li> <li>Rationale: Professional development assumes correct spelling</li> <li> <p>Exception: Hyphen/space variations (e.g., \"open-source\" = \"open source\" = \"opensource\")</p> </li> <li> <p>Hierarchical Feature Inheritance Text Only<pre><code>Fast Mode:\n  - Simple substring matching (no word boundaries)\n  - Case-insensitive\n  - Path relevance\n  - NO corpus building (key performance optimization)\n\nBalanced Mode (completely different from Fast):\n  - Word boundary matching with regex\n  - BM25 corpus building and scoring\n  - CamelCase/snake_case splitting (authManager \u2192 auth, manager)\n  - Common abbreviation expansion (config \u2192 configuration)\n  - Plural/singular normalization\n\nThorough Mode (extends Balanced, includes all its features plus):\n  - Semantic similarity (auth \u2192 authentication, login)\n  - ML-based embeddings (requires tenets[ml])\n  - Both BM25 AND TF-IDF scoring\n  - Context-aware matching\n</code></pre></p> </li> <li> <p>Case Insensitivity Throughout</p> </li> <li>All matching is case-insensitive</li> <li> <p>Rationale: Users may type queries in any case</p> </li> <li> <p>Import Preservation</p> </li> <li>Import statements are intelligently truncated while preserving structure</li> <li>Critical imports are always retained</li> <li>Rationale: Maintains code context while optimizing token usage</li> </ol>"},{"location":"architecture/core_systems/#system-flow--pipeline","title":"System Flow &amp; Pipeline","text":"<p>The entire Tenets system follows a carefully orchestrated pipeline:</p> <pre><code>graph TB\n    A[User Input] --&gt; B[Prompt Parser]\n    B --&gt; C[Intent Detection]\n    C --&gt; D[File Discovery Engine]\n    D --&gt; E[Language-Specific Analysis]\n    E --&gt; F[Multi-Factor Ranking]\n    F --&gt; G[Token Optimization]\n    G --&gt; H[Context Assembly]\n    H --&gt; I[Output Formatting]\n\n    subgraph \"Analysis Pipeline\"\n        E1[Static Analysis]\n        E2[Complexity Analysis]\n        E3[Git History Analysis]\n        E4[Dependency Analysis]\n        E5[Pattern Recognition]\n    end\n\n    subgraph \"Ranking Pipeline\"\n        F1[Keyword Matching]\n        F2[BM25 Scoring]\n        F3[Semantic Similarity]\n        F4[Git Signals]\n        F5[File Characteristics]\n        F6[Project Structure]\n    end\n\n    E --&gt; E1\n    E --&gt; E2\n    E --&gt; E3\n    E --&gt; E4\n    E --&gt; E5\n\n    F --&gt; F1\n    F --&gt; F2\n    F --&gt; F3\n    F --&gt; F4\n    F --&gt; F5\n    F --&gt; F6\n\n    style A fill:#e1f5fe\n    style I fill:#e8f5e8\n    style E fill:#fff3e0\n    style F fill:#fce4ec</code></pre>"},{"location":"architecture/core_systems/#nlpml-pipeline-architecture","title":"NLP/ML Pipeline Architecture","text":"<p>The NLP/ML pipeline powers Tenets' semantic understanding capabilities, enabling intelligent keyword extraction, similarity matching, and context-aware ranking.</p>"},{"location":"architecture/core_systems/#pipeline-component-flow","title":"Pipeline Component Flow","text":"<pre><code>graph TD\n    subgraph \"Input Processing\"\n        INPUT[Raw Text Input]\n        PROMPT[User Prompt]\n        CODE[Code Content]\n    end\n\n    subgraph \"Tokenization Layer\"\n        CODE_TOK[Code Tokenizer&lt;br/&gt;camelCase, snake_case]\n        TEXT_TOK[Text Tokenizer&lt;br/&gt;NLP processing]\n    end\n\n    subgraph \"Keyword Extraction\"\n        YAKE_EXT[YAKE Extractor&lt;br/&gt;Statistical]\n        TFIDF_EXT[TF-IDF Extractor&lt;br/&gt;Alternative option]\n        FREQ_EXT[Frequency Extractor&lt;br/&gt;Fallback]\n    end\n\n    subgraph \"Embedding Generation\"\n        LOCAL_EMB[Local Embeddings&lt;br/&gt;sentence-transformers]\n        MODEL_SEL[Model Selection&lt;br/&gt;MiniLM, MPNet]\n        FALLBACK[BM25 Fallback&lt;br/&gt;No ML required]\n    end\n\n    subgraph \"Similarity Computing\"\n        COSINE[Cosine Similarity]\n        BATCH[Batch Processing]\n        CACHE[Result Caching]\n    end\n\n    INPUT --&gt; CODE_TOK\n    INPUT --&gt; TEXT_TOK\n    PROMPT --&gt; TEXT_TOK\n    CODE --&gt; CODE_TOK\n\n    CODE_TOK --&gt; YAKE_EXT\n    TEXT_TOK --&gt; YAKE_EXT\n    YAKE_EXT --&gt; TFIDF_EXT\n    TFIDF_EXT --&gt; FREQ_EXT\n\n    FREQ_EXT --&gt; LOCAL_EMB\n    LOCAL_EMB --&gt; MODEL_SEL\n    MODEL_SEL --&gt; FALLBACK\n\n    FALLBACK --&gt; COSINE\n    COSINE --&gt; BATCH\n    BATCH --&gt; CACHE\n\n    style INPUT fill:#e1f5fe\n    style CACHE fill:#e8f5e8</code></pre>"},{"location":"architecture/core_systems/#tokenization-strategy","title":"Tokenization Strategy","text":"<p>Tenets uses specialized tokenizers for different content types:</p>"},{"location":"architecture/core_systems/#code-tokenizer","title":"Code Tokenizer","text":"Python<pre><code>class CodeTokenizer:\n    \"\"\"Handles programming language tokens.\"\"\"\n\n    def tokenize(self, text: str) -&gt; List[str]:\n        # Split on camelCase: 'getUserName' \u2192 ['get', 'User', 'Name']\n        # Split on snake_case: 'get_user_name' \u2192 ['get', 'user', 'name']\n        # Split on kebab-case: 'get-user-name' \u2192 ['get', 'user', 'name']\n        # Preserve special tokens: '__init__', 'UTF-8'\n        # Handle operators: '++', '==', '!='\n</code></pre>"},{"location":"architecture/core_systems/#stopword-management","title":"Stopword Management","text":"<ul> <li>Code Stopwords: Minimal set (~30 words) - 'function', 'class', 'return'</li> <li>Prompt Stopwords: Aggressive filtering (~200+ words) - common English words</li> <li>Intent Action Words: Filters generic action words from keyword matching</li> <li>Prevents words like \"fix\", \"debug\", \"implement\" from affecting file ranking</li> <li>Preserves domain-specific terms for accurate matching</li> <li>Configurable per intent type</li> <li>Context-Aware: Different stopword sets for different operations</li> </ul>"},{"location":"architecture/core_systems/#embedding-models","title":"Embedding Models","text":"Model Size Speed Quality Use Case all-MiniLM-L6-v2 90MB ~180ms/file Good Default, semantic searches all-MiniLM-L12-v2 120MB ~250ms/file Better Higher accuracy needs all-mpnet-base-v2 420MB Moderate Best Thorough mode multi-qa-MiniLM 90MB Fast Specialized Q&amp;A optimized tasks <p>The system automatically selects models based on: - Available memory - Task complexity - User configuration - Performance requirements</p>"},{"location":"architecture/core_systems/#file-discovery--scanning-system","title":"File Discovery &amp; Scanning System","text":"<p>The file discovery system efficiently traverses codebases of any size, applying intelligent filtering and parallel processing.</p>"},{"location":"architecture/core_systems/#scanner-architecture","title":"Scanner Architecture","text":"<pre><code>graph TD\n    subgraph \"Entry Points\"\n        ROOT[Project Root]\n        PATHS[Specified Paths]\n        PATTERNS[Include Patterns]\n    end\n\n    subgraph \"Ignore System Hierarchy\"\n        CLI[CLI Arguments&lt;br/&gt;Highest Priority]\n        TENETS[.tenetsignore]\n        GIT[.gitignore]\n        GLOBAL[Global Ignores&lt;br/&gt;Lowest Priority]\n    end\n\n    subgraph \"Detection Systems\"\n        BINARY[Binary Detection]\n        SIZE[Size Check&lt;br/&gt;Max 10MB]\n        CONTENT[Content Analysis]\n    end\n\n    subgraph \"Parallel Processing\"\n        QUEUE[Work Queue]\n        WORKERS[Thread Pool]\n        PROGRESS[Progress Tracking]\n    end\n\n    ROOT --&gt; CLI\n    PATHS --&gt; CLI\n    PATTERNS --&gt; CLI\n\n    CLI --&gt; TENETS\n    TENETS --&gt; GIT\n    GIT --&gt; GLOBAL\n\n    GLOBAL --&gt; BINARY\n    BINARY --&gt; SIZE\n    SIZE --&gt; CONTENT\n\n    CONTENT --&gt; QUEUE\n    QUEUE --&gt; WORKERS\n    WORKERS --&gt; PROGRESS\n\n    style ROOT fill:#e1f5fe\n    style PROGRESS fill:#e8f5e8</code></pre>"},{"location":"architecture/core_systems/#binary-detection-strategy","title":"Binary Detection Strategy","text":"<pre><code>flowchart TD\n    FILE[Input File] --&gt; EXT{Known Binary&lt;br/&gt;Extension?}\n    EXT --&gt;|Yes| BINARY[Mark as Binary]\n    EXT --&gt;|No| SIZE{Size &gt; 10MB?}\n    SIZE --&gt;|Yes| SKIP[Skip File]\n    SIZE --&gt;|No| SAMPLE[Sample First 8KB]\n    SAMPLE --&gt; NULL{Contains&lt;br/&gt;Null Bytes?}\n    NULL --&gt;|Yes| BINARY\n    NULL --&gt;|No| RATIO[Calculate Text Ratio]\n    RATIO --&gt; THRESHOLD{&gt;95%&lt;br/&gt;Printable?}\n    THRESHOLD --&gt;|Yes| TEXT[Mark as Text]\n    THRESHOLD --&gt;|No| BINARY\n    TEXT --&gt; ANALYZE[Ready for Analysis]\n    BINARY --&gt; IGNORE[Skip Analysis]\n    SKIP --&gt; IGNORE</code></pre>"},{"location":"architecture/core_systems/#1-ranking-system-deep-dive","title":"1. Ranking System Deep Dive","text":"<p>The ranking system (<code>tenets.core.ranking</code>) is the intelligence core of Tenets, using multiple sophisticated algorithms to determine file relevance.</p>"},{"location":"architecture/core_systems/#ranking-algorithms","title":"Ranking Algorithms","text":""},{"location":"architecture/core_systems/#fast-algorithm-fastrankingstrategy","title":"Fast Algorithm (<code>FastRankingStrategy</code>)","text":"<ul> <li>Use Case: Quick exploration, CI/CD, initial discovery</li> <li>Relative Performance: Baseline (100%)</li> <li>Actual Time: ~17s on medium codebase</li> <li>Implementation: </li> <li>Lightweight file analysis (8KB samples)</li> <li>No corpus building (saves significant time)</li> <li>Simple keyword matching with word boundaries</li> <li>Deep analysis only on top 20 files</li> <li>Factors: Simple keyword matching, path analysis, file type relevance</li> <li>Accuracy: Good for obvious matches, may miss nuanced connections</li> </ul> Python<pre><code># Fast algorithm priorities:\n1. Lightweight analysis for all files\n2. Exact keyword matches in content samples\n3. File extension relevance to prompt\n4. Directory importance (src/ &gt; tests/)\n5. Deep analysis on top-ranked files only\n</code></pre>"},{"location":"architecture/core_systems/#balanced-algorithm-balancedrankingstrategy---default","title":"Balanced Algorithm (<code>BalancedRankingStrategy</code>) - Default","text":"<ul> <li>Use Case: Daily development, general context building</li> <li>Relative Performance: 135% (1.3x slower than fast)</li> <li>Actual Time: ~23s on medium codebase</li> <li>Implementation: </li> <li>Full AST analysis for all files</li> <li>BM25 corpus building for accurate ranking</li> <li>Word boundary matching for precision</li> <li>Intelligent summarization for token optimization</li> <li>Factors: BM25 scoring, word boundaries, abbreviation expansion, structure analysis</li> <li>Accuracy: Excellent balance of speed and accuracy</li> </ul> Python<pre><code># Balanced algorithm combines:\n- BM25 relevance scoring (35% weight)\n- BM25 document ranking (30% weight)\n- Git activity signals (15% weight)\n- File structure analysis (15% weight)\n</code></pre>"},{"location":"architecture/core_systems/#thorough-algorithm-thoroughrankingstrategy","title":"Thorough Algorithm (<code>ThoroughRankingStrategy</code>)","text":"<ul> <li>Use Case: Complex refactoring, architecture reviews, semantic search</li> <li>Relative Performance: 536% (5.4x slower than fast)</li> <li>Actual Time: ~91s on medium codebase</li> <li>Implementation: </li> <li>ML model loading (all-MiniLM-L6-v2, ~10s first run)</li> <li>Builds both BM25 and TF-IDF corpus (~5s)</li> <li>Semantic embeddings for all files</li> <li>Comprehensive ranking with ML (~23s)</li> <li>Pattern matching and dependency analysis</li> <li>Factors: Dual scoring algorithms, semantic similarity, dependency graphs, architectural patterns</li> <li>Accuracy: Best possible with ML-powered understanding</li> </ul> Python<pre><code># Thorough algorithm includes:\n- ML embeddings (384-dim vectors)\n- Semantic similarity via cosine distance\n- Dual corpus (BM25 + TF-IDF)\n- Programming pattern recognition\n- Complex dependency analysis\n- Cross-reference analysis\n- Advanced git history mining\n</code></pre>"},{"location":"architecture/core_systems/#multi-factor-ranking-architecture","title":"Multi-Factor Ranking Architecture","text":"<p>The ranking system combines multiple signals to determine file relevance:</p> <pre><code>graph TD\n    subgraph \"Ranking Strategies\"\n        FAST[Fast Strategy&lt;br/&gt;0.5ms/file&lt;br/&gt;Word Boundaries]\n        BALANCED[Balanced Strategy&lt;br/&gt;2.1ms/file&lt;br/&gt;BM25 + Compounds]\n        THOROUGH[Thorough Strategy&lt;br/&gt;2.0ms/file&lt;br/&gt;Semantic + ML]\n    end\n\n    subgraph \"Core Ranking Factors\"\n        SEM[Semantic Similarity&lt;br/&gt;25% weight]\n        TEXT[Text Matching&lt;br/&gt;30% weight]\n        STRUCT[Code Structure&lt;br/&gt;20% weight]\n        GIT[Git Signals&lt;br/&gt;15% weight]\n        FILE[File Characteristics&lt;br/&gt;10% weight]\n    end\n\n    subgraph \"Scoring Engine\"\n        COMBINE[Weighted Combination]\n        NORMALIZE[Score Normalization]\n        FILTER[Threshold Filtering]\n        RANK[Final Rankings]\n    end\n\n    FAST --&gt; TEXT\n    BALANCED --&gt; STRUCT\n    THOROUGH --&gt; SEM\n\n    SEM --&gt; COMBINE\n    TEXT --&gt; COMBINE\n    STRUCT --&gt; COMBINE\n    GIT --&gt; COMBINE\n    FILE --&gt; COMBINE\n\n    COMBINE --&gt; NORMALIZE\n    NORMALIZE --&gt; FILTER\n    FILTER --&gt; RANK\n\n    style FAST fill:#90caf9\n    style BALANCED fill:#a5d6a7\n    style THOROUGH fill:#ffab91</code></pre>"},{"location":"architecture/core_systems/#ranking-factors-explained","title":"Ranking Factors Explained","text":""},{"location":"architecture/core_systems/#1-bm25-best-matching-25---primary-ranking-algorithm","title":"1. BM25 (Best Matching 25) - Primary Ranking Algorithm","text":"Python<pre><code>class BM25Calculator:\n    def score(self, query, document):\n        \"\"\"\n        BM25 is the primary probabilistic ranking function used\n        throughout Tenets for document relevance scoring.\n\n        Key features:\n        - Document length normalization (parameter b)\n        - Term frequency saturation (parameter k1)\n        - Significantly faster than alternatives\n        - Industry standard in search engines\n        \"\"\"\n</code></pre> <p>Use Case: Primary ranking algorithm with 35% weight in balanced mode, handling varying document lengths and preventing term repetition over-weighting.</p>"},{"location":"architecture/core_systems/#2-tf-idf-term-frequency-inverse-document-frequency---optional","title":"2. TF-IDF (Term Frequency-Inverse Document Frequency) - Optional","text":"Python<pre><code>class TFIDFCalculator:\n    def calculate_relevance(self, document, query_terms, corpus):\n        \"\"\"\n        TF-IDF is available as an alternative ranking method\n        for experimentation. Not recommended for production use.\n\n        TF = (term frequency in doc) / (total terms in doc)\n        IDF = log(total docs / docs containing term)\n        TF-IDF = TF \u00d7 IDF\n\n        Note: BM25 provides superior performance and accuracy.\n        \"\"\"\n</code></pre> <p>Status: Available for experimentation but not recommended. Use BM25 for production workloads.</p>"},{"location":"architecture/core_systems/#3-git-activity-signals","title":"3. Git Activity Signals","text":"Python<pre><code>class GitRankingFactor:\n    def calculate_git_signals(self, file_path):\n        return {\n            'recent_commits': recent_commit_count,      # Files changed recently\n            'commit_frequency': historical_changes,     # Frequently modified files\n            'author_diversity': unique_contributors,    # Files many people work on\n            'recency_score': days_since_last_change,   # How fresh is the file\n            'blame_distribution': line_ownership,      # Code ownership patterns\n        }\n</code></pre> <p>Why This Matters: Recently changed files are more likely relevant to current work. Files with many contributors often contain core logic.</p>"},{"location":"architecture/core_systems/#4-structural-analysis-factors","title":"4. Structural Analysis Factors","text":"Python<pre><code>class StructuralRankingFactor:\n    def analyze_structure(self, file_analysis):\n        return {\n            'complexity_score': cyclomatic_complexity,   # Code complexity\n            'import_centrality': incoming_references,    # How many files import this\n            'export_richness': outgoing_dependencies,    # What this file provides\n            'directory_importance': path_significance,    # /src vs /tests importance\n            'file_role': detected_file_type,            # Config, model, util, etc.\n        }\n</code></pre>"},{"location":"architecture/core_systems/#ranking-strategy-selection","title":"Ranking Strategy Selection","text":"<p>The system intelligently selects strategies based on context:</p> Python<pre><code>def select_strategy(self, context: PromptContext, config: TenetsConfig) -&gt; RankingStrategy:\n    \"\"\"\n    Auto-select the best strategy based on:\n    - Codebase size\n    - Available dependencies\n    - User preferences\n    - Time constraints\n    \"\"\"\n    if config.ranking.algorithm == \"auto\":\n        if file_count &gt; 10000:\n            return FastRankingStrategy()\n        elif has_ml_dependencies and context.complexity_level == \"high\":\n            return ThoroughRankingStrategy()\n        else:\n            return BalancedRankingStrategy()\n</code></pre>"},{"location":"architecture/core_systems/#2-analysis-engine-architecture","title":"2. Analysis Engine Architecture","text":"<p>The analysis engine (<code>tenets.core.analysis</code>) provides deep code understanding through language-specific parsers and cross-language analysis.</p>"},{"location":"architecture/core_systems/#language-analyzer-system","title":"Language Analyzer System","text":""},{"location":"architecture/core_systems/#analyzer-architecture","title":"Analyzer Architecture","text":"Python<pre><code>class LanguageAnalyzer(ABC):\n    \"\"\"Base class for all language-specific analyzers\"\"\"\n\n    @abstractmethod\n    def analyze_structure(self, content: str) -&gt; CodeStructure:\n        \"\"\"Extract functions, classes, imports, etc.\"\"\"\n\n    @abstractmethod\n    def calculate_complexity(self, content: str) -&gt; ComplexityMetrics:\n        \"\"\"Calculate cyclomatic complexity, maintainability index\"\"\"\n\n    @abstractmethod\n    def extract_dependencies(self, content: str) -&gt; List[ImportInfo]:\n        \"\"\"Find imports, requires, includes\"\"\"\n</code></pre>"},{"location":"architecture/core_systems/#supported-languages--features","title":"Supported Languages &amp; Features","text":"Language Parser Complexity Dependencies AST Analysis Special Features Python Full AST \u2705 CC, MI \u2705 imports, from \u2705 Full Decorators, async/await JavaScript/TypeScript Full AST \u2705 CC, MI \u2705 import/require \u2705 Full React components, Node.js Java Full AST \u2705 CC, MI \u2705 import, package \u2705 Full Annotations, Spring C# Full AST \u2705 CC, MI \u2705 using, namespace \u2705 Full Attributes, LINQ Go Full AST \u2705 CC, MI \u2705 import, package \u2705 Full Goroutines, interfaces Rust Full AST \u2705 CC, MI \u2705 use, extern \u2705 Full Traits, lifetimes C++ Regex+ \u2705 Basic \u2705 #include Partial Headers, namespaces Ruby Full AST \u2705 CC, MI \u2705 require, gem \u2705 Full Gems, Rails detection PHP Full AST \u2705 CC, MI \u2705 require, use \u2705 Full Composer, namespaces Kotlin Full AST \u2705 CC, MI \u2705 import, package \u2705 Full Coroutines, DSLs <p>CC = Cyclomatic Complexity, MI = Maintainability Index</p>"},{"location":"architecture/core_systems/#complexity-analysis-deep-dive","title":"Complexity Analysis Deep Dive","text":""},{"location":"architecture/core_systems/#cyclomatic-complexity","title":"Cyclomatic Complexity","text":"Python<pre><code>def calculate_cyclomatic_complexity(self, node):\n    \"\"\"\n    Measures the number of linearly independent paths through code.\n\n    Decision points that increase complexity:\n    - if/elif statements\n    - for/while loops\n    - try/catch blocks\n    - switch/case statements\n    - logical operators (&amp;&amp;, ||)\n    - ternary operators\n    \"\"\"\n    complexity = 1  # Base complexity\n    for child in ast.walk(node):\n        if isinstance(child, (ast.If, ast.For, ast.While, ast.ExceptHandler)):\n            complexity += 1\n        elif isinstance(child, ast.BoolOp):\n            complexity += len(child.values) - 1\n    return complexity\n</code></pre> <p>Complexity Levels: - 1-10: Simple, low risk - 11-20: Moderate complexity - 21-50: High complexity, refactor recommended - 50+: Very high risk, immediate attention needed</p>"},{"location":"architecture/core_systems/#maintainability-index","title":"Maintainability Index","text":"Python<pre><code>def calculate_maintainability_index(self, metrics):\n    \"\"\"\n    MI = 171 - 5.2 * ln(HV) - 0.23 * CC - 16.2 * ln(LOC)\n\n    Where:\n    - HV = Halstead Volume (operators + operands)\n    - CC = Cyclomatic Complexity\n    - LOC = Lines of Code\n\n    Scale: 0-100 (higher is more maintainable)\n    \"\"\"\n</code></pre> <p>MI Interpretation: - 85-100: Excellent maintainability - 70-85: Good maintainability - 50-70: Moderate maintainability - 25-50: Below average, needs attention - 0-25: Difficult to maintain</p>"},{"location":"architecture/core_systems/#ranking-factor-details","title":"Ranking Factor Details","text":"Factor Category Weight Components Description Semantic Similarity 25% Embedding cosine similarity, contextual relevance ML-based understanding of semantic meaning Text Matching 35% Keyword matches (20%), BM25 (15%) Direct term matching and probabilistic relevance Code Structure 20% Import centrality (10%), path relevance (10%) File importance in codebase architecture Git Signals 15% Recency (5%), frequency (5%), authors (5%) Version control activity indicators File Characteristics 10% File type (5%), code patterns (5%) Language and pattern-based relevance"},{"location":"architecture/core_systems/#dependency-analysis","title":"Dependency Analysis","text":""},{"location":"architecture/core_systems/#import-graph-construction","title":"Import Graph Construction","text":"Python<pre><code>class DependencyAnalyzer:\n    def build_dependency_graph(self, project_files):\n        \"\"\"\n        Builds a directed graph of file dependencies:\n\n        1. Extract all imports/includes from each file\n        2. Resolve import paths to actual files\n        3. Create edges between dependent files\n        4. Calculate centrality metrics\n        5. Identify circular dependencies\n        \"\"\"\n\n        graph = nx.DiGraph()\n\n        for file in project_files:\n            imports = self.extract_imports(file)\n            for imp in imports:\n                target = self.resolve_import_path(imp, file)\n                if target:\n                    graph.add_edge(file.path, target.path)\n\n        return self.calculate_centrality_metrics(graph)\n</code></pre>"},{"location":"architecture/core_systems/#dependency-metrics","title":"Dependency Metrics","text":"<p>The system calculates various centrality metrics to understand file importance:</p> <pre><code>graph LR\n    subgraph \"Centrality Metrics\"\n        IN[In-Degree&lt;br/&gt;Files importing this]\n        OUT[Out-Degree&lt;br/&gt;Files imported]\n        BETWEEN[Betweenness&lt;br/&gt;Bridge importance]\n        PAGE[PageRank&lt;br/&gt;Link analysis]\n    end\n\n    subgraph \"Analysis\"\n        CRITICAL[Critical Files&lt;br/&gt;High in-degree]\n        HUBS[Hub Files&lt;br/&gt;High out-degree]\n        BRIDGES[Bridge Files&lt;br/&gt;High betweenness]\n        CENTRAL[Central Files&lt;br/&gt;High PageRank]\n    end\n\n    IN --&gt; CRITICAL\n    OUT --&gt; HUBS\n    BETWEEN --&gt; BRIDGES\n    PAGE --&gt; CENTRAL\n\n    style CRITICAL fill:#ffcdd2\n    style CENTRAL fill:#c5e1a5</code></pre> <ul> <li>In-Degree: Number of files that import this file (dependency count)</li> <li>Out-Degree: Number of files this file imports (dependency scope)</li> <li>Betweenness: How often this file appears in shortest paths between other files</li> <li>PageRank: Importance based on the importance of files that import it</li> <li>Clustering Coefficient: How connected this file's dependencies are to each other</li> </ul>"},{"location":"architecture/core_systems/#3-summarization-system","title":"3. Summarization System","text":"<p>When files exceed token limits, Tenets uses intelligent summarization to preserve the most important content.</p>"},{"location":"architecture/core_systems/#summarization-strategies","title":"Summarization Strategies","text":""},{"location":"architecture/core_systems/#1-structural-preservation","title":"1. Structural Preservation","text":"Python<pre><code>class StructuralSummarizer:\n    def summarize(self, content, target_size):\n        \"\"\"\n        Preserves code structure while reducing content:\n\n        Priority Order:\n        1. Function/class signatures (always keep)\n        2. Public API definitions\n        3. Complex logic blocks\n        4. Important comments/docstrings\n        5. Configuration/constants\n        6. Simple variable assignments (truncate)\n        7. Repetitive code (remove)\n        \"\"\"\n</code></pre>"},{"location":"architecture/core_systems/#2-semantic-summarization-ml-mode","title":"2. Semantic Summarization (ML mode)","text":"Python<pre><code>class SemanticSummarizer:\n    def summarize(self, content, context, target_size):\n        \"\"\"\n        Uses ML to understand semantic importance:\n\n        1. Split content into logical segments\n        2. Generate embeddings for each segment\n        3. Calculate relevance to user prompt\n        4. Rank segments by importance\n        5. Select top segments that fit token budget\n        6. Maintain code coherence\n        \"\"\"\n</code></pre>"},{"location":"architecture/core_systems/#3-contextual-summarization","title":"3. Contextual Summarization","text":"Python<pre><code>class ContextualSummarizer:\n    def summarize_for_prompt(self, file_content, prompt_context):\n        \"\"\"\n        Tailors summarization to specific prompt needs:\n\n        - \"debug bug\": Keep error handling, logging\n        - \"add feature\": Keep interfaces, extension points\n        - \"refactor\": Keep complex logic, dependencies\n        - \"security\": Keep authentication, validation\n        \"\"\"\n</code></pre>"},{"location":"architecture/core_systems/#4-context-optimization--token-management","title":"4. Context Optimization &amp; Token Management","text":"<p>Tenets intelligently manages token budgets to maximize relevant context while staying within model limits.</p>"},{"location":"architecture/core_systems/#token-budget-allocation","title":"Token Budget Allocation","text":"<pre><code>graph TB\n    subgraph \"Token Budget\"\n        TOTAL[Total Token Limit]\n        PROMPT[Prompt Reservation&lt;br/&gt;~2K tokens]\n        RESPONSE[Response Buffer&lt;br/&gt;~4K tokens]\n        CONTEXT[Available for Context]\n    end\n\n    subgraph \"Optimization Strategy\"\n        RANK[Rank Files]\n        SELECT[Select Top N]\n        MEASURE[Measure Tokens]\n        SUMMARIZE[Summarize if Needed]\n    end\n\n    subgraph \"Content Priority\"\n        CRITICAL[Critical Files&lt;br/&gt;Always included]\n        HIGH[High Relevance&lt;br/&gt;Full content]\n        MEDIUM[Medium Relevance&lt;br/&gt;Summarized]\n        LOW[Low Relevance&lt;br/&gt;Signatures only]\n    end\n\n    TOTAL --&gt; PROMPT\n    TOTAL --&gt; RESPONSE\n    TOTAL --&gt; CONTEXT\n\n    CONTEXT --&gt; RANK\n    RANK --&gt; SELECT\n    SELECT --&gt; MEASURE\n    MEASURE --&gt; SUMMARIZE\n\n    SUMMARIZE --&gt; CRITICAL\n    SUMMARIZE --&gt; HIGH\n    SUMMARIZE --&gt; MEDIUM\n    SUMMARIZE --&gt; LOW\n\n    style TOTAL fill:#e1f5fe\n    style CRITICAL fill:#ffcdd2</code></pre>"},{"location":"architecture/core_systems/#5-caching--performance-architecture","title":"5. Caching &amp; Performance Architecture","text":"<p>Tenets uses sophisticated multi-level caching to provide instant responses after initial analysis.</p>"},{"location":"architecture/core_systems/#cache-hierarchy","title":"Cache Hierarchy","text":"<pre><code>graph TB\n    A[Request] --&gt; B[Memory Cache]\n    B --&gt;|Miss| C[SQLite Session DB]\n    C --&gt;|Miss| D[File System Cache]\n    D --&gt;|Miss| E[Analysis Pipeline]\n\n    E --&gt; F[Store in File Cache]\n    F --&gt; G[Store in SQLite]\n    G --&gt; H[Store in Memory]\n    H --&gt; I[Return Results]\n\n    subgraph \"Cache Types\"\n        J[Hot Data - Memory]\n        K[Structured Data - SQLite]\n        L[Analysis Results - Disk]\n        M[Embeddings - Specialized]\n    end\n\n    style B fill:#ffeb3b\n    style C fill:#4caf50\n    style D fill:#2196f3\n    style E fill:#ff5722</code></pre>"},{"location":"architecture/core_systems/#cache-invalidation-strategy","title":"Cache Invalidation Strategy","text":"Python<pre><code>class CacheManager:\n    def should_invalidate(self, file_path, cache_entry):\n        \"\"\"\n        Smart cache invalidation based on:\n\n        1. File modification time (mtime)\n        2. Content hash changes (for accuracy)\n        3. Git commit changes (for git-based signals)\n        4. Configuration changes (for ranking params)\n        5. Dependency changes (for import analysis)\n        \"\"\"\n\n        reasons = []\n\n        if file_path.stat().st_mtime &gt; cache_entry.mtime:\n            reasons.append(\"file_modified\")\n\n        if self.calculate_content_hash(file_path) != cache_entry.content_hash:\n            reasons.append(\"content_changed\")\n\n        if self.get_git_commit_hash() != cache_entry.git_hash:\n            reasons.append(\"git_changed\")\n\n        return len(reasons) &gt; 0, reasons\n</code></pre>"},{"location":"architecture/core_systems/#prompt-parsing--understanding","title":"Prompt Parsing &amp; Understanding","text":"<pre><code>graph LR\n    subgraph \"Prompt Analysis\"\n        INPUT[User Prompt]\n        INTENT[Intent Detection]\n        KEYWORDS[Keyword Extraction]\n        FILTER[Intent Filtering]\n        CONTEXT[Context Building]\n    end\n\n    subgraph \"Intent Types\"\n        DEBUG[Debug/Fix&lt;br/&gt;Error handling focus]\n        FEATURE[Feature/Add&lt;br/&gt;New functionality]\n        REFACTOR[Refactor/Optimize&lt;br/&gt;Code improvement]\n        EXPLORE[Explore/Understand&lt;br/&gt;Learning focus]\n    end\n\n    subgraph \"Extraction Methods\"\n        YAKE[YAKE Algorithm&lt;br/&gt;Statistical extraction]\n        BM25[BM25&lt;br/&gt;Probabilistic relevance]\n        NER[Named Entity&lt;br/&gt;Code entities]\n    end\n\n    INPUT --&gt; INTENT\n    INTENT --&gt; KEYWORDS\n    KEYWORDS --&gt; FILTER\n    FILTER --&gt; CONTEXT\n\n    INTENT --&gt; DEBUG\n    INTENT --&gt; FEATURE\n    INTENT --&gt; REFACTOR\n    INTENT --&gt; EXPLORE\n\n    KEYWORDS --&gt; YAKE\n    KEYWORDS --&gt; BM25\n    KEYWORDS --&gt; NER</code></pre>"},{"location":"architecture/core_systems/#keyword-extraction-and-matching","title":"Keyword Extraction and Matching","text":""},{"location":"architecture/core_systems/#how-keywords-are-extracted","title":"How Keywords Are Extracted","text":"<p>Tenets extracts both individual words and multi-word phrases from prompts:</p> <ul> <li>Individual words: <code>token</code>, <code>debug</code>, <code>issue</code>, <code>parser</code></li> <li>Multi-word phrases: <code>debug tokenizing issue</code>, <code>authentication module</code>, <code>database connection</code></li> </ul>"},{"location":"architecture/core_systems/#intent-action-word-filtering","title":"Intent Action Word Filtering","text":"<p>Common action words are filtered from individual keywords but preserved in phrases:</p> <p>Example: \"fix the debug tokenizing issue\" 1. Extracted keywords: <code>['fix', 'debug', 'issue', 'token', 'debug tokenizing issue']</code> 2. After filtering: <code>['token', 'debug tokenizing issue']</code> 3. Why this works:    - <code>fix</code>, <code>debug</code>, <code>issue</code> are removed as standalone generic words    - <code>debug tokenizing issue</code> is kept as a specific phrase    - <code>token</code> is kept as a domain-specific term</p>"},{"location":"architecture/core_systems/#how-keywords-match-files","title":"How Keywords Match Files","text":"<p>Keywords match files using different strategies based on the mode:</p> <p>Fast Mode (substring matching): - <code>token</code> matches: <code>token</code>, <code>tokens</code>, <code>tokenizer</code>, <code>tokenization</code>, <code>tokenizing</code> - Simple substring search: if keyword is contained in file content, it matches - Example: searching for \"token\" will find all files containing that substring</p> <p>Balanced/Thorough Mode (word boundary matching): - More precise matching with word boundaries - Still matches variations (plural forms, different word forms) - Example: \"token\" matches \"tokens\" but not \"atoken\" or \"tokenx\"</p> <p>Multi-word Phrase Matching: - Phrases like \"debug tokenizing issue\" match if:   - The exact phrase appears in the file, OR   - Individual words from the phrase appear (partial matching) - Higher scores for exact phrase matches</p>"},{"location":"architecture/core_systems/#real-world-example","title":"Real-World Example","text":"<p>Query: \"fix the tokenizing bug in the parser\"</p> <ol> <li>Keywords extracted: </li> <li>Individual: <code>fix</code>, <code>bug</code>, <code>tokenizing</code>, <code>parser</code></li> <li> <p>Phrases: <code>tokenizing bug</code>, <code>bug in the parser</code></p> </li> <li> <p>After intent filtering:</p> </li> <li>Filtered out: <code>fix</code>, <code>bug</code> (generic action words)</li> <li> <p>Kept: <code>tokenizing</code>, <code>parser</code>, <code>tokenizing bug</code>, <code>bug in the parser</code></p> </li> <li> <p>Files that will match:</p> </li> <li>Files containing \"tokenizing\", \"tokenizer\", \"tokenization\", etc.</li> <li>Files containing \"parser\", \"parsing\", \"parse\", etc.</li> <li>Files with the exact phrases get higher scores</li> <li><code>tokenizer.py</code>, <code>parser.py</code>, <code>parse_tokens.py</code> all rank high</li> </ol> <p>This approach ensures: - Generic action words don't pollute search results - Domain-specific terms are preserved for accurate matching - Multi-word phrases provide additional context - Related word forms are still discovered</p>"},{"location":"architecture/core_systems/#performance-optimizations","title":"Performance Optimizations","text":""},{"location":"architecture/core_systems/#parallel-processing","title":"Parallel Processing","text":"Python<pre><code>def analyze_files_parallel(self, files, max_workers=None):\n    \"\"\"\n    Parallel analysis with intelligent work distribution:\n\n    - Small files: Batch process in single thread\n    - Large files: Individual threads\n    - I/O bound: Higher thread count\n    - CPU bound: Thread count = CPU cores\n    \"\"\"\n\n    with ThreadPoolExecutor(max_workers=self.optimal_worker_count()) as executor:\n        future_to_file = {\n            executor.submit(self.analyze_file, f): f\n            for f in files\n        }\n</code></pre>"},{"location":"architecture/core_systems/#memory-management","title":"Memory Management","text":"Python<pre><code>class MemoryManager:\n    def manage_analysis_memory(self):\n        \"\"\"\n        Prevents memory bloat during large project analysis:\n\n        1. Stream file processing (don't load all at once)\n        2. Release analysis objects after ranking\n        3. Use generators for large result sets\n        4. Monitor memory usage and trigger cleanup\n        \"\"\"\n</code></pre>"},{"location":"architecture/core_systems/#5-session-management--state","title":"5. Session Management &amp; State","text":"<p>Sessions provide persistent context and configuration for project work.</p>"},{"location":"architecture/core_systems/#session-architecture","title":"Session Architecture","text":"Python<pre><code>class SessionManager:\n    def create_session(self, name, project_path):\n        \"\"\"\n        Session contains:\n        - Pinned files (guaranteed inclusion)\n        - Custom ranking weights\n        - Project-specific configuration\n        - Analysis cache keys\n        - Tenet definitions (guiding principles)\n        \"\"\"\n\n    def merge_contexts(self, session_name, new_context):\n        \"\"\"\n        Intelligently merge new analysis with existing session:\n\n        1. Preserve pinned files\n        2. Update file rankings\n        3. Maintain tenet priorities\n        4. Merge analysis results\n        5. Update cache references\n        \"\"\"\n</code></pre>"},{"location":"architecture/core_systems/#tenets-system","title":"Tenets System","text":"Python<pre><code>class TenetManager:\n    \"\"\"\n    Manages 'tenets' - persistent principles that guide context generation\n\n    Examples:\n    - \"Always include error handling examples\"\n    - \"Prioritize async/await patterns\"\n    - \"Include security considerations\"\n    - \"Focus on performance implications\"\n    \"\"\"\n\n    def apply_tenets(self, context_result, active_tenets):\n        \"\"\"\n        Inject tenets into generated context:\n\n        1. Analyze context for tenet relevance\n        2. Select most applicable tenets\n        3. Format tenets appropriately\n        4. Insert at strategic positions\n        5. Ensure token budget compliance\n        \"\"\"\n</code></pre>"},{"location":"architecture/core_systems/#integration--extension-points","title":"Integration &amp; Extension Points","text":""},{"location":"architecture/core_systems/#custom-analyzers","title":"Custom Analyzers","text":"Python<pre><code>class MyCustomAnalyzer(LanguageAnalyzer):\n    def analyze_structure(self, content: str) -&gt; CodeStructure:\n        # Implement custom parsing logic\n        pass\n\n    def get_language_patterns(self) -&gt; Dict[str, str]:\n        return {\n            'function_def': r'def\\s+(\\w+)\\s*\\(',\n            'class_def': r'class\\s+(\\w+)',\n            'import': r'import\\s+(.+)',\n        }\n</code></pre>"},{"location":"architecture/core_systems/#custom-ranking-factors","title":"Custom Ranking Factors","text":"Python<pre><code>class MyRankingFactor(RankingFactor):\n    def calculate(self, file_analysis, prompt_context):\n        \"\"\"\n        Add custom ranking logic:\n        - Domain-specific patterns\n        - Company coding standards\n        - Architecture preferences\n        - Performance considerations\n        \"\"\"\n        return relevance_score\n</code></pre>"},{"location":"architecture/core_systems/#plugin-system","title":"Plugin System","text":"Python<pre><code>class TenetsPlugin:\n    def register_analyzer(self, language: str, analyzer_class):\n        \"\"\"Register custom language analyzer\"\"\"\n\n    def register_ranking_factor(self, factor_name: str, factor_class):\n        \"\"\"Register custom ranking factor\"\"\"\n\n    def register_output_formatter(self, format_name: str, formatter_class):\n        \"\"\"Register custom output format\"\"\"\n</code></pre>"},{"location":"architecture/core_systems/#performance-characteristics","title":"Performance Characteristics","text":""},{"location":"architecture/core_systems/#benchmarked-performance-real-world-codebase","title":"Benchmarked Performance (Real-World Codebase)","text":"Mode Relative Performance Typical Time Key Optimizations Fast 100% (baseline) ~17s Lightweight analysis, no corpus, minimal processing Balanced 135% (1.3x slower) ~23s BM25 corpus, word boundaries, intelligent summarization Thorough 536% (5.4x slower) ~91s ML embeddings, dual algorithms, comprehensive analysis"},{"location":"architecture/core_systems/#performance-breakdown-by-phase","title":"Performance Breakdown by Phase","text":"Phase Fast Mode Balanced Mode Thorough Mode File Scanning ~1s (same) ~1s (same) ~1s (same) Analysis Lightweight (0.2s) Full AST (2-3s) Full AST (2-3s) Corpus Building Skipped BM25 only (1s) BM25 + TF-IDF (5s) ML Model Loading N/A N/A ~10s (first run) Ranking Simple (0.02s) BM25 (0.4s) ML + Dual (23s) Aggregation Truncation (0.1s) Summarization (2s) Deep summarization (3s)"},{"location":"architecture/core_systems/#key-performance-improvements","title":"Key Performance Improvements","text":""},{"location":"architecture/core_systems/#fast-mode-with-lightweight-analysis","title":"Fast Mode with Lightweight Analysis","text":"<ul> <li>Implementation: <code>LightweightAnalyzer</code> class reads only first 8KB of files</li> <li>Performance: 10-100x faster than full AST parsing</li> <li>Trade-off: Less accurate structure analysis but sufficient for ranking</li> <li>Deep Analysis: Applied only to top 20 files after ranking</li> </ul>"},{"location":"architecture/core_systems/#performance-notes","title":"Performance Notes","text":"<ul> <li>Fast mode achieves baseline performance through lightweight analysis</li> <li>Balanced mode adds only 35% overhead while providing significantly better accuracy</li> <li>Thorough mode's 5.4x slowdown is justified by ML-powered semantic understanding</li> <li>All modes benefit from aggressive caching (cache hits &lt; 500ms)</li> <li>First-run ML model loading in thorough mode adds one-time 10s overhead</li> </ul>"},{"location":"architecture/core_systems/#memory-usage","title":"Memory Usage","text":"Python<pre><code># Typical memory footprint:\nbase_memory = 50_000_000      # ~50MB base\nper_file_overhead = 5_000     # ~5KB per analyzed file\nembedding_cache = 200_000_000 # ~200MB for ML embeddings (when used)\n</code></pre>"},{"location":"architecture/core_systems/#scalability-limits","title":"Scalability Limits","text":"<ul> <li>Files: Tested up to 100K+ files</li> <li>File Size: Handles up to 10MB individual files</li> <li>Concurrency: Scales to available CPU cores</li> <li>Memory: Degrades gracefully with limited RAM</li> </ul>"},{"location":"architecture/core_systems/#language-analyzer-architecture","title":"Language Analyzer Architecture","text":"<p>Tenets provides specialized analyzers for each programming language, extracting structure, complexity, and relationships.</p>"},{"location":"architecture/core_systems/#analyzer-system-overview","title":"Analyzer System Overview","text":"<pre><code>graph TB\n    subgraph \"Base Analyzer Interface\"\n        BASE[LanguageAnalyzer&lt;br/&gt;Abstract Base]\n        EXTRACT[extract_structure()]\n        COMPLEX[calculate_complexity()]\n        DEPS[trace_dependencies()]\n    end\n\n    subgraph \"Language-Specific Analyzers\"\n        PYTHON[Python&lt;br/&gt;Full AST]\n        JS[JavaScript&lt;br/&gt;ES6+ support]\n        GO[Go&lt;br/&gt;Package detection]\n        JAVA[Java&lt;br/&gt;OOP patterns]\n        RUST[Rust&lt;br/&gt;Ownership]\n        GENERIC[Generic&lt;br/&gt;Pattern-based]\n    end\n\n    subgraph \"Analysis Output\"\n        CLASSES[Classes &amp; Methods]\n        FUNCTIONS[Functions &amp; Signatures]\n        IMPORTS[Import Graph]\n        METRICS[Complexity Metrics]\n    end\n\n    BASE --&gt; EXTRACT\n    BASE --&gt; COMPLEX\n    BASE --&gt; DEPS\n\n    BASE --&gt; PYTHON\n    BASE --&gt; JS\n    BASE --&gt; GO\n    BASE --&gt; JAVA\n    BASE --&gt; RUST\n    BASE --&gt; GENERIC\n\n    PYTHON --&gt; CLASSES\n    PYTHON --&gt; FUNCTIONS\n    PYTHON --&gt; IMPORTS\n    PYTHON --&gt; METRICS\n\n    style BASE fill:#e1f5fe\n    style METRICS fill:#e8f5e8</code></pre>"},{"location":"architecture/core_systems/#python-analyzer-deep-dive","title":"Python Analyzer Deep Dive","text":"<pre><code>graph LR\n    subgraph \"AST Analysis\"\n        PARSE[AST Parser]\n        VISIT[Node Visitor]\n        TABLE[Symbol Table]\n    end\n\n    subgraph \"Structure Extraction\"\n        CLASSES[Classes&lt;br/&gt;Inheritance]\n        FUNCTIONS[Functions&lt;br/&gt;Async/Sync]\n        DECORATORS[Decorators]\n        TYPES[Type Hints]\n    end\n\n    subgraph \"Import Analysis\"\n        ABS[Absolute Imports]\n        REL[Relative Imports]\n        GRAPH[Import Graph]\n    end\n\n    subgraph \"Complexity\"\n        CYCLO[Cyclomatic]\n        COGNITIVE[Cognitive]\n        HALSTEAD[Halstead]\n    end\n\n    PARSE --&gt; VISIT\n    VISIT --&gt; TABLE\n\n    TABLE --&gt; CLASSES\n    TABLE --&gt; FUNCTIONS\n    TABLE --&gt; DECORATORS\n    TABLE --&gt; TYPES\n\n    VISIT --&gt; ABS\n    VISIT --&gt; REL\n    ABS --&gt; GRAPH\n    REL --&gt; GRAPH\n\n    TABLE --&gt; CYCLO\n    TABLE --&gt; COGNITIVE\n    TABLE --&gt; HALSTEAD</code></pre>"},{"location":"architecture/core_systems/#language-support-matrix","title":"Language Support Matrix","text":"Language AST Support Complexity Analysis Import Resolution Special Features Python \u2705 Full \u2705 CC, MI, Halstead \u2705 Complete Decorators, async/await, type hints JavaScript/TypeScript \u2705 Full \u2705 CC, MI \u2705 ES6+ modules React components, JSX, Node.js Java \u2705 Full \u2705 CC, MI \u2705 Package/class Annotations, Spring framework C# \u2705 Full \u2705 CC, MI \u2705 Namespace/using Attributes, LINQ, async Go \u2705 Full \u2705 CC, MI \u2705 Package/import Goroutines, interfaces Rust \u2705 Full \u2705 CC, MI \u2705 Use/extern Traits, lifetimes, macros Ruby \u2705 Full \u2705 CC, MI \u2705 Require/gem Gems, Rails detection PHP \u2705 Full \u2705 CC, MI \u2705 Require/use Composer, namespaces C/C++ \ud83d\udfe8 Regex+ \u2705 Basic \u2705 #include Headers, templates Others \ud83d\udfe8 Pattern \ud83d\udfe8 Basic \ud83d\udfe8 Pattern-based Extensible patterns <p>CC = Cyclomatic Complexity, MI = Maintainability Index</p> <p>This architecture provides the foundation for intelligent, fast, and scalable code intelligence that adapts to any project size and developer workflow.</p>"},{"location":"architecture/data_storage/","title":"Data &amp; Storage Architecture","text":""},{"location":"architecture/data_storage/#overview","title":"Overview","text":"<p>Tenets uses a layered storage architecture to manage configuration, sessions, cache, and temporary data efficiently.</p>"},{"location":"architecture/data_storage/#storage-architecture-overview","title":"Storage Architecture Overview","text":"<pre><code>graph TB\n    subgraph \"Memory Cache - L1\"\n        LRU[LRU Cache&lt;br/&gt;1000 items&lt;br/&gt;Sub-ms access]\n        HOT[Hot Data&lt;br/&gt;Recent queries&lt;br/&gt;Active sessions]\n    end\n\n    subgraph \"SQLite Database - L2\"\n        SESSIONS[Sessions&lt;br/&gt;User state]\n        CONFIG[Configuration&lt;br/&gt;Settings]\n        RELATIONS[Dependencies&lt;br/&gt;File graphs]\n    end\n\n    subgraph \"Disk Cache - L3\"\n        ANALYSIS[Analysis Results&lt;br/&gt;Parse trees]\n        EMBEDDINGS[ML Embeddings&lt;br/&gt;Vectors]\n        CONTENT[Processed Files&lt;br/&gt;Token counts]\n    end\n\n    subgraph \"File System - L4\"\n        LOGS[Application Logs]\n        EXPORTS[Exported Sessions]\n        ARCHIVES[Historical Data]\n    end\n\n    LRU --&gt; SESSIONS\n    HOT --&gt; CONFIG\n\n    SESSIONS --&gt; ANALYSIS\n    CONFIG --&gt; EMBEDDINGS\n    RELATIONS --&gt; CONTENT\n\n    ANALYSIS --&gt; LOGS\n    EMBEDDINGS --&gt; EXPORTS\n    CONTENT --&gt; ARCHIVES\n\n    style LRU fill:#ffeb3b\n    style SESSIONS fill:#4caf50\n    style ANALYSIS fill:#2196f3\n    style LOGS fill:#9e9e9e</code></pre>"},{"location":"architecture/data_storage/#storage-hierarchy","title":"Storage Hierarchy","text":""},{"location":"architecture/data_storage/#1-configuration-storage","title":"1. Configuration Storage","text":"<p>Location: Project root or user home directory</p> Text Only<pre><code>.tenets.yml          # Project configuration\n~/.tenets/config     # Global user configuration\n</code></pre> <p>Contents: - Ranking algorithms settings - Output format preferences - Ignore patterns - Token limits</p>"},{"location":"architecture/data_storage/#2-session-storage","title":"2. Session Storage","text":"<p>Location: <code>.tenets/sessions/</code> in project root</p> Text Only<pre><code>.tenets/\n  sessions/\n    default.json\n    feature-auth.json\n    bug-fix-123.json\n</code></pre> <p>Session Data Structure: JSON<pre><code>{\n  \"id\": \"feature-auth\",\n  \"created_at\": \"2024-01-15T10:00:00Z\",\n  \"updated_at\": \"2024-01-15T14:30:00Z\",\n  \"pinned_files\": [\n    \"src/auth/oauth.py\",\n    \"src/auth/jwt.py\"\n  ],\n  \"tenets\": [\n    {\n      \"content\": \"Always validate JWT tokens\",\n      \"priority\": \"critical\"\n    }\n  ],\n  \"context\": {\n    \"last_prompt\": \"implement OAuth2\",\n    \"last_files\": [\"src/auth/*.py\"]\n  }\n}\n</code></pre></p>"},{"location":"architecture/data_storage/#3-cache-management","title":"3. Cache Management","text":"<p>Location: <code>.tenets/cache/</code> or system temp directory</p>"},{"location":"architecture/data_storage/#cache-layers","title":"Cache Layers","text":"<ol> <li>File Metadata Cache</li> <li>File sizes</li> <li>Modification times</li> <li>Language detection</li> <li> <p>Complexity scores</p> </li> <li> <p>Parse Tree Cache</p> </li> <li>AST representations</li> <li>Import graphs</li> <li>Symbol tables</li> <li> <p>Documentation blocks</p> </li> <li> <p>Git History Cache</p> </li> <li>Commit information</li> <li>Author statistics</li> <li>File change frequency</li> <li> <p>Branch information</p> </li> <li> <p>Ranking Cache</p> </li> <li>Computed relevance scores</li> <li>TF-IDF vectors</li> <li>Semantic embeddings</li> <li>Factor calculations</li> </ol>"},{"location":"architecture/data_storage/#4-temporary-storage","title":"4. Temporary Storage","text":"<p>Location: System temp directory</p> <p>Used for: - Processing large files - Intermediate computations - Export generation - Visualization rendering</p>"},{"location":"architecture/data_storage/#session-management-architecture","title":"Session Management Architecture","text":""},{"location":"architecture/data_storage/#session-lifecycle","title":"Session Lifecycle","text":"<pre><code>stateDiagram-v2\n    [*] --&gt; Created: tenets session create\n    Created --&gt; Active: First prompt\n    Active --&gt; Analyzing: Processing files\n    Analyzing --&gt; Active: Context built\n    Active --&gt; Updated: Subsequent prompts\n    Updated --&gt; Analyzing: Incremental update\n    Active --&gt; Exported: Save for sharing\n    Exported --&gt; Archived: Long-term storage\n    Active --&gt; [*]: Session ends\n    Archived --&gt; [*]\n\n    note right of Active\n        - Pinned files included\n        - Tenets applied\n        - Context cached\n    end note\n\n    note right of Updated\n        - Only changed files\n        - Incremental updates\n        - Previous context referenced\n    end note</code></pre>"},{"location":"architecture/data_storage/#session-storage-schema","title":"Session Storage Schema","text":"<pre><code>graph TB\n    subgraph \"Session Tables\"\n        SESSIONS[sessions&lt;br/&gt;id, name, project, created]\n        PROMPTS[prompts&lt;br/&gt;id, session_id, text]\n        CONTEXTS[contexts&lt;br/&gt;id, prompt_id, content]\n        FILES[pinned_files&lt;br/&gt;session_id, file_path]\n        TENETS[tenets&lt;br/&gt;session_id, content, priority]\n    end\n\n    subgraph \"Relationships\"\n        S_P[1:N Prompts per Session]\n        P_C[1:1 Context per Prompt]\n        S_F[1:N Pinned Files]\n        S_T[1:N Tenets]\n    end\n\n    SESSIONS --&gt; S_P\n    S_P --&gt; PROMPTS\n    PROMPTS --&gt; P_C\n    P_C --&gt; CONTEXTS\n    SESSIONS --&gt; S_F\n    S_F --&gt; FILES\n    SESSIONS --&gt; S_T\n    S_T --&gt; TENETS\n\n    style SESSIONS fill:#e1f5fe\n    style CONTEXTS fill:#e8f5e8</code></pre>"},{"location":"architecture/data_storage/#data-models","title":"Data Models","text":""},{"location":"architecture/data_storage/#fileinfo-model","title":"FileInfo Model","text":"Python<pre><code>class FileInfo:\n    path: str\n    size: int\n    language: str\n    modified: datetime\n    complexity: float\n    imports: List[str]\n    exports: List[str]\n    relevance_score: float\n</code></pre>"},{"location":"architecture/data_storage/#context-model","title":"Context Model","text":"Python<pre><code>class Context:\n    prompt: str\n    files: List[FileInfo]\n    token_count: int\n    timestamp: datetime\n    session_id: Optional[str]\n    tenets: List[Tenet]\n</code></pre>"},{"location":"architecture/data_storage/#session-model","title":"Session Model","text":"Python<pre><code>class Session:\n    id: str\n    created_at: datetime\n    updated_at: datetime\n    pinned_files: List[str]\n    tenets: List[Tenet]\n    history: List[Context]\n    config_overrides: Dict\n</code></pre>"},{"location":"architecture/data_storage/#persistence-strategies","title":"Persistence Strategies","text":""},{"location":"architecture/data_storage/#1-lazy-loading","title":"1. Lazy Loading","text":"<ul> <li>Load file contents only when needed</li> <li>Stream large files instead of loading entirely</li> <li>Defer expensive computations</li> </ul>"},{"location":"architecture/data_storage/#2-incremental-updates","title":"2. Incremental Updates","text":"<ul> <li>Update only changed files in cache</li> <li>Recompute scores only for affected files</li> <li>Maintain dirty flags for cache invalidation</li> </ul>"},{"location":"architecture/data_storage/#3-compression","title":"3. Compression","text":"<ul> <li>Compress cached parse trees</li> <li>Use efficient serialization formats</li> <li>Apply content deduplication</li> </ul>"},{"location":"architecture/data_storage/#multi-level-cache-system","title":"Multi-Level Cache System","text":""},{"location":"architecture/data_storage/#cache-architecture","title":"Cache Architecture","text":"<pre><code>graph LR\n    subgraph \"Cache Levels\"\n        L1[Memory Cache&lt;br/&gt;Hot data]\n        L2[SQLite Cache&lt;br/&gt;Structured data]\n        L3[Disk Cache&lt;br/&gt;Bulk storage]\n    end\n\n    subgraph \"Cache Types\"\n        META[File Metadata&lt;br/&gt;Size, mtime, language]\n        PARSE[Parse Trees&lt;br/&gt;AST, symbols]\n        RANK[Rankings&lt;br/&gt;Scores, relevance]\n        EMBED[Embeddings&lt;br/&gt;ML vectors]\n    end\n\n    subgraph \"Performance\"\n        HIT[Cache Hit&lt;br/&gt;&lt;1ms]\n        MISS[Cache Miss&lt;br/&gt;Recompute]\n        WARM[Cache Warming&lt;br/&gt;Preload]\n    end\n\n    L1 --&gt; META\n    L2 --&gt; PARSE\n    L2 --&gt; RANK\n    L3 --&gt; EMBED\n\n    META --&gt; HIT\n    PARSE --&gt; HIT\n    RANK --&gt; MISS\n    EMBED --&gt; WARM\n\n    style HIT fill:#4caf50\n    style MISS fill:#ff9800</code></pre>"},{"location":"architecture/data_storage/#cache-key-generation","title":"Cache Key Generation","text":"Python<pre><code>class CacheKeyGenerator:\n    \"\"\"Generate cache keys for different data types.\"\"\"\n\n    def file_metadata_key(self, path: str) -&gt; str:\n        return f\"meta:{path}:{mtime}\"\n\n    def analysis_key(self, path: str, analyzer: str) -&gt; str:\n        return f\"analysis:{analyzer}:{path}:{content_hash}\"\n\n    def ranking_key(self, prompt: str, algorithm: str) -&gt; str:\n        return f\"rank:{algorithm}:{prompt_hash}:{git_commit}\"\n\n    def embedding_key(self, content: str, model: str) -&gt; str:\n        return f\"embed:{model}:{content_hash}\"\n</code></pre>"},{"location":"architecture/data_storage/#cache-invalidation","title":"Cache Invalidation","text":""},{"location":"architecture/data_storage/#triggers","title":"Triggers","text":"<ol> <li>File System Changes</li> <li>File modification</li> <li>File deletion</li> <li> <p>New file creation</p> </li> <li> <p>Git Operations</p> </li> <li>New commits</li> <li>Branch switches</li> <li> <p>Merge operations</p> </li> <li> <p>Configuration Changes</p> </li> <li>Algorithm updates</li> <li>Threshold adjustments</li> <li> <p>Pattern modifications</p> </li> <li> <p>Time-based</p> </li> <li>TTL expiration</li> <li>Scheduled cleanup</li> <li>Age-based eviction</li> </ol>"},{"location":"architecture/data_storage/#invalidation-strategies","title":"Invalidation Strategies","text":"<pre><code>graph TD\n    subgraph \"Invalidation Triggers\"\n        MTIME[File Modified]\n        HASH[Content Changed]\n        GIT[Git Commit]\n        DEP[Dependencies Changed]\n        TTL[TTL Expired]\n        MANUAL[User Refresh]\n    end\n\n    subgraph \"Invalidation Actions\"\n        EVICT[Evict Entry]\n        CASCADE[Cascade Delete]\n        MARK[Mark Stale]\n        REBUILD[Rebuild Cache]\n    end\n\n    subgraph \"Rebuild Strategy\"\n        LAZY[Lazy - On demand]\n        EAGER[Eager - Background]\n        BATCH[Batch - Multiple files]\n    end\n\n    MTIME --&gt; EVICT\n    HASH --&gt; CASCADE\n    GIT --&gt; CASCADE\n    DEP --&gt; CASCADE\n    TTL --&gt; MARK\n    MANUAL --&gt; REBUILD\n\n    EVICT --&gt; LAZY\n    CASCADE --&gt; EAGER\n    MARK --&gt; LAZY\n    REBUILD --&gt; BATCH\n\n    style MTIME fill:#ffcdd2\n    style CASCADE fill:#fff3e0\n    style LAZY fill:#c5e1a5</code></pre>"},{"location":"architecture/data_storage/#cache-strategy-comparison","title":"Cache Strategy Comparison","text":"Strategy Use Case Pros Cons LRU Parse trees, hot data Simple, effective May evict important items TTL Git history, embeddings Predictable freshness Wasteful if unchanged Size-based File metadata Memory bounded May thrash on large files Content Hash Analysis results Accurate Hash computation overhead Hybrid Production default Balanced approach Complex implementation"},{"location":"architecture/data_storage/#storage-optimization","title":"Storage Optimization","text":""},{"location":"architecture/data_storage/#1-deduplication","title":"1. Deduplication","text":"<ul> <li>Identify duplicate content</li> <li>Store single copy with references</li> <li>Apply at file and block level</li> </ul>"},{"location":"architecture/data_storage/#2-compression","title":"2. Compression","text":"<ul> <li>Gzip for text content</li> <li>Binary serialization for structures</li> <li>Delta compression for versions</li> </ul>"},{"location":"architecture/data_storage/#3-indexing","title":"3. Indexing","text":"<ul> <li>Build indices for fast lookup</li> <li>Maintain sorted structures</li> <li>Use bloom filters for existence checks</li> </ul>"},{"location":"architecture/data_storage/#data-security","title":"Data Security","text":""},{"location":"architecture/data_storage/#1-sensitive-data-handling","title":"1. Sensitive Data Handling","text":"<ul> <li>Never cache credentials</li> <li>Exclude sensitive patterns</li> <li>Sanitize output</li> </ul>"},{"location":"architecture/data_storage/#2-access-control","title":"2. Access Control","text":"<ul> <li>Respect file system permissions</li> <li>Honor .gitignore patterns</li> <li>Apply project-specific rules</li> </ul>"},{"location":"architecture/data_storage/#3-cleanup","title":"3. Cleanup","text":"<ul> <li>Clear temporary files</li> <li>Sanitize memory</li> <li>Secure deletion when needed</li> </ul>"},{"location":"architecture/data_storage/#storage-configuration","title":"Storage Configuration","text":""},{"location":"architecture/data_storage/#environment-variables","title":"Environment Variables","text":"Bash<pre><code>TENETS_CACHE_DIR=/custom/cache/path\nTENETS_SESSION_DIR=/custom/session/path\nTENETS_CACHE_TTL=3600\nTENETS_MAX_CACHE_SIZE=1GB\n</code></pre>"},{"location":"architecture/data_storage/#configuration-file","title":"Configuration File","text":"YAML<pre><code>storage:\n  cache:\n    enabled: true\n    directory: .tenets/cache\n    max_size: 1073741824  # 1GB in bytes\n    ttl: 3600  # seconds\n    compression: true\n\n  sessions:\n    directory: .tenets/sessions\n    auto_save: true\n    max_history: 100\n\n  temp:\n    cleanup_on_exit: true\n    max_file_size: 104857600  # 100MB\n</code></pre>"},{"location":"architecture/data_storage/#performance-metrics","title":"Performance Metrics","text":""},{"location":"architecture/data_storage/#cache-hit-rates","title":"Cache Hit Rates","text":"<ul> <li>Target: &gt;80% for repeated operations</li> <li>Monitor: File metadata, parse trees, git data</li> <li>Optimize: Adjust TTL and size limits</li> </ul>"},{"location":"architecture/data_storage/#storage-usage","title":"Storage Usage","text":"<ul> <li>Monitor: Disk space consumption</li> <li>Alert: When approaching limits</li> <li>Action: Automatic cleanup policies</li> </ul>"},{"location":"architecture/data_storage/#access-patterns","title":"Access Patterns","text":"<ul> <li>Track: Most accessed files</li> <li>Optimize: Preload frequently used data</li> <li>Adjust: Cache priorities based on usage</li> </ul>"},{"location":"architecture/integration/","title":"Integration &amp; APIs","text":""},{"location":"architecture/integration/#overview","title":"Overview","text":"<p>Tenets provides multiple integration points for embedding its functionality into your development workflow, CI/CD pipelines, and custom tools.</p>"},{"location":"architecture/integration/#python-api","title":"Python API","text":""},{"location":"architecture/integration/#basic-integration","title":"Basic Integration","text":"Python<pre><code>from tenets import Tenets\n\n# Initialize\ntenets = Tenets()\n\n# Basic usage\nresult = tenets.distill(\"implement authentication\")\nprint(f\"Generated {result.token_count} tokens\")\nprint(result.content)\n</code></pre>"},{"location":"architecture/integration/#advanced-integration","title":"Advanced Integration","text":"Python<pre><code>from tenets import Tenets, Config\nfrom tenets.core.ranking import RelevanceRanker, RankingAlgorithm\n\n# Custom configuration\nconfig = Config(\n    algorithm=RankingAlgorithm.THOROUGH,\n    max_tokens=100000,\n    include_tests=False\n)\n\n# Initialize with config\ntenets = Tenets(config=config)\n\n# Use sessions\ntenets.session.create(\"feature-x\")\ntenets.session.pin_file(\"src/core/auth.py\")\n\n# Rank files\nranker = RelevanceRanker(algorithm=\"balanced\")\nfiles = tenets.discover_files(\"./src\")\nranked = ranker.rank(files, \"optimize database queries\")\n\n# Custom processing\nfor file in ranked[:10]:\n    print(f\"{file.path}: {file.relevance_score:.3f}\")\n</code></pre>"},{"location":"architecture/integration/#cli-integration","title":"CLI Integration","text":""},{"location":"architecture/integration/#shell-scripts","title":"Shell Scripts","text":"Bash<pre><code>#!/bin/bash\n# Generate context and copy to clipboard\ntenets distill \"fix bug in payment processing\" --copy\n\n# Export ranked files for processing\ntenets rank \"refactor auth\" --format json | \\\n  jq -r '.files[].path' | \\\n  xargs pylint\n\n# Create session and build context\ntenets session create feature-123\ntenets instill --session feature-123 --add-file src/main.py\ntenets distill \"implement feature\" --session feature-123\n</code></pre>"},{"location":"architecture/integration/#makefiles","title":"Makefiles","text":"Makefile<pre><code># Makefile integration\n.PHONY: context\ncontext:\n    @tenets distill \"$(PROMPT)\" --format markdown &gt; context.md\n\n.PHONY: analyze\nanalyze:\n    @tenets examine . --complexity --threshold 10\n\n.PHONY: deps\ndeps:\n    @tenets viz deps --output architecture.svg\n</code></pre>"},{"location":"architecture/integration/#cicd-integration","title":"CI/CD Integration","text":""},{"location":"architecture/integration/#github-actions","title":"GitHub Actions","text":"YAML<pre><code>name: Code Analysis\non: [push, pull_request]\n\njobs:\n  analyze:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2\n\n      - name: Install Tenets\n        run: pip install tenets[all]\n\n      - name: Generate Context\n        run: |\n          tenets distill \"${{ github.event.pull_request.title }}\" \\\n            --format markdown &gt; pr_context.md\n\n      - name: Check Complexity\n        run: |\n          tenets examine . --complexity --threshold 15 \\\n            --format json &gt; complexity.json\n\n      - name: Upload Artifacts\n        uses: actions/upload-artifact@v2\n        with:\n          name: analysis-results\n          path: |\n            pr_context.md\n            complexity.json\n</code></pre>"},{"location":"architecture/integration/#gitlab-ci","title":"GitLab CI","text":"YAML<pre><code>analyze:\n  stage: test\n  script:\n    - pip install tenets\n    - tenets examine . --complexity --threshold 10\n    - tenets viz deps --format html -o deps.html\n  artifacts:\n    paths:\n      - deps.html\n    expire_in: 1 week\n</code></pre>"},{"location":"architecture/integration/#jenkins","title":"Jenkins","text":"Groovy<pre><code>pipeline {\n    agent any\n    stages {\n        stage('Analysis') {\n            steps {\n                sh 'pip install tenets'\n                sh 'tenets distill \"${CHANGE_TITLE}\" --format html -o context.html'\n                publishHTML([\n                    reportDir: '.',\n                    reportFiles: 'context.html',\n                    reportName: 'Code Context'\n                ])\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"architecture/integration/#ide-integration","title":"IDE Integration","text":""},{"location":"architecture/integration/#vs-code-extension","title":"VS Code Extension","text":"JavaScript<pre><code>// Extension integration example\nconst { exec } = require('child_process');\n\nfunction generateContext(prompt) {\n    return new Promise((resolve, reject) =&gt; {\n        exec(`tenets distill \"${prompt}\" --format json`,\n            (error, stdout, stderr) =&gt; {\n                if (error) reject(error);\n                else resolve(JSON.parse(stdout));\n            }\n        );\n    });\n}\n\n// Use in extension\nvscode.commands.registerCommand('tenets.generateContext', async () =&gt; {\n    const prompt = await vscode.window.showInputBox({\n        prompt: 'Enter context prompt'\n    });\n\n    const result = await generateContext(prompt);\n    // Process result...\n});\n</code></pre>"},{"location":"architecture/integration/#vim-integration","title":"Vim Integration","text":"VimL<pre><code>\" .vimrc configuration\nfunction! TenetsContext(prompt)\n    let output = system('tenets distill \"' . a:prompt . '\" --format markdown')\n    new\n    setlocal buftype=nofile\n    call setline(1, split(output, '\\n'))\nendfunction\n\ncommand! -nargs=1 Tenets call TenetsContext(&lt;q-args&gt;)\n</code></pre>"},{"location":"architecture/integration/#api-endpoints-future","title":"API Endpoints (Future)","text":""},{"location":"architecture/integration/#rest-api-design","title":"REST API Design","text":"YAML<pre><code>openapi: 3.0.0\ninfo:\n  title: Tenets API\n  version: 1.0.0\n\npaths:\n  /distill:\n    post:\n      summary: Generate context\n      requestBody:\n        content:\n          application/json:\n            schema:\n              type: object\n              properties:\n                prompt:\n                  type: string\n                path:\n                  type: string\n                options:\n                  type: object\n      responses:\n        200:\n          description: Success\n          content:\n            application/json:\n              schema:\n                type: object\n                properties:\n                  content:\n                    type: string\n                  token_count:\n                    type: integer\n                  files:\n                    type: array\n</code></pre>"},{"location":"architecture/integration/#plugin-system","title":"Plugin System","text":""},{"location":"architecture/integration/#creating-plugins","title":"Creating Plugins","text":"Python<pre><code>from tenets.plugins import Plugin, hook\n\nclass CustomPlugin(Plugin):\n    \"\"\"Example custom plugin.\"\"\"\n\n    @hook('pre_rank')\n    def modify_ranking(self, files, context):\n        \"\"\"Modify files before ranking.\"\"\"\n        # Custom logic\n        return files\n\n    @hook('post_distill')\n    def process_output(self, result):\n        \"\"\"Process distilled output.\"\"\"\n        # Custom processing\n        return result\n\n# Register plugin\nfrom tenets import register_plugin\nregister_plugin(CustomPlugin())\n</code></pre>"},{"location":"architecture/integration/#available-hooks","title":"Available Hooks","text":"<ul> <li><code>pre_discover</code>: Before file discovery</li> <li><code>post_discover</code>: After file discovery</li> <li><code>pre_rank</code>: Before ranking</li> <li><code>post_rank</code>: After ranking</li> <li><code>pre_distill</code>: Before distillation</li> <li><code>post_distill</code>: After distillation</li> <li><code>pre_output</code>: Before output formatting</li> <li><code>post_output</code>: After output formatting</li> </ul>"},{"location":"architecture/integration/#integration-patterns","title":"Integration Patterns","text":""},{"location":"architecture/integration/#1-ai-assistant-integration","title":"1. AI Assistant Integration","text":"Python<pre><code># OpenAI GPT Integration\nimport openai\nfrom tenets import Tenets\n\ndef get_ai_response(user_query):\n    # Generate context\n    t = Tenets()\n    context = t.distill(user_query, max_tokens=4000)\n\n    # Send to AI\n    response = openai.ChatCompletion.create(\n        model=\"gpt-4\",\n        messages=[\n            {\"role\": \"system\", \"content\": \"You are a helpful coding assistant.\"},\n            {\"role\": \"user\", \"content\": f\"Context:\\n{context.content}\\n\\nQuery: {user_query}\"}\n        ]\n    )\n\n    return response.choices[0].message.content\n</code></pre>"},{"location":"architecture/integration/#2-documentation-generation","title":"2. Documentation Generation","text":"Python<pre><code>from tenets import Tenets\nfrom tenets.utils.markdown import MarkdownGenerator\n\ndef generate_docs():\n    t = Tenets()\n\n    # Analyze codebase\n    result = t.distill(\"document public APIs\",\n                      include_patterns=[\"*.py\"],\n                      exclude_patterns=[\"test_*.py\"])\n\n    # Generate documentation\n    gen = MarkdownGenerator()\n    docs = gen.generate_api_docs(result)\n\n    with open(\"API_DOCS.md\", \"w\") as f:\n        f.write(docs)\n</code></pre>"},{"location":"architecture/integration/#3-code-review-automation","title":"3. Code Review Automation","text":"Python<pre><code>def automated_review(pr_files):\n    t = Tenets()\n\n    issues = []\n    for file in pr_files:\n        # Check complexity\n        complexity = t.examine(file, metric=\"complexity\")\n        if complexity &gt; 15:\n            issues.append(f\"{file}: High complexity ({complexity})\")\n\n        # Check for patterns\n        result = t.distill(f\"find security issues in {file}\")\n        if \"password\" in result.content.lower():\n            issues.append(f\"{file}: Potential security issue\")\n\n    return issues\n</code></pre>"},{"location":"architecture/integration/#webhook-integration","title":"Webhook Integration","text":""},{"location":"architecture/integration/#github-webhooks","title":"GitHub Webhooks","text":"Python<pre><code>from flask import Flask, request\nfrom tenets import Tenets\n\napp = Flask(__name__)\n\n@app.route('/webhook', methods=['POST'])\ndef github_webhook():\n    payload = request.json\n\n    if payload['action'] == 'opened':\n        pr_title = payload['pull_request']['title']\n\n        # Generate context for PR\n        t = Tenets()\n        context = t.distill(pr_title)\n\n        # Post as comment\n        post_github_comment(\n            repo=payload['repository']['full_name'],\n            pr=payload['number'],\n            comment=f\"## Context\\n{context.content}\"\n        )\n\n    return 'OK'\n</code></pre>"},{"location":"architecture/integration/#performance-considerations","title":"Performance Considerations","text":""},{"location":"architecture/integration/#batch-processing","title":"Batch Processing","text":"Python<pre><code># Process multiple prompts efficiently\nfrom tenets import Tenets\n\nt = Tenets()\nprompts = [\"fix auth bug\", \"optimize database\", \"add caching\"]\n\nresults = []\nfor prompt in prompts:\n    result = t.distill(prompt, use_cache=True)\n    results.append(result)\n</code></pre>"},{"location":"architecture/integration/#async-integration","title":"Async Integration","text":"Python<pre><code>import asyncio\nfrom tenets import AsyncTenets\n\nasync def process_requests(prompts):\n    tenets = AsyncTenets()\n\n    tasks = [\n        tenets.distill(prompt)\n        for prompt in prompts\n    ]\n\n    results = await asyncio.gather(*tasks)\n    return results\n</code></pre>"},{"location":"architecture/integration/#error-handling","title":"Error Handling","text":"Python<pre><code>from tenets import Tenets, TenetsError\n\ntry:\n    t = Tenets()\n    result = t.distill(\"analyze code\")\nexcept TenetsError as e:\n    print(f\"Error: {e}\")\n    # Handle specific errors\n    if e.code == \"TOKEN_LIMIT_EXCEEDED\":\n        # Retry with lower limit\n        result = t.distill(\"analyze code\", max_tokens=50000)\n</code></pre>"},{"location":"architecture/overview/","title":"System Overview","text":"Loading optimized content..."},{"location":"architecture/overview/#what-is-tenets","title":"What is Tenets?","text":"<p>Tenets is a sophisticated, local-first code intelligence platform that revolutionizes how developers interact with their codebases when working with AI assistants.</p>"},{"location":"architecture/overview/#core-philosophy--design-principles","title":"Core Philosophy &amp; Design Principles","text":""},{"location":"architecture/overview/#1-local-first-processing","title":"1. Local-First Processing","text":"<p>All analysis, ranking, and context generation happens on the developer's machine. No code ever leaves the local environment. External API calls are only made for optional LLM-based summarization, and even then, only with explicit user consent.</p>"},{"location":"architecture/overview/#2-progressive-enhancement","title":"2. Progressive Enhancement","text":"<p>The system provides value immediately with just Python installed, and scales up with optional dependencies. Core functionality works without any ML libraries, git integration works without any configuration, and advanced features gracefully degrade when dependencies are missing.</p>"},{"location":"architecture/overview/#3-intelligent-caching","title":"3. Intelligent Caching","text":"<p>Every expensive operation is cached at multiple levels - memory caches for hot data, SQLite for structured data, disk caches for analysis results, and specialized caches for embeddings. Cache invalidation is intelligent, using file modification times, git commits, and content hashes.</p>"},{"location":"architecture/overview/#4-configurable-intelligence","title":"4. Configurable Intelligence","text":"<p>Every aspect of the ranking and analysis can be configured. Users can adjust factor weights, enable/disable features, add custom ranking functions, and tune performance parameters. The system adapts to different codebases and use cases.</p>"},{"location":"architecture/overview/#5-streaming-architecture","title":"5. Streaming Architecture","text":"<p>The system uses streaming and incremental processing wherever possible. Files are analyzed as they're discovered, rankings are computed in parallel, and results stream to the user as they become available.</p>"},{"location":"architecture/overview/#complete-system-architecture","title":"Complete System Architecture","text":"<pre><code>graph TB\n    subgraph \"User Layer\"\n        CLI[CLI Interface]\n        API[Python API]\n        IDE[IDE Extensions]\n    end\n\n    subgraph \"Command Processing\"\n        DISPATCHER[Command Dispatcher]\n        DISTILL[Distill Command]\n        RANK[Rank Command]\n        EXAMINE[Examine Command]\n        SESSION[Session Management]\n    end\n\n    subgraph \"Intelligence Pipeline\"\n        PARSER[Prompt Parser]\n        INTENT[Intent Detection]\n        KEYWORDS[Keyword Extraction]\n        SCANNER[File Scanner]\n        ANALYZER[Code Analyzer]\n        RANKER[Ranking Engine]\n        BUILDER[Context Builder]\n    end\n\n    subgraph \"Storage Layer\"\n        MEMORY[Memory Cache&lt;br/&gt;Hot Data]\n        SQLITE[SQLite DB&lt;br/&gt;Sessions]\n        DISK[Disk Cache&lt;br/&gt;Analysis]\n        EMBED[Embedding Cache&lt;br/&gt;ML Vectors]\n    end\n\n    CLI --&gt; DISPATCHER\n    API --&gt; DISPATCHER\n    IDE --&gt; DISPATCHER\n\n    DISPATCHER --&gt; DISTILL\n    DISPATCHER --&gt; RANK\n    DISPATCHER --&gt; EXAMINE\n    DISPATCHER --&gt; SESSION\n\n    DISTILL --&gt; PARSER\n    PARSER --&gt; INTENT\n    INTENT --&gt; KEYWORDS\n    KEYWORDS --&gt; SCANNER\n    SCANNER --&gt; ANALYZER\n    ANALYZER --&gt; RANKER\n    RANKER --&gt; BUILDER\n\n    ANALYZER --&gt; MEMORY\n    RANKER --&gt; SQLITE\n    BUILDER --&gt; DISK\n    INTENT --&gt; EMBED\n\n    style CLI fill:#e1f5fe\n    style BUILDER fill:#e8f5e8\n    style ANALYZER fill:#fff3e0\n    style RANKER fill:#fce4ec</code></pre>"},{"location":"architecture/overview/#data-flow-architecture","title":"Data Flow Architecture","text":"<pre><code>flowchart LR\n    subgraph \"Input\"\n        PROMPT[User Prompt]\n        FILES[Codebase Files]\n        CONFIG[Configuration]\n    end\n\n    subgraph \"Processing\"\n        DISCOVER[Discovery&lt;br/&gt;Find Files]\n        ANALYZE[Analysis&lt;br/&gt;Extract Structure]\n        RANK[Ranking&lt;br/&gt;Score Relevance]\n        OPTIMIZE[Optimization&lt;br/&gt;Token Budget]\n    end\n\n    subgraph \"Output\"\n        CONTEXT[Context]\n        REPORT[Reports]\n        VIZ[Visualizations]\n    end\n\n    PROMPT --&gt; DISCOVER\n    FILES --&gt; DISCOVER\n    CONFIG --&gt; DISCOVER\n\n    DISCOVER --&gt; ANALYZE\n    ANALYZE --&gt; RANK\n    RANK --&gt; OPTIMIZE\n\n    OPTIMIZE --&gt; CONTEXT\n    OPTIMIZE --&gt; REPORT\n    OPTIMIZE --&gt; VIZ\n\n    style PROMPT fill:#e3f2fd\n    style CONTEXT fill:#e8f5e9</code></pre>"},{"location":"architecture/overview/#system-components-overview","title":"System Components Overview","text":"<p>The Tenets system consists of several key architectural layers:</p>"},{"location":"architecture/overview/#-discovery-layer","title":"\ud83d\udd0d Discovery Layer","text":"<ul> <li>File discovery and scanning</li> <li>Git repository analysis</li> <li>Language detection and classification</li> </ul>"},{"location":"architecture/overview/#-analysis-layer","title":"\ud83e\udde0 Analysis Layer","text":"<ul> <li>Multi-language static analysis</li> <li>Code complexity measurement</li> <li>Pattern recognition and extraction</li> </ul>"},{"location":"architecture/overview/#-intelligence-layer","title":"\ud83d\udcca Intelligence Layer","text":"<ul> <li>Relevance ranking algorithms</li> <li>Machine learning pipelines</li> <li>Natural language processing</li> </ul>"},{"location":"architecture/overview/#-storage-layer","title":"\ud83d\udcbe Storage Layer","text":"<ul> <li>Session management</li> <li>Caching architecture</li> <li>Data persistence</li> </ul>"},{"location":"architecture/overview/#-interface-layer","title":"\ud83d\udd0c Interface Layer","text":"<ul> <li>CLI commands</li> <li>API endpoints</li> <li>Configuration management</li> </ul>"},{"location":"architecture/overview/#technology-stack","title":"Technology Stack","text":""},{"location":"architecture/overview/#core-technologies","title":"Core Technologies","text":"<pre><code>graph LR\n    subgraph \"Languages &amp; Frameworks\"\n        PYTHON[Python 3.9+]\n        TYPER[Typer CLI]\n        PYDANTIC[Pydantic]\n    end\n\n    subgraph \"Analysis Tools\"\n        AST[Python AST]\n        TREE_SITTER[Tree-sitter]\n        PYGMENTS[Pygments]\n    end\n\n    subgraph \"ML/NLP Stack\"\n        SKLEARN[scikit-learn]\n        TRANSFORMERS[Sentence Transformers]\n        YAKE[YAKE]\n        NUMPY[NumPy]\n    end\n\n    subgraph \"Storage &amp; Cache\"\n        SQLITE[SQLite]\n        REDIS[Redis Optional]\n        PICKLE[Pickle Cache]\n    end\n\n    PYTHON --&gt; AST\n    PYTHON --&gt; SKLEARN\n    TYPER --&gt; SQLITE\n    PYDANTIC --&gt; PICKLE</code></pre>"},{"location":"architecture/overview/#key-components-by-feature","title":"Key Components by Feature","text":"Component Technology Purpose CLI Framework Typer + Rich Modern CLI with progress bars Configuration YAML + Pydantic Type-safe configuration Code Parsing AST + Tree-sitter Multi-language analysis Text Analysis BM25 (primary) Probabilistic relevance scoring ML Models Sentence Transformers Semantic similarity Keyword Extraction YAKE Statistical extraction Git Integration GitPython Version control mining Database SQLite Session storage Caching LRU + Disk Multi-level caching Visualization Graphviz + D3.js Dependency graphs"},{"location":"architecture/overview/#-related-documentation","title":"\ud83d\udd17 Related Documentation","text":"<ul> <li>Core Systems \u2192 - Detailed analysis engines and pipelines</li> <li>Data &amp; Storage \u2192 - Database design and caching</li> <li>Performance \u2192 - Optimization and scalability</li> </ul>  \ud83d\udca1 This is a performance-optimized version. For the complete technical documentation, see the [original architecture file](../ARCHITECTURE-original.md)."},{"location":"architecture/performance/","title":"Performance Architecture","text":""},{"location":"architecture/performance/#overview","title":"Overview","text":"<p>Tenets is designed to handle large codebases efficiently through intelligent caching, parallel processing, and optimized algorithms. This document covers performance characteristics, optimization strategies, and best practices.</p>"},{"location":"architecture/performance/#performance-architecture-overview","title":"Performance Architecture Overview","text":"<pre><code>graph TD\n    subgraph \"Parallel Processing\"\n        FILE_SCAN[File Scanning&lt;br/&gt;Thread Pool]\n        ANALYSIS[Code Analysis&lt;br/&gt;Process Pool]\n        RANKING[Ranking&lt;br/&gt;Vectorized Ops]\n        EMBEDDING[Embeddings&lt;br/&gt;Batch Processing]\n    end\n\n    subgraph \"Streaming Architecture\"\n        INCREMENTAL[Incremental Discovery]\n        PROGRESSIVE[Progressive Ranking]\n        CHUNKED[Chunked Analysis]\n        STREAMING[Result Streaming]\n    end\n\n    subgraph \"Optimization Strategies\"\n        LAZY[Lazy Evaluation]\n        CACHE[Aggressive Caching]\n        MEMORY[Memory Management]\n        ADAPTIVE[Adaptive Behavior]\n    end\n\n    FILE_SCAN --&gt; INCREMENTAL\n    ANALYSIS --&gt; PROGRESSIVE\n    RANKING --&gt; CHUNKED\n    EMBEDDING --&gt; STREAMING\n\n    INCREMENTAL --&gt; LAZY\n    PROGRESSIVE --&gt; CACHE\n    CHUNKED --&gt; MEMORY\n    STREAMING --&gt; ADAPTIVE\n\n    style FILE_SCAN fill:#e3f2fd\n    style LAZY fill:#e8f5e9</code></pre>"},{"location":"architecture/performance/#performance-modes","title":"Performance Modes","text":""},{"location":"architecture/performance/#mode-comparison-benchmarked-on-real-codebase","title":"Mode Comparison (Benchmarked on Real Codebase)","text":"Mode Relative Performance Typical Time Files Analyzed Token Output Fast 100% (baseline) ~17s 10-20 files ~7K tokens Balanced 135% (1.3x slower) ~23s 50-75 files ~90K tokens Thorough 536% (5.4x slower) ~91s 75-100 files ~90K tokens"},{"location":"architecture/performance/#fast-mode","title":"Fast Mode","text":"<ul> <li>Relative Speed: Baseline (100%)</li> <li>Accuracy: Good for quick exploration</li> <li>Methods: Lightweight analysis, simple matching, no corpus building</li> <li>Use Cases: Quick searches, initial exploration, CI/CD pipelines</li> <li>Optimizations: </li> <li>Uses lightweight file analyzer (8KB samples)</li> <li>Skips AST parsing and language-specific analysis</li> <li>No corpus building or complex NLP</li> <li>Deep analysis only on top 20 ranked files</li> </ul>"},{"location":"architecture/performance/#balanced-mode-default","title":"Balanced Mode (Default)","text":"<ul> <li>Relative Speed: ~1.3x slower than Fast (135%)</li> <li>Accuracy: Excellent for most use cases</li> <li>Methods: BM25 scoring with corpus, word boundaries, intelligent summarization</li> <li>Use Cases: General development, feature building, most common scenarios</li> <li>Trade-offs: Full analysis provides better accuracy at minor speed cost</li> </ul>"},{"location":"architecture/performance/#thorough-mode","title":"Thorough Mode","text":"<ul> <li>Relative Speed: ~5.4x slower than Fast (536%)</li> <li>Accuracy: Best possible with ML-powered understanding</li> <li>Methods: </li> <li>ML embeddings (all-MiniLM-L6-v2 model)</li> <li>Dual algorithms (BM25 + TF-IDF)</li> <li>Programming pattern detection</li> <li>Comprehensive dependency analysis</li> <li>Use Cases: Complex refactoring, architectural changes, deep code understanding</li> <li>Performance Breakdown:</li> <li>~10s for ML model loading (first run)</li> <li>~23s for comprehensive ranking</li> <li>~5s for dual corpus building</li> <li>Remaining time for analysis and aggregation</li> </ul>"},{"location":"architecture/performance/#enhanced-text-matching-implementation","title":"Enhanced Text Matching Implementation","text":""},{"location":"architecture/performance/#fast-mode-optimizations","title":"Fast Mode Optimizations","text":"<p>The enhanced Fast mode uses RapidFuzz and optimized patterns for high-performance matching:</p> Python<pre><code># Word boundary enforcement with caching\n@lru_cache(maxsize=256)\ndef _get_word_boundary_pattern(keyword: str) -&gt; re.Pattern:\n    return re.compile(r'\\b' + re.escape(keyword) + r'\\b', re.IGNORECASE)\n\n# Hyphen/space variation handling\ndef _normalize_variations(text: str) -&gt; Set[str]:\n    return {\n        text.lower(),\n        text.replace('-', ''),\n        text.replace(' ', ''),\n        text.replace('-', ' '),\n        text.replace(' ', '-')\n    }\n</code></pre> <p>Performance Characteristics: - Word boundary matching: O(n) with compiled regex - Variation generation: O(1) with fixed transformations - LRU cache for patterns: 256 most recent patterns cached - Target: 1.0x baseline (&lt; 1ms per file)</p>"},{"location":"architecture/performance/#balanced-mode-enhancements","title":"Balanced Mode Enhancements","text":"<p>Balanced mode adds practical features while maintaining performance:</p> Python<pre><code># Efficient tokenization for compound words\ndef _tokenize_for_matching(text: str) -&gt; Set[str]:\n    tokens = set()\n    # Single pass tokenization\n    for word in re.findall(r'\\b\\w+\\b', text.lower()):\n        tokens.add(word)\n        # Split camelCase/snake_case in same pass\n        if '_' in word or any(c.isupper() for c in word):\n            tokens.update(split_compound(word))\n    return tokens\n</code></pre> <p>Abbreviation Expansion: - Pre-computed dictionary with 50+ common abbreviations - O(1) lookup for expansion - Lazy loading to reduce startup time</p>"},{"location":"architecture/performance/#thorough-mode-intelligence","title":"Thorough Mode Intelligence","text":"<p>Thorough mode adds semantic understanding:</p> Python<pre><code># Semantic caching for repeated queries\n@lru_cache(maxsize=1024)\ndef _get_semantic_similarity(text_hash: str, query_hash: str) -&gt; float:\n    # Cache semantic computations\n    return compute_similarity(text, query)\n</code></pre>"},{"location":"architecture/performance/#optimization-strategies","title":"Optimization Strategies","text":""},{"location":"architecture/performance/#1-file-discovery-optimization","title":"1. File Discovery Optimization","text":""},{"location":"architecture/performance/#parallel-traversal","title":"Parallel Traversal","text":"Python<pre><code># Parallel file system traversal\nfrom concurrent.futures import ThreadPoolExecutor\n\ndef discover_files_parallel(paths, max_workers=8):\n    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n        futures = [executor.submit(traverse, path) for path in paths]\n        results = [f.result() for f in futures]\n    return flatten(results)\n</code></pre>"},{"location":"architecture/performance/#early-filtering","title":"Early Filtering","text":"<ul> <li>Apply ignore patterns during traversal</li> <li>Skip binary files immediately</li> <li>Honor .gitignore patterns</li> <li>Prune excluded directories</li> </ul>"},{"location":"architecture/performance/#2-content-processing","title":"2. Content Processing","text":""},{"location":"architecture/performance/#streaming-large-files","title":"Streaming Large Files","text":"Python<pre><code>def process_large_file(path, chunk_size=8192):\n    with open(path, 'r', encoding='utf-8', errors='ignore') as f:\n        while True:\n            chunk = f.read(chunk_size)\n            if not chunk:\n                break\n            yield process_chunk(chunk)\n</code></pre>"},{"location":"architecture/performance/#smart-summarization","title":"Smart Summarization","text":"<ul> <li>Preserve imports and signatures</li> <li>Keep complex logic blocks</li> <li>Maintain documentation</li> <li>Remove redundant whitespace</li> <li>Strip comments when requested</li> </ul>"},{"location":"architecture/performance/#3-ranking-optimization","title":"3. Ranking Optimization","text":""},{"location":"architecture/performance/#vector-caching","title":"Vector Caching","text":"Python<pre><code># Cache TF-IDF vectors\nclass VectorCache:\n    def __init__(self, ttl=3600):\n        self.cache = {}\n        self.timestamps = {}\n        self.ttl = ttl\n\n    def get_or_compute(self, file_path, compute_fn):\n        if self.is_valid(file_path):\n            return self.cache[file_path]\n\n        vector = compute_fn(file_path)\n        self.cache[file_path] = vector\n        self.timestamps[file_path] = time.time()\n        return vector\n</code></pre>"},{"location":"architecture/performance/#batch-processing","title":"Batch Processing","text":"<ul> <li>Process multiple files in parallel</li> <li>Vectorize operations where possible</li> <li>Use numpy for numerical computations</li> <li>Minimize Python loops</li> </ul>"},{"location":"architecture/performance/#4-cache-architecture","title":"4. Cache Architecture","text":""},{"location":"architecture/performance/#multi-level-cache","title":"Multi-Level Cache","text":"YAML<pre><code>cache_hierarchy:\n  l1_memory:\n    - Hot paths (LRU, 100 items)\n    - Active session data\n    - Recent queries\n\n  l2_disk:\n    - File metadata\n    - Parse trees\n    - TF-IDF vectors\n\n  l3_persistent:\n    - Git history\n    - Dependency graphs\n    - ML model outputs\n</code></pre>"},{"location":"architecture/performance/#cache-warming","title":"Cache Warming","text":"Python<pre><code>def warm_cache(project_path):\n    \"\"\"Pre-populate cache with likely needed data.\"\"\"\n    # Common patterns to pre-cache\n    patterns = ['*.py', '*.js', '*.md', 'package.json', 'requirements.txt']\n\n    for pattern in patterns:\n        files = glob(os.path.join(project_path, '**', pattern), recursive=True)\n        for file in files[:100]:  # Limit to avoid memory issues\n            cache.preload(file)\n</code></pre>"},{"location":"architecture/performance/#algorithm-feature-comparison","title":"Algorithm Feature Comparison","text":""},{"location":"architecture/performance/#performance-benchmarks-per-file-ranking-only","title":"Performance Benchmarks (Per-File Ranking Only)","text":"Mode Avg Time/File Relative Speed Percentage Slower Files/Second Fast 0.5ms 1.0x baseline 0% ~2,000 Balanced 2.1ms 4.2x slower 320% ~476 Thorough 2.0ms (no ML) 4.0x slower 300% ~500 Thorough 50ms (with ML) 100x slower 9,900% ~20 <p>Note: These times exclude corpus building overhead. Total time depends on codebase size and whether corpus is cached.</p>"},{"location":"architecture/performance/#performance-by-file-count-total-time-including-corpus-building","title":"Performance by File Count (Total Time Including Corpus Building)","text":"Scenario Files Fast Mode Balanced Mode Thorough Mode (no ML) Small 10 files ~0.5s ~0.5s ~0.5s Medium 100 files ~5s ~2-3s ~2-3s Large 1000 files ~40s ~15-20s ~15-20s"},{"location":"architecture/performance/#why-balancedthorough-can-be-faster","title":"Why Balanced/Thorough Can Be Faster","text":"<p>The counterintuitive performance results occur because: 1. Fast mode skips corpus building but uses less efficient per-file ranking 2. Balanced mode builds a BM25 corpus once, then uses highly efficient scoring 3. Thorough mode (without ML) is essentially Balanced mode with additional TF-IDF 4. The corpus building overhead is quickly offset by faster per-file operations on medium/large codebases</p>"},{"location":"architecture/performance/#algorithm-capabilities","title":"Algorithm Capabilities","text":"Feature Fast Mode Balanced Mode Thorough Mode Keyword Matching \u2705 Basic \u2705 Enhanced \u2705 Advanced Path Analysis \u2705 Simple \u2705 Full \u2705 Full TF-IDF Scoring \u274c \u2705 Standard \u2705 Optimized BM25 Ranking \u274c \u2705 Standard \u2705 Enhanced Semantic Similarity \u274c \u26a0\ufe0f Optional \u2705 Full ML Git History Analysis \u274c \u2705 Recent \u2705 Complete Dependency Graphs \u274c \u26a0\ufe0f Basic \u2705 Full Graph Pattern Recognition \u2705 Regex \u2705 Enhanced \u2705 ML-based Import Analysis \u26a0\ufe0f Basic \u2705 Standard \u2705 Deep Complexity Analysis \u274c \u2705 Basic \u2705 Full Metrics"},{"location":"architecture/performance/#processing-characteristics","title":"Processing Characteristics","text":"Aspect Fast Balanced Thorough Parallel Workers 4 threads 8 threads 16+ threads Batch Size 1000 files 100 files 10 files Cache Strategy LRU only LRU + TTL Multi-level Memory Usage Minimal Moderate Adaptive Incremental Updates \u274c \u2705 \u2705 Streaming Results \u274c \u26a0\ufe0f Partial \u2705 Full"},{"location":"architecture/performance/#scalability-characteristics","title":"Scalability Characteristics","text":"Codebase Type Fast Mode Balanced Mode Thorough Mode Small Projects (&lt;1K files) \u2705 Instant \u2705 Quick \u26a0\ufe0f Overkill Medium Projects (1-10K) \u2705 Very Fast \u2705 Optimal \u2705 Detailed Large Monorepos (10-50K) \u2705 Recommended \u26a0\ufe0f Slower \u274c Too Slow Massive Codebases (50K+) \u2705 Only Option \u274c Impractical \u274c Unusable"},{"location":"architecture/performance/#bottleneck-analysis","title":"Bottleneck Analysis","text":""},{"location":"architecture/performance/#common-bottlenecks","title":"Common Bottlenecks","text":"<ol> <li>File I/O</li> <li> <p>Solution: Parallel reads, caching, memory mapping</p> </li> <li> <p>Git Operations</p> </li> <li> <p>Solution: Cache git data, use libgit2 bindings</p> </li> <li> <p>ML Model Loading</p> </li> <li> <p>Solution: Lazy loading, model quantization, caching</p> </li> <li> <p>Token Counting</p> </li> <li>Solution: Fast approximation, cached counts</li> </ol>"},{"location":"architecture/performance/#profiling-tools","title":"Profiling Tools","text":"Bash<pre><code># CPU profiling\npython -m cProfile -o profile.stats tenets distill \"query\"\n\n# Memory profiling\nmprof run tenets distill \"query\"\nmprof plot\n\n# Line profiling\nkernprof -l -v tenets_profile.py\n</code></pre>"},{"location":"architecture/performance/#optimization-tips","title":"Optimization Tips","text":""},{"location":"architecture/performance/#1-for-large-codebases","title":"1. For Large Codebases","text":"YAML<pre><code># .tenets.yml\nperformance:\n  max_files: 10000\n  max_file_size: 1048576  # 1MB\n  cache_size: 1073741824   # 1GB\n  parallel_workers: 8\n\nranking:\n  algorithm: fast  # Use fast mode for large repos\n  threshold: 0.3   # Higher threshold = fewer files\n</code></pre>"},{"location":"architecture/performance/#2-for-cicd","title":"2. For CI/CD","text":"Bash<pre><code># Pre-warm cache before operations\ntenets cache warm --patterns \"*.py,*.js\"\n\n# Use JSON output for parsing\ntenets rank \"query\" --format json --no-content\n\n# Limit scope for faster results\ntenets distill \"query\" --path src/ --max-files 100\n</code></pre>"},{"location":"architecture/performance/#3-for-interactive-use","title":"3. For Interactive Use","text":"Python<pre><code>from tenets import Tenets\n\n# Keep instance alive for session\nt = Tenets(persistent_cache=True)\n\n# Reuse for multiple queries\nfor query in queries:\n    result = t.distill(query, use_cache=True)\n</code></pre>"},{"location":"architecture/performance/#memory-management","title":"Memory Management","text":""},{"location":"architecture/performance/#garbage-collection","title":"Garbage Collection","text":"Python<pre><code>import gc\n\nclass MemoryManager:\n    @staticmethod\n    def cleanup_after_operation():\n        # Force garbage collection\n        gc.collect()\n\n        # Clear caches if memory usage is high\n        if get_memory_usage() &gt; threshold:\n            clear_caches()\n</code></pre>"},{"location":"architecture/performance/#streaming-operations","title":"Streaming Operations","text":"Python<pre><code>def stream_large_context(files, max_memory=104857600):  # 100MB\n    \"\"\"Stream context generation to avoid memory spikes.\"\"\"\n    current_size = 0\n    buffer = []\n\n    for file in files:\n        content = read_file(file)\n        size = len(content.encode('utf-8'))\n\n        if current_size + size &gt; max_memory:\n            yield ''.join(buffer)\n            buffer = []\n            current_size = 0\n\n        buffer.append(content)\n        current_size += size\n\n    if buffer:\n        yield ''.join(buffer)\n</code></pre>"},{"location":"architecture/performance/#network-performance","title":"Network Performance","text":""},{"location":"architecture/performance/#api-response-optimization","title":"API Response Optimization","text":"Python<pre><code># Compress responses\nimport gzip\n\ndef compress_response(data):\n    return gzip.compress(data.encode('utf-8'))\n\n# Pagination for large results\ndef paginate_results(results, page=1, per_page=100):\n    start = (page - 1) * per_page\n    end = start + per_page\n    return results[start:end]\n</code></pre>"},{"location":"architecture/performance/#database-performance","title":"Database Performance","text":""},{"location":"architecture/performance/#index-optimization","title":"Index Optimization","text":"SQL<pre><code>-- Indexes for session storage\nCREATE INDEX idx_session_updated ON sessions(updated_at);\nCREATE INDEX idx_files_relevance ON files(relevance_score DESC);\nCREATE INDEX idx_cache_key ON cache(cache_key);\n</code></pre>"},{"location":"architecture/performance/#query-optimization","title":"Query Optimization","text":"Python<pre><code># Batch database operations\ndef batch_insert(records, batch_size=1000):\n    for i in range(0, len(records), batch_size):\n        batch = records[i:i + batch_size]\n        db.insert_many(batch)\n</code></pre>"},{"location":"architecture/performance/#monitoring","title":"Monitoring","text":""},{"location":"architecture/performance/#performance-metrics","title":"Performance Metrics","text":"Python<pre><code>from dataclasses import dataclass\nfrom typing import Dict\nimport time\n\n@dataclass\nclass PerformanceMetrics:\n    operation: str\n    duration: float\n    memory_used: int\n    files_processed: int\n    cache_hits: int\n    cache_misses: int\n\n    def report(self) -&gt; Dict:\n        return {\n            'operation': self.operation,\n            'duration_ms': self.duration * 1000,\n            'memory_mb': self.memory_used / 1048576,\n            'files_processed': self.files_processed,\n            'cache_hit_rate': self.cache_hits / (self.cache_hits + self.cache_misses)\n        }\n</code></pre>"},{"location":"architecture/performance/#logging","title":"Logging","text":"Python<pre><code>import logging\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n)\n\nlogger = logging.getLogger('tenets.performance')\n\n# Log slow operations\nif duration &gt; threshold:\n    logger.warning(f\"Slow operation: {operation} took {duration}s\")\n</code></pre>"}]}